{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T16:05:00.741959Z",
     "iopub.status.busy": "2020-12-11T16:05:00.741492Z",
     "iopub.status.idle": "2020-12-11T16:05:00.960792Z",
     "shell.execute_reply": "2020-12-11T16:05:00.959480Z",
     "shell.execute_reply.started": "2020-12-11T16:05:00.741906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T14:28:06.135469Z",
     "iopub.status.busy": "2020-12-11T14:28:06.135006Z",
     "iopub.status.idle": "2020-12-11T14:28:06.666641Z",
     "shell.execute_reply": "2020-12-11T14:28:06.665381Z",
     "shell.execute_reply.started": "2020-12-11T14:28:06.135422Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T14:05:00.184195Z",
     "iopub.status.busy": "2020-12-11T14:05:00.183802Z",
     "iopub.status.idle": "2020-12-11T14:05:00.224995Z",
     "shell.execute_reply": "2020-12-11T14:05:00.223836Z",
     "shell.execute_reply.started": "2020-12-11T14:05:00.184150Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['axes.xmargin'] = .05\n",
    "plt.rcParams['axes.ymargin'] = .05\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T14:05:43.327388Z",
     "iopub.status.busy": "2020-12-11T14:05:43.326975Z",
     "iopub.status.idle": "2020-12-11T14:05:43.372786Z",
     "shell.execute_reply": "2020-12-11T14:05:43.371624Z",
     "shell.execute_reply.started": "2020-12-11T14:05:43.327343Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "        for chunk in response.iter_content(CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def download_data(data_path):\n",
    "    toy_data_path = os.path.join(data_path, \"fever_data.zip\")\n",
    "    toy_data_url_id = \"1wArZhF9_SHW17WKNGeLmX-QTYw9Zscl1\"\n",
    "    toy_url = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "\n",
    "    if not os.path.exists(toy_data_path):\n",
    "        print(\"Downloading FEVER data splits...\")\n",
    "        with requests.Session() as current_session:\n",
    "            response = current_session.get(\n",
    "                toy_url, params={\"id\": toy_data_url_id}, stream=True\n",
    "            )\n",
    "        save_response_content(response, toy_data_path)\n",
    "        print(\"Download completed!\")\n",
    "\n",
    "        print(\"Extracting dataset...\")\n",
    "        with zipfile.ZipFile(toy_data_path) as loaded_zip:\n",
    "            loaded_zip.extractall(data_path)\n",
    "        print(\"Extraction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T14:05:46.746306Z",
     "iopub.status.busy": "2020-12-11T14:05:46.745899Z",
     "iopub.status.idle": "2020-12-11T14:05:46.785126Z",
     "shell.execute_reply": "2020-12-11T14:05:46.784050Z",
     "shell.execute_reply.started": "2020-12-11T14:05:46.746260Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_folder = os.path.join(\"datasets\", \"fever\")\n",
    "download_data(dataset_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:20:44.742948Z",
     "iopub.status.busy": "2020-12-11T15:20:44.742513Z",
     "iopub.status.idle": "2020-12-11T15:20:45.608506Z",
     "shell.execute_reply": "2020-12-11T15:20:45.607308Z",
     "shell.execute_reply.started": "2020-12-11T15:20:44.742904Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Hemsworth appeared in A Perfect Getaway.</td>\n",
       "      <td>2\\tHemsworth has also appeared in the science ...</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Roald Dahl is a writer.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Roald Dahl is a governor.</td>\n",
       "      <td>0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ireland has relatively low-lying mountains.</td>\n",
       "      <td>10\\tThe island 's geography comprises relative...</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>10\\tThe island 's geography comprises relative...</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0     Chris Hemsworth appeared in A Perfect Getaway.   \n",
       "1                            Roald Dahl is a writer.   \n",
       "2                          Roald Dahl is a governor.   \n",
       "3        Ireland has relatively low-lying mountains.   \n",
       "4  Ireland does not have relatively low-lying mou...   \n",
       "\n",
       "                                            evidence  id     label  split  \n",
       "0  2\\tHemsworth has also appeared in the science ...   3  SUPPORTS  train  \n",
       "1  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   7  SUPPORTS  train  \n",
       "2  0\\tRoald Dahl -LRB- -LSB- langpronˈroʊ.əld _ ˈ...   8   REFUTES  train  \n",
       "3  10\\tThe island 's geography comprises relative...   9  SUPPORTS  train  \n",
       "4  10\\tThe island 's geography comprises relative...  10   REFUTES  train  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for split in [\"train\", \"test\", \"val\"]:\n",
    "    split_file = os.path.join(dataset_folder, f\"{split}_pairs.csv\")\n",
    "    if os.path.isfile(split_file):\n",
    "        split_df = pd.read_csv(split_file, index_col=0)\n",
    "        split_df[\"split\"] = pd.Series([split] * len(split_df), index=split_df.index)\n",
    "        dfs.append(split_df)\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "df.columns = map(str.lower, df.columns)\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T14:06:35.091255Z",
     "iopub.status.busy": "2020-12-11T14:06:35.090841Z",
     "iopub.status.idle": "2020-12-11T14:06:35.147270Z",
     "shell.execute_reply": "2020-12-11T14:06:35.145915Z",
     "shell.execute_reply.started": "2020-12-11T14:06:35.091210Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['SUPPORTS', 'REFUTES'], dtype=object)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:09:31.366115Z",
     "iopub.status.busy": "2020-12-11T15:09:31.365668Z",
     "iopub.status.idle": "2020-12-11T15:09:31.484409Z",
     "shell.execute_reply": "2020-12-11T15:09:31.483041Z",
     "shell.execute_reply.started": "2020-12-11T15:09:31.366069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (121740, 6)\n",
      "Validation set shape: (7165, 6)\n",
      "Test set shape: (7189, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f'Training set shape: {df[df[\"split\"] == \"train\"].shape}')\n",
    "print(f'Validation set shape: {df[df[\"split\"] == \"val\"].shape}')\n",
    "print(f'Test set shape: {df[df[\"split\"] == \"test\"].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T14:57:32.763317Z",
     "iopub.status.busy": "2020-12-11T14:57:32.762875Z",
     "iopub.status.idle": "2020-12-11T14:57:32.823871Z",
     "shell.execute_reply": "2020-12-11T14:57:32.822731Z",
     "shell.execute_reply.started": "2020-12-11T14:57:32.763271Z"
    }
   },
   "outputs": [],
   "source": [
    "TAB_CHAR = \"\\t\"\n",
    "PERIOD_CHAR = \".\"\n",
    "LEFT_PARENS = \"-LRB-\"\n",
    "RIGHT_PARENS = \"-RRB-\"\n",
    "SQUARE_BRACKETS = r\"-LSB-.*-RSB-\"\n",
    "\n",
    "\n",
    "def preprocess_claim(text):\n",
    "    if text[-1] in string.punctuation:\n",
    "        text = text[:-1]\n",
    "    return text.strip().lower()\n",
    "\n",
    "\n",
    "def preprocess_evidence(text):\n",
    "    # Remove everything before the first tab character\n",
    "    text = text[text.find(TAB_CHAR) + len(TAB_CHAR) :]\n",
    "    # Replace tabs with spaces\n",
    "    text = text.replace(TAB_CHAR, \" \")\n",
    "    # Remove parenthesis\n",
    "    text = text.replace(LEFT_PARENS, \" \").replace(RIGHT_PARENS, \" \")\n",
    "    # Remove everything between square brackets\n",
    "    text = re.sub(SQUARE_BRACKETS, \"\", text)\n",
    "    # Remove everything after the last period\n",
    "    last_period = text.rfind(PERIOD_CHAR)\n",
    "    if last_period is not None:\n",
    "        text = text[:last_period]\n",
    "    # Remove punctuation\n",
    "    text = text.translate(text.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Remove extra whitespaces\n",
    "    text = \" \".join(text.split())\n",
    "    # Convert to lowercase\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:01:26.350197Z",
     "iopub.status.busy": "2020-12-11T15:01:26.349724Z",
     "iopub.status.idle": "2020-12-11T15:01:26.420244Z",
     "shell.execute_reply": "2020-12-11T15:01:26.419127Z",
     "shell.execute_reply.started": "2020-12-11T15:01:26.350152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random evidence 89133\n",
      "1\tHe won an Academy Award for Best Actor , BAFTA Award for Best Actor in a Leading Role , and Golden Globe Award for Best Actor in a Musical or Comedy , for his portrayal of Ray Charles in the 2004 biographical film Ray .\tAcademy Award for Best Actor\tAcademy Award for Best Actor\tBAFTA Award\tBAFTA Award\tGolden Globe Award\tGolden Globe Award\tRay Charles\tRay Charles\tRay\tRay (film)\n",
      "he won an academy award for best actor bafta award for best actor in a leading role and golden globe award for best actor in a musical or comedy for his portrayal of ray charles in the 2004 biographical film ray\n"
     ]
    }
   ],
   "source": [
    "random_index = random.choice(df.index.tolist())\n",
    "random_evidence = df.loc[random_index][\"evidence\"]\n",
    "preprocessed_random_evidence = preprocess_evidence(random_evidence)\n",
    "print(f\"Random evidence {random_index}\")\n",
    "print(f\"{random_evidence}\")\n",
    "print(f\"{preprocessed_random_evidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:20:53.398172Z",
     "iopub.status.busy": "2020-12-11T15:20:53.397739Z",
     "iopub.status.idle": "2020-12-11T15:20:56.836749Z",
     "shell.execute_reply": "2020-12-11T15:20:56.835415Z",
     "shell.execute_reply.started": "2020-12-11T15:20:53.398127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chris hemsworth appeared in a perfect getaway</td>\n",
       "      <td>hemsworth has also appeared in the science fic...</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roald dahl is a writer</td>\n",
       "      <td>roald dahl 13 september 1916 23 november 1990 ...</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roald dahl is a governor</td>\n",
       "      <td>roald dahl 13 september 1916 23 november 1990 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ireland has relatively low-lying mountains</td>\n",
       "      <td>the island s geography comprises relatively lo...</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ireland does not have relatively low-lying mou...</td>\n",
       "      <td>the island s geography comprises relatively lo...</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0      chris hemsworth appeared in a perfect getaway   \n",
       "1                             roald dahl is a writer   \n",
       "2                           roald dahl is a governor   \n",
       "3         ireland has relatively low-lying mountains   \n",
       "4  ireland does not have relatively low-lying mou...   \n",
       "\n",
       "                                            evidence  id     label  split  \n",
       "0  hemsworth has also appeared in the science fic...   3  SUPPORTS  train  \n",
       "1  roald dahl 13 september 1916 23 november 1990 ...   7  SUPPORTS  train  \n",
       "2  roald dahl 13 september 1916 23 november 1990 ...   8   REFUTES  train  \n",
       "3  the island s geography comprises relatively lo...   9  SUPPORTS  train  \n",
       "4  the island s geography comprises relatively lo...  10   REFUTES  train  "
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"claim\"] = df[\"claim\"].apply(preprocess_claim)\n",
    "df[\"evidence\"] = df[\"evidence\"].apply(preprocess_evidence)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:34:58.746388Z",
     "iopub.status.busy": "2020-12-11T15:34:58.745937Z",
     "iopub.status.idle": "2020-12-11T15:35:00.781416Z",
     "shell.execute_reply": "2020-12-11T15:35:00.780221Z",
     "shell.execute_reply.started": "2020-12-11T15:34:58.746342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    False\n",
      "dtype: bool\n",
      "0    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(df[\"claim\"].str.extract(r\"\\b(~)\\b\").any())\n",
    "print(df[\"evidence\"].str.extract(r\"\\b(~)\\b\").any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:35:10.750704Z",
     "iopub.status.busy": "2020-12-11T15:35:10.750256Z",
     "iopub.status.idle": "2020-12-11T15:35:10.804889Z",
     "shell.execute_reply": "2020-12-11T15:35:10.803501Z",
     "shell.execute_reply.started": "2020-12-11T15:35:10.750658Z"
    }
   },
   "outputs": [],
   "source": [
    "PADDING_TOKEN = \"~\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:35:29.782913Z",
     "iopub.status.busy": "2020-12-11T15:35:29.782476Z",
     "iopub.status.idle": "2020-12-11T15:35:29.837052Z",
     "shell.execute_reply": "2020-12-11T15:35:29.835906Z",
     "shell.execute_reply.started": "2020-12-11T15:35:29.782867Z"
    }
   },
   "outputs": [],
   "source": [
    "Vocabulary = namedtuple(\"Vocabulary\", [\"to_string\", \"from_string\", \"string\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:45:19.866127Z",
     "iopub.status.busy": "2020-12-11T15:45:19.865779Z",
     "iopub.status.idle": "2020-12-11T15:45:19.924642Z",
     "shell.execute_reply": "2020-12-11T15:45:19.923385Z",
     "shell.execute_reply.started": "2020-12-11T15:45:19.866084Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, padding_token):\n",
    "    \"\"\"\n",
    "    Given a list of strings, builds the corresponding Vocabulary object\n",
    "    \"\"\"\n",
    "    assert padding_token not in tokens\n",
    "    words = sorted(set(tokens))\n",
    "    vocabulary, inverse_vocabulary = dict(), dict()\n",
    "    vocabulary[0] = str(padding_token)\n",
    "    inverse_vocabulary[str(padding_token)] = 0\n",
    "    for i, w in tqdm(enumerate(words)):\n",
    "        vocabulary[i + 1] = w\n",
    "        inverse_vocabulary[w] = i + 1\n",
    "    return Vocabulary(\n",
    "        to_string=vocabulary,\n",
    "        from_string=inverse_vocabulary,\n",
    "        string=[padding_token] + words,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:44:19.174731Z",
     "iopub.status.busy": "2020-12-11T15:44:19.174188Z",
     "iopub.status.idle": "2020-12-11T15:44:27.361859Z",
     "shell.execute_reply": "2020-12-11T15:44:27.360757Z",
     "shell.execute_reply.started": "2020-12-11T15:44:19.174685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chris',\n",
       " 'hemsworth',\n",
       " 'appeared',\n",
       " 'in',\n",
       " 'a',\n",
       " 'perfect',\n",
       " 'getaway',\n",
       " 'roald',\n",
       " 'dahl',\n",
       " 'is']"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claim_tokens = list(flatten(df[\"claim\"].str.split().tolist()))\n",
    "evidence_tokens = list(flatten(df[\"evidence\"].str.split().tolist()))\n",
    "tokens = claim_tokens + evidence_tokens\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:45:26.515930Z",
     "iopub.status.busy": "2020-12-11T15:45:26.515602Z",
     "iopub.status.idle": "2020-12-11T15:45:27.331724Z",
     "shell.execute_reply": "2020-12-11T15:45:27.330559Z",
     "shell.execute_reply.started": "2020-12-11T15:45:26.515888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55af5d45b76946cfa42c21cf29c21b02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vocabulary = build_vocabulary(tokens, PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:47:07.792667Z",
     "iopub.status.busy": "2020-12-11T15:47:07.792345Z",
     "iopub.status.idle": "2020-12-11T15:47:07.853793Z",
     "shell.execute_reply": "2020-12-11T15:47:07.852435Z",
     "shell.execute_reply.started": "2020-12-11T15:47:07.792626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9476"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.from_string[\"chris\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T15:46:11.790663Z",
     "iopub.status.busy": "2020-12-11T15:46:11.790201Z",
     "iopub.status.idle": "2020-12-11T15:46:11.849003Z",
     "shell.execute_reply": "2020-12-11T15:46:11.847493Z",
     "shell.execute_reply.started": "2020-12-11T15:46:11.790617Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43508"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:22:01.118721Z",
     "iopub.status.busy": "2020-12-11T17:22:01.118270Z",
     "iopub.status.idle": "2020-12-11T17:22:03.692223Z",
     "shell.execute_reply": "2020-12-11T17:22:03.690887Z",
     "shell.execute_reply.started": "2020-12-11T17:22:01.118675Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evidence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>split</th>\n",
       "      <th>indexed_claim</th>\n",
       "      <th>indexed_evidence</th>\n",
       "      <th>indexed_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chris hemsworth appeared in a perfect getaway</td>\n",
       "      <td>hemsworth has also appeared in the science fic...</td>\n",
       "      <td>3</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "      <td>[9476, 19069, 4942, 20352, 3069, 29751, 17367]</td>\n",
       "      <td>[19069, 18742, 4240, 4942, 20352, 38680, 34479...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>roald dahl is a writer</td>\n",
       "      <td>roald dahl 13 september 1916 23 november 1990 ...</td>\n",
       "      <td>7</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "      <td>[33347, 11731, 21088, 3069, 42418]</td>\n",
       "      <td>[33347, 11731, 893, 35007, 1558, 2058, 28017, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>roald dahl is a governor</td>\n",
       "      <td>roald dahl 13 september 1916 23 november 1990 ...</td>\n",
       "      <td>8</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "      <td>[33347, 11731, 21088, 3069, 17844]</td>\n",
       "      <td>[33347, 11731, 893, 35007, 1558, 2058, 28017, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ireland has relatively low-lying mountains</td>\n",
       "      <td>the island s geography comprises relatively lo...</td>\n",
       "      <td>9</td>\n",
       "      <td>SUPPORTS</td>\n",
       "      <td>train</td>\n",
       "      <td>[21040, 18742, 32610, 24122, 26679]</td>\n",
       "      <td>[38680, 21120, 33900, 17284, 10467, 32610, 241...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ireland does not have relatively low-lying mou...</td>\n",
       "      <td>the island s geography comprises relatively lo...</td>\n",
       "      <td>10</td>\n",
       "      <td>REFUTES</td>\n",
       "      <td>train</td>\n",
       "      <td>[21040, 13184, 27962, 18802, 32610, 24122, 26679]</td>\n",
       "      <td>[38680, 21120, 33900, 17284, 10467, 32610, 241...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0      chris hemsworth appeared in a perfect getaway   \n",
       "1                             roald dahl is a writer   \n",
       "2                           roald dahl is a governor   \n",
       "3         ireland has relatively low-lying mountains   \n",
       "4  ireland does not have relatively low-lying mou...   \n",
       "\n",
       "                                            evidence  id     label  split  \\\n",
       "0  hemsworth has also appeared in the science fic...   3  SUPPORTS  train   \n",
       "1  roald dahl 13 september 1916 23 november 1990 ...   7  SUPPORTS  train   \n",
       "2  roald dahl 13 september 1916 23 november 1990 ...   8   REFUTES  train   \n",
       "3  the island s geography comprises relatively lo...   9  SUPPORTS  train   \n",
       "4  the island s geography comprises relatively lo...  10   REFUTES  train   \n",
       "\n",
       "                                       indexed_claim  \\\n",
       "0     [9476, 19069, 4942, 20352, 3069, 29751, 17367]   \n",
       "1                 [33347, 11731, 21088, 3069, 42418]   \n",
       "2                 [33347, 11731, 21088, 3069, 17844]   \n",
       "3                [21040, 18742, 32610, 24122, 26679]   \n",
       "4  [21040, 13184, 27962, 18802, 32610, 24122, 26679]   \n",
       "\n",
       "                                    indexed_evidence  indexed_label  \n",
       "0  [19069, 18742, 4240, 4942, 20352, 38680, 34479...              1  \n",
       "1  [33347, 11731, 893, 35007, 1558, 2058, 28017, ...              1  \n",
       "2  [33347, 11731, 893, 35007, 1558, 2058, 28017, ...              0  \n",
       "3  [38680, 21120, 33900, 17284, 10467, 32610, 241...              1  \n",
       "4  [38680, 21120, 33900, 17284, 10467, 32610, 241...              0  "
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_indexes(values, to_index):\n",
    "    \"\"\"\n",
    "    Given a list of keys and a dictionary indexed by those keys,\n",
    "    return the corresponding values in the dictionary\n",
    "    \"\"\"\n",
    "    return [to_index[v] for v in values]\n",
    "\n",
    "\n",
    "df[\"indexed_claim\"] = df[\"claim\"].apply(\n",
    "    lambda s: to_indexes(s.split(), vocabulary.from_string)\n",
    ")\n",
    "df[\"indexed_evidence\"] = df[\"evidence\"].apply(\n",
    "    lambda s: to_indexes(s.split(), vocabulary.from_string)\n",
    ")\n",
    "df[\"indexed_label\"] = pd.Categorical(df[\"label\"], ordered=True).codes\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:22:26.743315Z",
     "iopub.status.busy": "2020-12-11T17:22:26.742883Z",
     "iopub.status.idle": "2020-12-11T17:22:26.805124Z",
     "shell.execute_reply": "2020-12-11T17:22:26.803636Z",
     "shell.execute_reply.started": "2020-12-11T17:22:26.743269Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeverDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Fever dataset for fact checking\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, int)\n",
    "        claim = self.df.loc[index, \"indexed_claim\"]\n",
    "        evidence = self.df.loc[index, \"indexed_evidence\"]\n",
    "        label = self.df.loc[index, \"indexed_label\"]\n",
    "        return claim, evidence, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:22:32.747768Z",
     "iopub.status.busy": "2020-12-11T17:22:32.747441Z",
     "iopub.status.idle": "2020-12-11T17:22:32.965141Z",
     "shell.execute_reply": "2020-12-11T17:22:32.963821Z",
     "shell.execute_reply.started": "2020-12-11T17:22:32.747726Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = FeverDataset(df[df[\"split\"] == \"train\"])\n",
    "val_dataset = FeverDataset(df[df[\"split\"] == \"val\"])\n",
    "test_dataset = FeverDataset(df[df[\"split\"] == \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:23:21.747022Z",
     "iopub.status.busy": "2020-12-11T17:23:21.746520Z",
     "iopub.status.idle": "2020-12-11T17:23:21.812748Z",
     "shell.execute_reply": "2020-12-11T17:23:21.811555Z",
     "shell.execute_reply.started": "2020-12-11T17:23:21.746976Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch, padding_index):\n",
    "    \"\"\"\n",
    "    This function expects to receive a list of tuples (i.e. a batch),\n",
    "    s.t. each tuple contains one (claim, evidence, label) triple\n",
    "    and returns the same sequences padded with the padding token\n",
    "    \"\"\"\n",
    "    (claims, evidences, labels) = zip(*batch)\n",
    "    claims_lenghts = [len(x) for x in claims]\n",
    "    evidences_lenghts = [len(y) for y in evidences]\n",
    "    padded_claims = pad_sequence(\n",
    "        [torch.tensor(t) for t in claims],\n",
    "        batch_first=True,\n",
    "        padding_value=padding_index,\n",
    "    )\n",
    "    padded_evidences = pad_sequence(\n",
    "        [torch.tensor(t) for t in evidences],\n",
    "        batch_first=True,\n",
    "        padding_value=padding_index,\n",
    "    )\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.bool)\n",
    "    return padded_claims, padded_evidences, claims_lenghts, evidences_lenghts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:23:27.800747Z",
     "iopub.status.busy": "2020-12-11T17:23:27.800419Z",
     "iopub.status.idle": "2020-12-11T17:23:27.856995Z",
     "shell.execute_reply": "2020-12-11T17:23:27.855889Z",
     "shell.execute_reply.started": "2020-12-11T17:23:27.800705Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:23:33.770501Z",
     "iopub.status.busy": "2020-12-11T17:23:33.770151Z",
     "iopub.status.idle": "2020-12-11T17:23:33.830700Z",
     "shell.execute_reply": "2020-12-11T17:23:33.829468Z",
     "shell.execute_reply.started": "2020-12-11T17:23:33.770459Z"
    }
   },
   "outputs": [],
   "source": [
    "default_dataloader = partial(\n",
    "    DataLoader,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=partial(pad_batch, padding_index=vocabulary.from_string[PADDING_TOKEN]),\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T17:23:39.708410Z",
     "iopub.status.busy": "2020-12-11T17:23:39.708089Z",
     "iopub.status.idle": "2020-12-11T17:23:39.771851Z",
     "shell.execute_reply": "2020-12-11T17:23:39.770762Z",
     "shell.execute_reply.started": "2020-12-11T17:23:39.708368Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = default_dataloader(train_dataset)\n",
    "val_dataloader = default_dataloader(val_dataset)\n",
    "test_dataloader = default_dataloader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T16:06:46.889961Z",
     "iopub.status.busy": "2020-12-11T16:06:46.889511Z",
     "iopub.status.idle": "2020-12-11T16:06:46.955972Z",
     "shell.execute_reply": "2020-12-11T16:06:46.954990Z",
     "shell.execute_reply.started": "2020-12-11T16:06:46.889916Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T16:06:53.618016Z",
     "iopub.status.busy": "2020-12-11T16:06:53.617689Z",
     "iopub.status.idle": "2020-12-11T16:11:53.531147Z",
     "shell.execute_reply": "2020-12-11T16:11:53.529316Z",
     "shell.execute_reply.started": "2020-12-11T16:06:53.617974Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = utils.load_embedding_model(\n",
    "    \"glove\", embedding_dimension=EMBEDDING_DIMENSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T16:55:57.747282Z",
     "iopub.status.busy": "2020-12-11T16:55:57.746841Z",
     "iopub.status.idle": "2020-12-11T16:55:57.825681Z",
     "shell.execute_reply": "2020-12-11T16:55:57.824527Z",
     "shell.execute_reply.started": "2020-12-11T16:55:57.747237Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrices(\n",
    "    df, embedding_model, embedding_dimension, vocabulary, splits, method=\"normal\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrices (one for each split) of a specific dataset,\n",
    "    given a pre-trained Gensim word embedding model\n",
    "    \"\"\"\n",
    "    assert method in (\"uniform\", \"normal\")\n",
    "\n",
    "    def uniform_embedding(interval=(-1, 1)):\n",
    "        return interval[0] + np.random.sample(embedding_dimension) + interval[1]\n",
    "\n",
    "    def normal_embedding():\n",
    "        return np.random.normal(embedding_dimension)\n",
    "\n",
    "    oov_terms = dict()\n",
    "    embedding_matrices = dict.fromkeys(\n",
    "        splits, np.zeros((len(vocabulary.string), embedding_dimension))\n",
    "    )\n",
    "    for split in splits:\n",
    "        tokens = set(\n",
    "            list(flatten(df[df[\"split\"] == split][\"claim\"].str.split().tolist()))\n",
    "            + list(flatten(df[df[\"split\"] == split][\"evidence\"].str.split().tolist()))\n",
    "        )\n",
    "        for word in tokens:\n",
    "            word_index = vocabulary.from_string[word]\n",
    "            word_vector = np.zeros((1, embedding_dimension))\n",
    "            # Words that are no OOV are taken from the Gensim model\n",
    "            if word in embedding_model.vocab:\n",
    "                word_vector = embedding_model[word]\n",
    "            # Compute OOV embedding, if not already done\n",
    "            elif word not in oov_terms:\n",
    "                # OOV words computed as random normal vectors\n",
    "                if method == \"normal\":\n",
    "                    word_vector = normal_embedding()\n",
    "                # OOV words computed as uniform vectors in range [-1, 1]\n",
    "                elif method == \"uniform\":\n",
    "                    word_vector = uniform_embedding()\n",
    "                # The word is not OOV anymore\n",
    "                oov_terms[word] = word_vector\n",
    "            else:\n",
    "                word_vector = oov_terms[word]\n",
    "            embedding_matrices[split][word_index, :] = word_vector\n",
    "    return embedding_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T16:56:03.977154Z",
     "iopub.status.busy": "2020-12-11T16:56:03.976831Z",
     "iopub.status.idle": "2020-12-11T16:56:15.409539Z",
     "shell.execute_reply": "2020-12-11T16:56:15.407703Z",
     "shell.execute_reply.started": "2020-12-11T16:56:03.977112Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_matrices = build_embedding_matrices(\n",
    "    df,\n",
    "    embedding_model,\n",
    "    EMBEDDING_DIMENSION,\n",
    "    vocabulary,\n",
    "    [\"train\", \"val\", \"test\"],\n",
    "    method=\"normal\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-11T16:56:58.738315Z",
     "iopub.status.busy": "2020-12-11T16:56:58.737971Z",
     "iopub.status.idle": "2020-12-11T16:56:58.813406Z",
     "shell.execute_reply": "2020-12-11T16:56:58.812220Z",
     "shell.execute_reply.started": "2020-12-11T16:56:58.738271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrices[\"train\"][0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactCheckingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dimension,\n",
    "        lr=1e-3,\n",
    "        padding_index=0,\n",
    "        sentence_strategy=\"bov\",\n",
    "        merging_strategy=\"mean\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a generic fact checking model, with recurrent modules\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Padding\n",
    "        self.padding_index = padding_index\n",
    "\n",
    "        # Get device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Embedding module\n",
    "        self.embedding = None\n",
    "\n",
    "        # Strategy to perform sentence embedding\n",
    "        assert sentence_strategy in (\"bov\", \"mlp\", \"rnn_out_avg\", \"rnn_last\")\n",
    "        self.sentence_strategy = sentence_strategy\n",
    "\n",
    "        # Strategy to merge claims and evidences\n",
    "        assert merging_strategy in (\"mean\", \"sum\", \"cat\")\n",
    "        self.merging_strategy = merging_strategy\n",
    "\n",
    "        # Dense and dropout layers\n",
    "        self.classifier = nn.Linear(\n",
    "            embedding_dimension * 2\n",
    "            if merging_strategy == \"cat\"\n",
    "            else embedding_dimension,\n",
    "            1,\n",
    "        )\n",
    "\n",
    "        # Criterion\n",
    "        self.criterion = nn.BCEWithLogitsLoss().to(self.device)\n",
    "\n",
    "        # Set default optimizer\n",
    "        self.lr = lr\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        # Define learning rate decay (defaults to no decay)\n",
    "        self.lr_scheduler = None\n",
    "\n",
    "        # Transfer model to device\n",
    "        self.to(self.device)\n",
    "\n",
    "    def lr_step(self):\n",
    "        \"\"\"\n",
    "        Decay the learning rate, based on the defined schedule\n",
    "        \"\"\"\n",
    "        if self.lr_scheduler is not None:\n",
    "            self.lr_scheduler.step()\n",
    "\n",
    "    def inject_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Inject the given embedding matrix in the Embedding module\n",
    "        and make it non-trainable\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(\n",
    "            embeddings.shape[0], embeddings.shape[1], padding_idx=self.padding_index\n",
    "        )\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(embeddings))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "    def count_parameters(self):\n",
    "        \"\"\"\n",
    "        Return the total number of trainable parameters in the model\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initialize all the model weights by sampling from a normal\n",
    "        distribution with zero mean and small standard deviation\n",
    "        \"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "    def _bag_of_vectors(self, claims, evidences):\n",
    "        return claims.mean(dim=1), evidences.mean(dim=1)\n",
    "\n",
    "    def _merge_embeddings(self, claims, evidences):\n",
    "        if self.merging_strategy == \"mean\":\n",
    "            return (claims + evidences) / 2\n",
    "        elif self.merging_strategy == \"sum\":\n",
    "            return claims + evidences\n",
    "        elif self.merging_strategy == \"cat\":\n",
    "            return torch.stack([claims, evidences], dim=1)\n",
    "        return None\n",
    "\n",
    "    def forward(self, claims, claims_lenghts, evidences, evidences_lenghts):\n",
    "        \"\"\"\n",
    "        Perform a forward pass and return predictions over\n",
    "        a mini-batch of sequences of the same lenght\n",
    "        (one value for each possible tag for each input token)\n",
    "        \"\"\"\n",
    "        assert self.embedding is not None\n",
    "        embedded_claims = self.embedding(claims)\n",
    "        embedded_evidences = self.embedding(evidences)\n",
    "        packed_claims = pack_padded_sequence(\n",
    "            embedded_claims, claims_lenghts, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_evidences = pack_padded_sequence(\n",
    "            embedded_evidences,\n",
    "            evidences_lenghts,\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        bov_claims, bov_evidences = self._bag_of_vectors(\n",
    "            packed_claims, packed_evidences\n",
    "        )\n",
    "        merged_inputs = self._merge_embeddings(bov_claims, bov_evidences)\n",
    "        predictions = self.classifier(merged_inputs)\n",
    "        return predictions\n",
    "\n",
    "    def predict(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Transform emission scores into tags indexes and return\n",
    "        flattened Python lists containing predictions and ground truths\n",
    "        \"\"\"\n",
    "        non_pad_elements = torch.where(tags.view(-1) != self.padding_index)[0]\n",
    "        ground_truth = tags.view(-1)[non_pad_elements].detach().cpu().tolist()\n",
    "        if not self.crf:\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            max_predictions = predictions.argmax(dim=1)\n",
    "            return (\n",
    "                max_predictions[non_pad_elements].detach().cpu().tolist(),\n",
    "                ground_truth,\n",
    "            )\n",
    "\n",
    "        mask = torch.ones(tags.shape, dtype=torch.uint8)\n",
    "        mask[tags == self.padding_index] = 0\n",
    "        max_predictions = self.criterion.decode(predictions, mask=mask)\n",
    "        return (list(flatten(max_predictions)), ground_truth)\n",
    "\n",
    "    def loss(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Compute the loss and return its value\n",
    "        \"\"\"\n",
    "        return self.criterion(predictions, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments/summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

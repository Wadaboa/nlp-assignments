{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/jobs/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/jobs/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /Users/jobs/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import mark_negation\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import (\n",
    "    CountVectorizer,\n",
    "    TfidfTransformer,\n",
    "    TfidfVectorizer,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    make_scorer,\n",
    "    mean_absolute_error,\n",
    "    mean_squared_error,\n",
    "    plot_confusion_matrix,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dataset name and URL\n",
    "dataset_name = \"imdb\"\n",
    "dataset_url = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "# Create the dataset folder\n",
    "dataset_folder = os.path.join(os.getcwd(), \"datasets\", dataset_name)\n",
    "original_dataset_folder = os.path.join(dataset_folder, \"original\")\n",
    "if not os.path.exists(original_dataset_folder):\n",
    "    os.makedirs(original_dataset_folder)\n",
    "\n",
    "# Create the dataframe folder\n",
    "df_folder = os.path.join(dataset_folder, \"dataframe\")\n",
    "if not os.path.exists(df_folder):\n",
    "    os.makedirs(df_folder)\n",
    "dataframe_path = os.path.join(df_folder, dataset_name + \".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset already downloaded and extracted\n"
     ]
    }
   ],
   "source": [
    "# Download and extract the dataset\n",
    "original_dataset_path = os.path.join(original_dataset_folder, \"movies.tar.gz\")\n",
    "if not os.path.exists(original_dataset_path):\n",
    "    urllib.request.urlretrieve(dataset_url, original_dataset_path)\n",
    "    print(\"Successful download\")\n",
    "    tar = tarfile.open(original_dataset_path)\n",
    "    tar.extractall(original_dataset_folder)\n",
    "    extracted_folder_name = tar.getnames()[0]\n",
    "    tar.close()\n",
    "    print(\"Successful extraction\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded and extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe already saved as a pickle file\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(dataframe_path):\n",
    "    dataframe_rows = []\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        for sentiment in [\"pos\", \"neg\"]:\n",
    "            folder = os.path.join(\n",
    "                original_dataset_folder, \"aclImdb\", split, sentiment\n",
    "            )\n",
    "            for filename in os.listdir(folder):\n",
    "                file_path = os.path.join(folder, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        with open(file_path, mode=\"r\", encoding=\"utf-8\") as text_file:\n",
    "                            # Extract info\n",
    "                            text = text_file.read()\n",
    "                            score = filename.split(\"_\")[1].split(\".\")[0]\n",
    "                            file_id = filename.split(\"_\")[0]\n",
    "\n",
    "                            # Compute sentiment\n",
    "                            num_sentiment = (\n",
    "                                1 if sentiment == \"pos\"\n",
    "                                else 0 if sentiment == \"neg\"\n",
    "                                else -1\n",
    "                            )\n",
    "\n",
    "                            # Create single dataframe row\n",
    "                            dataframe_row = {\n",
    "                                \"file_id\": file_id,\n",
    "                                \"score\": score,\n",
    "                                \"sentiment\": num_sentiment,\n",
    "                                \"split\": split,\n",
    "                                \"text\": text,\n",
    "                            }\n",
    "                            dataframe_rows.append(dataframe_row)\n",
    "                except Exception as e:\n",
    "                    print(\"Failed to process %s. Reason: %s\" % (file_path, e))\n",
    "                    sys.exit(0)\n",
    "\n",
    "    # Transform the list of rows in a proper dataframe\n",
    "    dataframe = pd.DataFrame(dataframe_rows)\n",
    "    dataframe_cols = [\"file_id\", \"score\", \"sentiment\", \"split\", \"text\"]\n",
    "    dataframe = dataframe[dataframe_cols]\n",
    "    dataframe.to_pickle(dataframe_path)\n",
    "else:\n",
    "    print(\"Dataframe already saved as a pickle file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "  file_id  score  sentiment  split  \\\n",
       "0    4715      9          1  train   \n",
       "1   12390      8          1  train   \n",
       "2    8329      7          1  train   \n",
       "3    9063      8          1  train   \n",
       "4    3092     10          1  train   \n",
       "\n",
       "                                                text  \n",
       "0  For a movie that gets no respect there sure ar...  \n",
       "1  Bizarre horror movie filled with famous faces ...  \n",
       "2  A solid, if unremarkable film. Matthau, as Ein...  \n",
       "3  It's a strange feeling to sit alone in a theat...  \n",
       "4  You probably all already know this by now, but...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_id</th>\n      <th>score</th>\n      <th>sentiment</th>\n      <th>split</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4715</td>\n      <td>9</td>\n      <td>1</td>\n      <td>train</td>\n      <td>For a movie that gets no respect there sure ar...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>12390</td>\n      <td>8</td>\n      <td>1</td>\n      <td>train</td>\n      <td>Bizarre horror movie filled with famous faces ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>8329</td>\n      <td>7</td>\n      <td>1</td>\n      <td>train</td>\n      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9063</td>\n      <td>8</td>\n      <td>1</td>\n      <td>train</td>\n      <td>It's a strange feeling to sit alone in a theat...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3092</td>\n      <td>10</td>\n      <td>1</td>\n      <td>train</td>\n      <td>You probably all already know this by now, but...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df = pd.read_pickle(dataframe_path)\n",
    "df['score'] = pd.to_numeric(df['score'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Distribution of scores: \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1     10122\n",
       "10     9731\n",
       "8      5859\n",
       "4      5331\n",
       "3      4961\n",
       "7      4803\n",
       "9      4607\n",
       "2      4586\n",
       "Name: score, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "print(\"Distribution of scores: \")\n",
    "df['score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score labels: [1, 2, 3, 4, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "score_labels = sorted(df['score'].unique())\n",
    "print(f\"Score labels: {score_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of duplicated texts: 418\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of duplicated texts: {df['text'].duplicated().value_counts()[True]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words in class 1: 61170\n",
      "10 most frequent (after 0.7 frequency) words in class 1:\n",
      "['book', 'maybe', 'gets', 'almost', 'may', '2', 'sure', 'since', 'however', '..']\n",
      "\n",
      "Number of different words in class 2: 44293\n",
      "10 most frequent (after 0.7 frequency) words in class 2:\n",
      "['rather', 'own', 'budget', 'sense', 'actor', 'both', 'feel', 'yet', 'having', 'half']\n",
      "\n",
      "Number of different words in class 3: 49303\n",
      "10 most frequent (after 0.7 frequency) words in class 3:\n",
      "['screen', 'audience', 'stupid', 'family', 'actor', 'house', 'rest', 'sex', 'once', 'during']\n",
      "\n",
      "Number of different words in class 4: 52665\n",
      "10 most frequent (after 0.7 frequency) words in class 4:\n",
      "['actor', 'death', 'different', 'help', 'fan', 'together', 'takes', 'each', 'less', 'house']\n",
      "\n",
      "Number of different words in class 7: 50749\n",
      "10 most frequent (after 0.7 frequency) words in class 7:\n",
      "['\\x96', 'until', 'second', 'believe', 'keep', 'kids', 'become', 'small', 'hollywood', 'production']\n",
      "\n",
      "Number of different words in class 8: 55589\n",
      "10 most frequent (after 0.7 frequency) words in class 8:\n",
      "['watched', 'said', 'works', 'fan', 'couple', \"'d\", 'someone', 'human', 'loved', 'others']\n",
      "\n",
      "Number of different words in class 9: 47572\n",
      "10 most frequent (after 0.7 frequency) words in class 9:\n",
      "['let', 'said', 'short', 'small', 'seem', 'mind', 'until', 'others', 'need', 'definitely']\n",
      "\n",
      "Number of different words in class 10: 63246\n",
      "10 most frequent (after 0.7 frequency) words in class 10:\n",
      "['american', 'three', 'fan', 'father', 'later', 'mind', 'goes', 'version', 'black', 'night']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def compute_freqs(words, top, thresh):\n",
    "    \"\"\"\n",
    "    Return the `top` most frequent words of the dataset,\n",
    "    after filtering them to be after `thresh` frequency \n",
    "    \"\"\"\n",
    "    fdist = nltk.FreqDist(words)\n",
    "    common = {\n",
    "        k: fdist.freq(k)\n",
    "        for k, _ in sorted(fdist.items(), key=lambda i: i[1], reverse=True)\n",
    "    }\n",
    "    words = list(common.keys())\n",
    "    freqs = list(common.values())\n",
    "    common_cumulative = {k: sum(freqs[:i]) for i, k in enumerate(words)}\n",
    "    return [k for k, v in common_cumulative.items() if v >= thresh][:top]\n",
    "\n",
    "\n",
    "def print_df_freqs(dataframe, top=10, thresh=0.7):\n",
    "    \"\"\"\n",
    "    Print the `top` most frequent words of the dataset,\n",
    "    for each class\n",
    "    \"\"\"\n",
    "    for c in score_labels:\n",
    "        words = []\n",
    "        text = df.query(f\"score == {c}\")[\"text\"].to_numpy()\n",
    "        for t in text:\n",
    "            words.extend(word_tokenize(t.lower()))\n",
    "        print(f\"Number of different words in class {c}: {len(set(words))}\")\n",
    "        common = compute_freqs(words, top=top, thresh=thresh)\n",
    "        print(f\"{top} most frequent (after {thresh} frequency) words in class {c}:\")\n",
    "        print(common)\n",
    "        print()\n",
    "\n",
    "\n",
    "print_df_freqs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of texts with dates:\n",
      "False    49997\n",
      "True         3\n",
      "Name: text, dtype: int64\n",
      "\n",
      "Number of texts with decimal numbers:\n",
      "False    49814\n",
      "True       186\n",
      "Name: text, dtype: int64\n",
      "\n",
      "Number of texts with integers:\n",
      "False    50000\n",
      "Name: text, dtype: int64\n",
      "\n",
      "Number of texts with elements in square brackets:\n",
      "False    49987\n",
      "True        13\n",
      "Name: text, dtype: int64\n",
      "\n",
      "Number of texts with HTML tags:\n",
      "False    49846\n",
      "True       154\n",
      "Name: text, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def re_match(regex, text):\n",
    "    \"\"\"\n",
    "    Return True if the given regex matches inside the given text,\n",
    "    otherwise return False\n",
    "    \"\"\"\n",
    "    return re.match(regex, text) is not None\n",
    "\n",
    "\n",
    "# Find dates\n",
    "DATE_RE = r\"\\d{1,2}[-\\/\\.]\\d{1,2}[-\\/\\.]\\d{2,4}\"\n",
    "num_dates = df[\"text\"].map(partial(re_match, DATE_RE)).value_counts()\n",
    "print(\"Number of texts with dates:\")\n",
    "print(num_dates)\n",
    "print()\n",
    "\n",
    "# Find floats\n",
    "FLOAT_RE = r\"(\\d*\\,)?\\d+.\\d*\"\n",
    "num_floats = df[\"text\"].map(partial(re_match, FLOAT_RE)).value_counts()\n",
    "print(\"Number of texts with decimal numbers:\")\n",
    "print(num_floats)\n",
    "print()\n",
    "\n",
    "# Find ints\n",
    "INT_RE = r\"(?<=\\s)\\d+(?=\\s)\"\n",
    "num_ints = df[\"text\"].map(partial(re_match, INT_RE)).value_counts()\n",
    "print(\"Number of texts with integers:\")\n",
    "print(num_ints)\n",
    "print()\n",
    "\n",
    "# Find brackets\n",
    "BRACKETS_RE = r\"\\[[^]]*\\]\"\n",
    "num_brackets = df[\"text\"].map(partial(re_match, BRACKETS_RE)).value_counts()\n",
    "print(\"Number of texts with elements in square brackets:\")\n",
    "print(num_brackets)\n",
    "print()\n",
    "\n",
    "# Find HTML tags\n",
    "HTML_RE = r\"<.*?>\"\n",
    "num_html = df[\"text\"].map(partial(re_match, HTML_RE)).value_counts()\n",
    "print(\"Number of texts with HTML tags:\")\n",
    "print(num_html)\n",
    "print()\n",
    "\n",
    "# Find punctuation\n",
    "PUNCTUATION_RE = r\"[^\\w{w}\\s\\{<>}]+\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[df['split'] == \"train\"]\n",
    "test_df = df.loc[df['split'] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of sentences in the training set: 25000\nNumber of sentences in the test set: 25000\n"
     ]
    }
   ],
   "source": [
    "train_corpus, train_scores, test_corpus, test_scores = (\n",
    "    train_df[\"text\"].tolist(),\n",
    "    np.array(train_df[\"score\"].tolist()),\n",
    "    test_df[\"text\"].tolist(),\n",
    "    np.array(test_df[\"score\"].tolist()),\n",
    ")\n",
    "print(f\"Number of sentences in the training set: {len(x_train)}\")\n",
    "print(f\"Number of sentences in the test set: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(classifier, x_train, x_test):\n",
    "    return (\n",
    "        classifier.predict(x_train),\n",
    "        classifier.predict(x_test),\n",
    "        np.around(y_pred_test),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(y_test, y_pred_test, y_pred_test_class):\n",
    "    # Evaluation as a regression task\n",
    "    print(\"R2 score %f\" % (r2_score(y_test, y_pred_test)))\n",
    "    print(\"MAE %f\" % (mean_absolute_error(y_test, y_pred_test)))\n",
    "    print(\"MSE %f\" % (mean_squared_error(y_test, y_pred_test)))\n",
    "    print()\n",
    "\n",
    "    # Evaluation as a multi-class classification task\n",
    "    report = classification_report(y_test, y_pred_test_class, labels=score_labels)\n",
    "    print(report)\n",
    "\n",
    "    # Fancy confusion matrix\n",
    "    plot_confusion_matrix(\n",
    "        classifier,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        normalize=\"true\",\n",
    "        cmap=plt.cm.Blues,\n",
    "        values_format=\".2f\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Stopwords (179): {'after', 'your', 'd', \"mustn't\", 'which', \"needn't\", 'it', 'doesn', 'couldn', 'our', 'some', \"weren't\", \"haven't\", 'm', 'ours', \"you're\", 'their', 'him', 'as', 'few', 'hasn', 'these', 'more', 'are', 'not', 'you', 'were', 'until', 'against', 'won', 'those', 'on', \"you've\", 'own', \"shouldn't\", \"she's\", \"aren't\", 'before', 'doing', \"it's\", 'the', 'from', \"don't\", 'for', 'such', 'theirs', 'didn', 'up', 'o', \"wouldn't\", 'his', 'about', 'over', 'here', 'if', 'most', 'be', 'into', 'above', 's', \"should've\", 'did', 'between', 'aren', 'itself', 'with', \"couldn't\", 'an', 'while', 've', 'of', 'off', 'further', 'why', 'both', 'same', 'this', 'than', 'just', 'no', 'she', 't', 'them', \"hadn't\", 'and', 'through', 'yours', \"isn't\", 'yourselves', 'himself', 'needn', \"hasn't\", 'is', 'where', 'don', \"shan't\", 'myself', 'been', 'in', 'll', 'haven', 'has', 'what', 'was', 'had', 'do', 'how', 'but', 'a', 'its', 'by', 'or', 'being', 'ma', 'shouldn', 'am', 'i', 'weren', 'during', 'they', 'themselves', \"won't\", 'down', \"that'll\", 'will', 'now', 'so', 'at', 'should', 'other', 'out', 'all', \"wasn't\", \"didn't\", 'my', 'having', 'her', 'then', 'below', 'mightn', 'he', 'ain', \"you'll\", 'we', 'there', 'wasn', \"mightn't\", \"you'd\", 'very', 'hadn', 'mustn', 'ourselves', 'whom', 'too', 're', 'because', 'to', 'wouldn', 'shan', 'who', 'that', 'does', 'nor', 'isn', 'again', 'y', 'any', 'hers', 'yourself', 'under', 'can', 'once', 'only', \"doesn't\", 'me', 'when', 'have', 'herself', 'each'}\n"
     ]
    }
   ],
   "source": [
    "EN_STOPWORDS = set(stopwords.words(\"english\"))\n",
    "print(f\"Stopwords ({len(EN_STOPWORDS)}): {EN_STOPWORDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEMMER = nltk.porter.PorterStemmer()\n",
    "LEMMATIZER = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    return [STEMMER.stem(word) for word in text]\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [LEMMATIZER.lemmatize(word) for word in text]\n",
    "\n",
    "\n",
    "def preprocess_text(\n",
    "    text,\n",
    "    mark_negation=False,\n",
    "    remove_punct=False,\n",
    "    remove_stopwords=False,\n",
    "    min_chars=None,\n",
    "    root=None,\n",
    "):\n",
    "    # Strip trailing spaces and remove newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(HTML_RE, \"\", text)\n",
    "    # Remove text in square brackets\n",
    "    text = re.sub(BRACKETS_RE, \"\", text)\n",
    "    # Remove dates\n",
    "    text = re.sub(DATE_RE, \"\", text)\n",
    "    # Remove floating numbers\n",
    "    text = re.sub(FLOAT_RE, \"\", text)\n",
    "    # Mark negation\n",
    "    if mark_negation:\n",
    "        text = \" \".join(mark_negation(text.split(), double_neg_flip=False))\n",
    "        EN_STOPWORDS.add(\"_NEG\")\n",
    "    # Remove punctuation\n",
    "    if remove_punct:\n",
    "        text = re.sub(PUNCTUATION_RE, \"\", text)\n",
    "    # Leave single whitespace\n",
    "    text = text.split()\n",
    "    # Remove words with less than `n` chars\n",
    "    if min_chars is not None and isinstance(min_chars, int):\n",
    "        text = [word for word in text if len(word) >= min_chars]\n",
    "    # Remove stopwords\n",
    "    if remove_stopwords:\n",
    "        text = [word for word in text if word not in EN_STOPWORDS]\n",
    "    # Perform stemming/lemmatization\n",
    "    if root == \"stem\":\n",
    "        text = stem_text(text)\n",
    "    elif root == \"lemmatize\":\n",
    "        text = lemmatize_text(text)\n",
    "    # Return the text as a string\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random text with score 3\n\nBefore pre-processing:\nThis picture reminds me of a Keneth More picture from 1957 called The Admirable Crighton\" whilst on the boat he was a servant and on the island he became the master and upon being saved reverted back to servant. Madonna did OK in some movies however this one doesn't fire. If there is any picture that show Madonna can't act it is this one.<br /><br />I am not sure whether this was a subtle copy of \"The Admirable Crighton\" but it sure looks like it and if thats the case then Hollywood must be running out of ideas and that is sad. to provide a platform for actors to improve their career profile and just on that this fails in every corner and detail.<br /><br />The plot is loose and the acting is mediocre. The script should have been put thru the shredder before taking it on location. While many here have canned Madoona for all her acting I think that in films like \"A League of Their own\" was quite good and enjoyable and \"Who's That Girl\" showed a quirkiness of Madonna's style not shown before or afterwards. Other Madonna films are not as enjoyable and I would have liked a to see a Madonna Animation series with the character from \"Who's That Girl\" I like the music from these two films however \"Swept away\" remains at the bottom of the pile here and will remain so.<br /><br />everyone has a bomb right?\n\nAfter pre-processing:\nthis picture reminds me of a keneth more picture from called the admirable crighton\" whilst on the boat he was a servant and on the island he became the master and upon being saved reverted back to servant. madonna did ok in some movies however this one doesn't fire. if there is any picture that show madonna can't act it is this one.i am not sure whether this was a subtle copy of \"the admirable crighton\" but it sure looks like it and if thats the case then hollywood must be running out of ideas and that is sad. to provide a platform for actors to improve their career profile and just on that this fails in every corner and detail.the plot is loose and the acting is mediocre. the script should have been put thru the shredder before taking it on location. while many here have canned madoona for all her acting i think that in films like \"a league of their own\" was quite good and enjoyable and \"who's that girl\" showed a quirkiness of madonna's style not shown before or afterwards. other madonna films are not as enjoyable and i would have liked a to see a madonna animation series with the character from \"who's that girl\" i like the music from these two films however \"swept away\" remains at the bottom of the pile here and will remain so.everyone has a bomb right?\n"
     ]
    }
   ],
   "source": [
    "rnd_text = np.random.choice(df.index, 1)[0]\n",
    "print(f\"Random text with score {df['score'].iloc[rnd_text]}\")\n",
    "print()\n",
    "print(f\"Before pre-processing:\")\n",
    "print(df['text'].iloc[rnd_text])\n",
    "print()\n",
    "print(\"After pre-processing:\")\n",
    "print(preprocess_text(df['text'].iloc[rnd_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline(\n",
    "    [\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "text_clf = text_clf.fit(train_corpus, train_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_pred_test, y_pred_test_class = predict(classifier, x_train, x_test)\n",
    "print_evaluation(y_test, y_pred_test, y_pred_test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-02f14b902979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr2_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best parameters: {classifier.best_params_}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    706\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m                     \u001b[0;31m# scheduling.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    559\u001b[0m         \"\"\"Shutdown the workers and restart a new one with the same parameters\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# When workers are killed in such a brutal manner, they cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexecutor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0;31m# To reduce the risk of opening too many files, remove references to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_clf = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\"clf\", LogisticRegression(random_state=RANDOM_SEED, max_iter=1000)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"vect__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__use_idf\": (True, False),\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    text_clf, parameters, scoring=make_scorer(r2_score), cv=5, n_jobs=-1\n",
    ")\n",
    "classifier = classifier.fit(x_train, y_train)\n",
    "print(f\"Best parameters: {classifier.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_pred_test, y_pred_test_class = predict(classifier, x_train, x_test)\n",
    "print_evaluation(y_test, y_pred_test, y_pred_test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with another classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf = Pipeline(\n",
    "    [\n",
    "        (\"vect\", CountVectorizer()),\n",
    "        (\"tfidf\", TfidfTransformer()),\n",
    "        (\n",
    "            \"clf\",\n",
    "            SGDClassifier(\n",
    "                random_state=RANDOM_SEED,\n",
    "                loss=\"hinge\",\n",
    "                penalty=\"l2\",\n",
    "                alpha=1e-3,\n",
    "                max_iter=500,\n",
    "                tol=None,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "parameters = {\n",
    "    \"vect__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__use_idf\": (True, False),\n",
    "    \"clf__alpha\": (1e-2, 1e-3),\n",
    "}\n",
    "classifier = GridSearchCV(\n",
    "    text_clf, parameters, scoring=make_scorer(r2_score), cv=5, n_jobs=-1\n",
    ")\n",
    "classifier = classifier.fit(x_train, y_train)\n",
    "print(f\"Best parameters: {classifier.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train, y_pred_test, y_pred_test_class = predict(classifier, x_train, x_test)\n",
    "print_evaluation(y_test, y_pred_test, y_pred_test_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
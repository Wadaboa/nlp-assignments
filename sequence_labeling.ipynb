{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3 : Sequence labelling with RNNs\n",
    "In this assignement we will ask you to perform POS tagging.\n",
    "\n",
    "You are asked to follow these steps:\n",
    "\n",
    "- Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "- Embed the words using GloVe embeddings\n",
    "- Create a baseline model, using a simple neural architecture\n",
    "- Experiment doing small modifications to the model\n",
    "- Evaluate your best model\n",
    "- Analyze the errors of your model\n",
    "- Corpora: Ignore the numeric value in the third column, use only the words/symbols and its label. https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
    "\n",
    "Splits: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
    "\n",
    "Baseline: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
    "\n",
    "Modifications: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
    "\n",
    "Training and Experiments: all the experiments must involve only the training and validation sets.\n",
    "\n",
    "Evaluation: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
    "\n",
    "Error Analysis (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
    "\n",
    "Report: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:31:16.959742Z",
     "iopub.status.busy": "2020-11-12T17:31:16.959289Z",
     "iopub.status.idle": "2020-11-12T17:31:17.034536Z",
     "shell.execute_reply": "2020-11-12T17:31:17.032908Z",
     "shell.execute_reply.started": "2020-11-12T17:31:16.959688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T16:11:38.427960Z",
     "iopub.status.busy": "2020-11-12T16:11:38.427561Z",
     "iopub.status.idle": "2020-11-12T16:11:38.716162Z",
     "shell.execute_reply": "2020-11-12T16:11:38.715085Z",
     "shell.execute_reply.started": "2020-11-12T16:11:38.427919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"dependency_treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:09.673246Z",
     "iopub.status.busy": "2020-11-12T17:34:09.672818Z",
     "iopub.status.idle": "2020-11-12T17:34:09.735332Z",
     "shell.execute_reply": "2020-11-12T17:34:09.734162Z",
     "shell.execute_reply.started": "2020-11-12T17:34:09.673204Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed):\n",
    "    \"\"\"\n",
    "    Fix all the possible sources of randomness\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "fix_random(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:10.179022Z",
     "iopub.status.busy": "2020-11-12T17:34:10.178726Z",
     "iopub.status.idle": "2020-11-12T17:34:10.232985Z",
     "shell.execute_reply": "2020-11-12T17:34:10.231821Z",
     "shell.execute_reply.started": "2020-11-12T17:34:10.178985Z"
    }
   },
   "outputs": [],
   "source": [
    "file_prefix = \"wsj_\"\n",
    "file_ext = \".dp\"\n",
    "train_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(1, 101)]\n",
    "val_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(101, 151)]\n",
    "test_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(151, 200)]\n",
    "splits = (\n",
    "    [\"train\"] * len(train_files) + [\"val\"] * len(val_files) + [\"test\"] * len(test_files)\n",
    ")\n",
    "whole_files = train_files + val_files + test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:10.722948Z",
     "iopub.status.busy": "2020-11-12T17:34:10.722648Z",
     "iopub.status.idle": "2020-11-12T17:34:10.776303Z",
     "shell.execute_reply": "2020-11-12T17:34:10.774925Z",
     "shell.execute_reply.started": "2020-11-12T17:34:10.722911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0001.dp',\n",
       " 'wsj_0021.dp',\n",
       " 'wsj_0041.dp',\n",
       " 'wsj_0061.dp',\n",
       " 'wsj_0081.dp',\n",
       " 'wsj_0101.dp',\n",
       " 'wsj_0121.dp',\n",
       " 'wsj_0141.dp',\n",
       " 'wsj_0161.dp',\n",
       " 'wsj_0181.dp']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_files[0:-1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:11.427088Z",
     "iopub.status.busy": "2020-11-12T17:34:11.426786Z",
     "iopub.status.idle": "2020-11-12T17:34:11.480857Z",
     "shell.execute_reply": "2020-11-12T17:34:11.479661Z",
     "shell.execute_reply.started": "2020-11-12T17:34:11.427051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre\\tNNP\\t2\\nVinken\\tNNP\\t8\\n,\\t,\\t2\\n61\\tCD\\t5\\nyears\\tNNS\\t6\\nold\\tJJ\\t2\\n,\\t,\\t2\\nwill\\tMD\\t0\\njoin\\tVB\\t8\\nthe\\tDT\\t11\\nboard\\tNN\\t9\\nas\\tIN\\t9\\na\\tDT\\t15\\nnonexecutive\\tJJ\\t15\\ndirector\\tNN\\t12\\nNov.\\tNNP\\t9\\n29\\tCD\\t16\\n.\\t.\\t8\\n\\nMr.\\tNNP\\t2\\nVinken\\tNNP\\t3\\nis\\tVBZ\\t0\\nchairman\\tNN\\t3\\nof\\tIN\\t4\\nElsevier\\tNNP\\t7\\nN.V.\\tNNP\\t12\\n,\\t,\\t12\\nthe\\tDT\\t12\\nDutch\\tNNP\\t12\\npublishing\\tVBG\\t12\\ngroup\\tNN\\t5\\n.\\t.\\t3\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_treebank.raw(fileids=whole_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:11.823053Z",
     "iopub.status.busy": "2020-11-12T17:34:11.822756Z",
     "iopub.status.idle": "2020-11-12T17:34:11.880736Z",
     "shell.execute_reply": "2020-11-12T17:34:11.879699Z",
     "shell.execute_reply.started": "2020-11-12T17:34:11.823017Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_file(fileid, preprocessor=None):\n",
    "    \"\"\"\n",
    "    Parse the given file identifier from the dependency treebank corpus\n",
    "    and return a tuple (`tokens`, `tags`), where `tokens` is the list\n",
    "    of tokens retrieved in the document and `tags` is the associated\n",
    "    list of tags (`tokens` and `tags` have the same lenght)\n",
    "    \n",
    "    If you wish to preprocess tokens, you can pass a function to the\n",
    "    `preprocessor` argument, which should take as input only the token\n",
    "    to transform\n",
    "    \"\"\"\n",
    "    file_str = dependency_treebank.raw(fileids=fileid)\n",
    "    splitted_file_str = [\n",
    "        x for x in re.split(\"\\t|\\n\", file_str.strip()) if x.strip() != \"\"\n",
    "    ]\n",
    "    if preprocessor is None:\n",
    "        preprocessor = lambda t: t\n",
    "    tokens, tags = [], []\n",
    "    for i in range(0, len(splitted_file_str), 3):\n",
    "        token = preprocessor(splitted_file_str[i])\n",
    "        tag = splitted_file_str[i + 1]\n",
    "        tokens.append(token)\n",
    "        tags.append(tag)\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def parse_files(fileids, preprocessor=None):\n",
    "    \"\"\"\n",
    "    Parse a set of file identifiers from the dependency treebank corpus\n",
    "    and return two lists, one which contains lists of tokens and one \n",
    "    containing lists of corresponding tags for each file identifier\n",
    "    \"\"\"\n",
    "    tokens_list, tags_list = [], []\n",
    "    for fileid in fileids:\n",
    "        tokens, tags = parse_file(fileid, preprocessor=preprocessor)\n",
    "        tokens_list.append(tokens)\n",
    "        tags_list.append(tags)\n",
    "    return tokens_list, tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:13.165872Z",
     "iopub.status.busy": "2020-11-12T17:34:13.165571Z",
     "iopub.status.idle": "2020-11-12T17:34:13.217359Z",
     "shell.execute_reply": "2020-11-12T17:34:13.215968Z",
     "shell.execute_reply.started": "2020-11-12T17:34:13.165835Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_token(token):\n",
    "    \"\"\"\n",
    "    Peform small modifications to the given token:\n",
    "        - Transform to lowercase\n",
    "        - Encode numbers as \"<num>\"\n",
    "    \"\"\"\n",
    "    token = token.lower()\n",
    "    token = \"<num>\" if re.match(utils.FLOAT_RE, token) else token\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:13.639359Z",
     "iopub.status.busy": "2020-11-12T17:34:13.639058Z",
     "iopub.status.idle": "2020-11-12T17:34:14.221134Z",
     "shell.execute_reply": "2020-11-12T17:34:14.219671Z",
     "shell.execute_reply.started": "2020-11-12T17:34:13.639323Z"
    }
   },
   "outputs": [],
   "source": [
    "whole_tokens, whole_tags = parse_files(whole_files, preprocessor=preprocess_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:14.291167Z",
     "iopub.status.busy": "2020-11-12T17:34:14.290875Z",
     "iopub.status.idle": "2020-11-12T17:34:14.368225Z",
     "shell.execute_reply": "2020-11-12T17:34:14.366821Z",
     "shell.execute_reply.started": "2020-11-12T17:34:14.291130Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(a):\n",
    "    \"\"\"\n",
    "    Given a 2D list, returns its flattened version\n",
    "    \"\"\"\n",
    "    return [i for s in a for i in s]\n",
    "\n",
    "\n",
    "flattened_tags = flatten(whole_tags)\n",
    "flattened_tokens = flatten(whole_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:15.178452Z",
     "iopub.status.busy": "2020-11-12T17:34:15.178153Z",
     "iopub.status.idle": "2020-11-12T17:34:15.265197Z",
     "shell.execute_reply": "2020-11-12T17:34:15.264153Z",
     "shell.execute_reply.started": "2020-11-12T17:34:15.178415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = np.unique(flattened_tags)\n",
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:15.897727Z",
     "iopub.status.busy": "2020-11-12T17:34:15.897427Z",
     "iopub.status.idle": "2020-11-12T17:34:16.077684Z",
     "shell.execute_reply": "2020-11-12T17:34:16.076675Z",
     "shell.execute_reply.started": "2020-11-12T17:34:15.897690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 13166),\n",
       " ('IN', 9857),\n",
       " ('NNP', 9410),\n",
       " ('DT', 8165),\n",
       " ('NNS', 6047),\n",
       " ('JJ', 5834),\n",
       " (',', 4886),\n",
       " ('.', 3874),\n",
       " ('CD', 3546),\n",
       " ('VBD', 3043)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_fd = nltk.probability.FreqDist(flattened_tags)\n",
    "tags_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:17.425939Z",
     "iopub.status.busy": "2020-11-12T17:34:17.425515Z",
     "iopub.status.idle": "2020-11-12T17:34:17.544989Z",
     "shell.execute_reply": "2020-11-12T17:34:17.543752Z",
     "shell.execute_reply.started": "2020-11-12T17:34:17.425897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9964"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = np.unique(flattened_tokens)\n",
    "len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:18.173252Z",
     "iopub.status.busy": "2020-11-12T17:34:18.172953Z",
     "iopub.status.idle": "2020-11-12T17:34:18.360529Z",
     "shell.execute_reply": "2020-11-12T17:34:18.359480Z",
     "shell.execute_reply.started": "2020-11-12T17:34:18.173215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4885),\n",
       " ('the', 4764),\n",
       " ('.', 3828),\n",
       " ('<num>', 2471),\n",
       " ('of', 2325),\n",
       " ('to', 2182),\n",
       " ('a', 1988),\n",
       " ('in', 1769),\n",
       " ('and', 1556),\n",
       " (\"'s\", 865)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_fd = nltk.probability.FreqDist(flattened_tokens)\n",
    "tokens_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:19.575524Z",
     "iopub.status.busy": "2020-11-12T17:34:19.575222Z",
     "iopub.status.idle": "2020-11-12T17:34:19.630167Z",
     "shell.execute_reply": "2020-11-12T17:34:19.628832Z",
     "shell.execute_reply.started": "2020-11-12T17:34:19.575487Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, padding_token=\"0\"):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, builds the corresponding word vocabulary\n",
    "    \"\"\"\n",
    "    words = sorted(set(tokens))\n",
    "    vocabulary, inverse_vocabulary = dict(), dict()\n",
    "    vocabulary[0] = str(padding_token)\n",
    "    inverse_vocabulary[str(padding_token)] = 0\n",
    "    for i, w in tqdm(enumerate(words)):\n",
    "        vocabulary[i + 1] = w\n",
    "        inverse_vocabulary[w] = i + 1\n",
    "    return vocabulary, inverse_vocabulary, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:20.144822Z",
     "iopub.status.busy": "2020-11-12T17:34:20.144522Z",
     "iopub.status.idle": "2020-11-12T17:34:20.197851Z",
     "shell.execute_reply": "2020-11-12T17:34:20.196514Z",
     "shell.execute_reply.started": "2020-11-12T17:34:20.144785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PADDING_TOKEN = \"0\"\n",
    "PADDING_TOKEN in (flattened_tokens, flattened_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:21.484733Z",
     "iopub.status.busy": "2020-11-12T17:34:21.484433Z",
     "iopub.status.idle": "2020-11-12T17:34:21.568143Z",
     "shell.execute_reply": "2020-11-12T17:34:21.566983Z",
     "shell.execute_reply.started": "2020-11-12T17:34:21.484697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9964it [00:00, 635079.55it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_word, word_to_index, word_listing = build_vocabulary(\n",
    "    flattened_tokens, padding_token=PADDING_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:22.027211Z",
     "iopub.status.busy": "2020-11-12T17:34:22.026907Z",
     "iopub.status.idle": "2020-11-12T17:34:22.096757Z",
     "shell.execute_reply": "2020-11-12T17:34:22.095408Z",
     "shell.execute_reply.started": "2020-11-12T17:34:22.027174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0'),\n",
       " (1, '!'),\n",
       " (2, '#'),\n",
       " (3, '$'),\n",
       " (4, '%'),\n",
       " (5, '&'),\n",
       " (6, \"'\"),\n",
       " (7, \"''\"),\n",
       " (8, \"'30s\"),\n",
       " (9, \"'40s\")]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:23.428466Z",
     "iopub.status.busy": "2020-11-12T17:34:23.428160Z",
     "iopub.status.idle": "2020-11-12T17:34:23.490462Z",
     "shell.execute_reply": "2020-11-12T17:34:23.489421Z",
     "shell.execute_reply.started": "2020-11-12T17:34:23.428429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:00, 166001.48it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_tag, tag_to_index, tag_listing = build_vocabulary(\n",
    "    flattened_tags, padding_token=PADDING_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:23.889513Z",
     "iopub.status.busy": "2020-11-12T17:34:23.889212Z",
     "iopub.status.idle": "2020-11-12T17:34:23.944127Z",
     "shell.execute_reply": "2020-11-12T17:34:23.942967Z",
     "shell.execute_reply.started": "2020-11-12T17:34:23.889476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0'),\n",
       " (1, '#'),\n",
       " (2, '$'),\n",
       " (3, \"''\"),\n",
       " (4, ','),\n",
       " (5, '-LRB-'),\n",
       " (6, '-RRB-'),\n",
       " (7, '.'),\n",
       " (8, ':'),\n",
       " (9, 'CC')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_tag.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:20.428894Z",
     "iopub.status.busy": "2020-11-12T18:19:20.428457Z",
     "iopub.status.idle": "2020-11-12T18:19:20.547856Z",
     "shell.execute_reply": "2020-11-12T18:19:20.546542Z",
     "shell.execute_reply.started": "2020-11-12T18:19:20.428852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'NNPS', 'WDT', 'IN', 'JJR', 'NN', 'PRP', 'VBD', 'WP$', 'LS', 'MD', 'NNS', 'VB', '``', 'RB', 'RP', 'RBS', 'CC', 'WP', 'VBG', 'CD', 'POS', 'FW', 'VBP', 'NNP', 'UH', 'VBZ', 'JJS', 'PRP$', '-LRB-', '-RRB-', 'JJ', 'SYM', 'EX', \"''\", 'RBR', 'WRB', 'PDT', 'VBN', 'TO']\n"
     ]
    }
   ],
   "source": [
    "no_punct_tags = list(set(tag_listing) - set(string.punctuation))\n",
    "no_punct_tags_indexes = [tag_to_index[t] for t in no_punct_tags]\n",
    "print(no_punct_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:25.429803Z",
     "iopub.status.busy": "2020-11-12T17:34:25.429500Z",
     "iopub.status.idle": "2020-11-12T17:34:25.483263Z",
     "shell.execute_reply": "2020-11-12T17:34:25.482141Z",
     "shell.execute_reply.started": "2020-11-12T17:34:25.429766Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_indexes(values, to_index):\n",
    "    \"\"\"\n",
    "    Given a list of keys and a dictionary indexed by those keys,\n",
    "    return the corresponding values in the dictionary\n",
    "    \"\"\"\n",
    "    return [to_index[v] for v in values]\n",
    "\n",
    "\n",
    "whole_indexed_tokens = map(\n",
    "    lambda tokens: to_indexes(tokens, word_to_index), whole_tokens\n",
    ")\n",
    "whole_indexed_tags = map(lambda tags: to_indexes(tags, tag_to_index), whole_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:25.855465Z",
     "iopub.status.busy": "2020-11-12T17:34:25.855166Z",
     "iopub.status.idle": "2020-11-12T17:34:26.008522Z",
     "shell.execute_reply": "2020-11-12T17:34:26.007416Z",
     "shell.execute_reply.started": "2020-11-12T17:34:25.855429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>indexed_tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>indexed_tags</th>\n",
       "      <th>split</th>\n",
       "      <th>fileid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pierre, vinken, ,, &lt;num&gt;, years, old, ,, will...</td>\n",
       "      <td>[6562, 9557, 20, 40, 9919, 6130, 20, 9793, 474...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0001.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[rudolph, agnew, ,, &lt;num&gt;, years, old, and, fo...</td>\n",
       "      <td>[7695, 256, 20, 40, 9919, 6130, 394, 3607, 146...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[21, 21, 4, 10, 23, 15, 9, 15, 20, 14, 21, 21,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0002.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[45, 3601, 6095, 570, 6146, 9453, 9063, 5302, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[11, 20, 14, 20, 28, 38, 33, 35, 21, 20, 23, 4...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0003.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[yields, on, money-market, mutual, funds, cont...</td>\n",
       "      <td>[9931, 6144, 5688, 5792, 3725, 1968, 9063, 821...</td>\n",
       "      <td>[NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...</td>\n",
       "      <td>[23, 14, 15, 15, 23, 36, 33, 35, 4, 14, 23, 14...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0004.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[j.p., bolduc, ,, vice, chairman, of, w.r., gr...</td>\n",
       "      <td>[4701, 1024, 20, 9535, 1462, 6095, 9609, 3894,...</td>\n",
       "      <td>[NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...</td>\n",
       "      <td>[21, 21, 4, 20, 20, 14, 21, 21, 9, 21, 4, 41, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0005.dp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [pierre, vinken, ,, <num>, years, old, ,, will...   \n",
       "1  [rudolph, agnew, ,, <num>, years, old, and, fo...   \n",
       "2  [a, form, of, asbestos, once, used, to, make, ...   \n",
       "3  [yields, on, money-market, mutual, funds, cont...   \n",
       "4  [j.p., bolduc, ,, vice, chairman, of, w.r., gr...   \n",
       "\n",
       "                                      indexed_tokens  \\\n",
       "0  [6562, 9557, 20, 40, 9919, 6130, 20, 9793, 474...   \n",
       "1  [7695, 256, 20, 40, 9919, 6130, 394, 3607, 146...   \n",
       "2  [45, 3601, 6095, 570, 6146, 9453, 9063, 5302, ...   \n",
       "3  [9931, 6144, 5688, 5792, 3725, 1968, 9063, 821...   \n",
       "4  [4701, 1024, 20, 9535, 1462, 6095, 9609, 3894,...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "1  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "2  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "3  [NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...   \n",
       "4  [NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...   \n",
       "\n",
       "                                        indexed_tags  split       fileid  \n",
       "0  [21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14,...  train  wsj_0001.dp  \n",
       "1  [21, 21, 4, 10, 23, 15, 9, 15, 20, 14, 21, 21,...  train  wsj_0002.dp  \n",
       "2  [11, 20, 14, 20, 28, 38, 33, 35, 21, 20, 23, 4...  train  wsj_0003.dp  \n",
       "3  [23, 14, 15, 15, 23, 36, 33, 35, 4, 14, 23, 14...  train  wsj_0004.dp  \n",
       "4  [21, 21, 4, 20, 20, 14, 21, 21, 9, 21, 4, 41, ...  train  wsj_0005.dp  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"tokens\": whole_tokens,\n",
    "        \"indexed_tokens\": whole_indexed_tokens,\n",
    "        \"tags\": whole_tags,\n",
    "        \"indexed_tags\": whole_indexed_tags,\n",
    "        \"split\": splits,\n",
    "        \"fileid\": whole_files,\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:27.429873Z",
     "iopub.status.busy": "2020-11-12T17:34:27.429495Z",
     "iopub.status.idle": "2020-11-12T17:34:27.493906Z",
     "shell.execute_reply": "2020-11-12T17:34:27.492180Z",
     "shell.execute_reply.started": "2020-11-12T17:34:27.429832Z"
    }
   },
   "outputs": [],
   "source": [
    "class DependencyTreebankDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dependency treebank dataset for POS tagging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, int)\n",
    "        tokens = self.df.loc[index, \"indexed_tokens\"]\n",
    "        tags = self.df.loc[index, \"indexed_tags\"]\n",
    "        return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:33.425466Z",
     "iopub.status.busy": "2020-11-12T17:34:33.425155Z",
     "iopub.status.idle": "2020-11-12T17:34:33.483153Z",
     "shell.execute_reply": "2020-11-12T17:34:33.481596Z",
     "shell.execute_reply.started": "2020-11-12T17:34:33.425428Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"train\"])\n",
    "val_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"val\"])\n",
    "test_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:34.430280Z",
     "iopub.status.busy": "2020-11-12T17:34:34.429971Z",
     "iopub.status.idle": "2020-11-12T17:34:34.483464Z",
     "shell.execute_reply": "2020-11-12T17:34:34.482194Z",
     "shell.execute_reply.started": "2020-11-12T17:34:34.430243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([6562, 9557, 20, 40, 9919, 6130, 20, 9793, 4749, 8966, 1013, 568, 45, 5988, 2554, 6028, 40, 27, 5755, 9557, 4676, 1462, 6095, 2953, 5808, 20, 8966, 2826, 7018, 3943, 27], [21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14, 11, 15, 20, 21, 10, 7, 21, 21, 40, 20, 14, 21, 21, 4, 11, 21, 37, 20, 7])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:47.029594Z",
     "iopub.status.busy": "2020-11-12T17:34:47.029161Z",
     "iopub.status.idle": "2020-11-12T17:34:47.094014Z",
     "shell.execute_reply": "2020-11-12T17:34:47.092745Z",
     "shell.execute_reply.started": "2020-11-12T17:34:47.029551Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    \"\"\"\n",
    "    This function expects to receive a list of tuples (i.e. a batch),\n",
    "    s.t. each tuple contains tokens and tags for one sentence in the batch\n",
    "    and returns the same sequences padded with the padding token\n",
    "    \"\"\"\n",
    "    (tokens, tags) = zip(*batch)\n",
    "    tokens_lenghts = [len(x) for x in tokens]\n",
    "    tags_lenghts = [len(y) for y in tags]\n",
    "    padded_tokens = pad_sequence(\n",
    "        [torch.tensor(t) for t in tokens],\n",
    "        batch_first=True,\n",
    "        padding_value=int(PADDING_TOKEN),\n",
    "    )\n",
    "    padded_tags = pad_sequence(\n",
    "        [torch.tensor(t) for t in tags],\n",
    "        batch_first=True,\n",
    "        padding_value=int(PADDING_TOKEN),\n",
    "    )\n",
    "    return padded_tokens, padded_tags, tokens_lenghts, tags_lenghts\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "default_dataloader = partial(\n",
    "    DataLoader,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_dataloader = default_dataloader(train_dataset)\n",
    "val_dataloader = default_dataloader(val_dataset)\n",
    "test_dataloader = default_dataloader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:49.427321Z",
     "iopub.status.busy": "2020-11-12T17:34:49.427012Z",
     "iopub.status.idle": "2020-11-12T17:35:40.046475Z",
     "shell.execute_reply": "2020-11-12T17:35:40.045109Z",
     "shell.execute_reply.started": "2020-11-12T17:34:49.427283Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "embedding_model = utils.load_embedding_model(\"glove\", embedding_dimension=embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:35:40.117376Z",
     "iopub.status.busy": "2020-11-12T17:35:40.117075Z",
     "iopub.status.idle": "2020-11-12T17:35:40.169480Z",
     "shell.execute_reply": "2020-11-12T17:35:40.168358Z",
     "shell.execute_reply.started": "2020-11-12T17:35:40.117339Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_oov_terms(embedding_model, word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms\n",
    "    \"\"\"\n",
    "    oov_terms = []\n",
    "    for word in word_listing:\n",
    "        if word not in embedding_model.vocab:\n",
    "            oov_terms.append(word)\n",
    "    return oov_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:35:40.171689Z",
     "iopub.status.busy": "2020-11-12T17:35:40.171386Z",
     "iopub.status.idle": "2020-11-12T17:35:40.231694Z",
     "shell.execute_reply": "2020-11-12T17:35:40.230314Z",
     "shell.execute_reply.started": "2020-11-12T17:35:40.171652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 508 (0.05%)\n"
     ]
    }
   ],
   "source": [
    "oov_terms = check_oov_terms(embedding_model, word_listing)\n",
    "print(\n",
    "    f\"Total OOV terms: {len(oov_terms)} ({round(len(oov_terms) / len(word_listing), 2)}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:06.138240Z",
     "iopub.status.busy": "2020-11-12T17:36:06.137810Z",
     "iopub.status.idle": "2020-11-12T17:36:06.202516Z",
     "shell.execute_reply": "2020-11-12T17:36:06.201244Z",
     "shell.execute_reply.started": "2020-11-12T17:36:06.138196Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_index,\n",
    "    oov_terms,\n",
    "    method=\"normal\",\n",
    "    padding_token=\"0\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained Gensim word embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    def uniform_embedding(embedding_dimension, interval=(-1, 1)):\n",
    "        return interval[0] + np.random.sample(embedding_dimension) + interval[1]\n",
    "\n",
    "    def normal_embedding(embedding_dimension):\n",
    "        return np.random.normal(embedding_dimension)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_index), embedding_dimension))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word == padding_token:\n",
    "            word_vector = np.zeros((1, embedding_dimension))\n",
    "        # Words that are no OOV are taken from the Gensim model\n",
    "        elif word not in oov_terms:\n",
    "            word_vector = embedding_model[word]\n",
    "        # OOV words computed as random normal vectors\n",
    "        elif method == \"normal\":\n",
    "            word_vector = normal_embedding(embedding_dimension)\n",
    "        # OOV words computed as uniform vectors in range [-1, 1]\n",
    "        elif method == \"uniform\":\n",
    "            word_vector = uniform_embedding(embedding_dimension)\n",
    "        embedding_matrix[index, :] = word_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:08.534515Z",
     "iopub.status.busy": "2020-11-12T17:36:08.534082Z",
     "iopub.status.idle": "2020-11-12T17:36:08.895488Z",
     "shell.execute_reply": "2020-11-12T17:36:08.894382Z",
     "shell.execute_reply.started": "2020-11-12T17:36:08.534472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,\n",
       "        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,\n",
       "       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,\n",
       "       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,\n",
       "        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,\n",
       "       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,\n",
       "       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,\n",
       "       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,\n",
       "       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,\n",
       "        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,\n",
       "       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,\n",
       "       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,\n",
       "       -1.15139998e-01, -7.85809994e-01])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_index,\n",
    "    oov_terms,\n",
    "    method=\"normal\",\n",
    "    padding_token=PADDING_TOKEN,\n",
    ")\n",
    "embedding_matrix[word_to_index[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:10.516587Z",
     "iopub.status.busy": "2020-11-12T17:36:10.516284Z",
     "iopub.status.idle": "2020-11-12T17:36:10.571847Z",
     "shell.execute_reply": "2020-11-12T17:36:10.570577Z",
     "shell.execute_reply.started": "2020-11-12T17:36:10.516550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:12.518713Z",
     "iopub.status.busy": "2020-11-12T17:36:12.518407Z",
     "iopub.status.idle": "2020-11-12T17:36:12.581916Z",
     "shell.execute_reply": "2020-11-12T17:36:12.580690Z",
     "shell.execute_reply.started": "2020-11-12T17:36:12.518675Z"
    }
   },
   "outputs": [],
   "source": [
    "class POSTaggingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        embedding_dimension,\n",
    "        hidden_dimension,\n",
    "        output_dimension,\n",
    "        embedding_matrix=None,\n",
    "        retrain_embeddings=True,\n",
    "        gru=True,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        dropout_rate=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a generic POS tagging model, with recurrent modules\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding module\n",
    "        self.embedding = nn.Embedding(\n",
    "            input_dimension, embedding_dimension, padding_idx=0\n",
    "        )\n",
    "        if embedding_matrix is not None:\n",
    "            assert (\n",
    "                embedding_matrix.shape[0] == input_dimension\n",
    "                and embedding_matrix.shape[1] == embedding_dimension\n",
    "            )\n",
    "            self.embedding.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = retrain_embeddings\n",
    "\n",
    "        # Recurrent module\n",
    "        recurrent_module = nn.GRU if gru else nn.LSTM\n",
    "        self.recurrent_module = recurrent_module(\n",
    "            embedding_dimension,\n",
    "            hidden_dimension,\n",
    "            batch_first=True,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Dense and dropout\n",
    "        self.dense = nn.Linear(\n",
    "            hidden_dimension * 2 if bidirectional else hidden_dimension,\n",
    "            output_dimension,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, tokens, tokens_lenghts):\n",
    "        embedded = self.dropout(self.embedding(tokens))\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, tokens_lenghts, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_outputs, (hidden, cell) = self.recurrent_module(packed)\n",
    "        padded_outputs, outputs_lengths = pad_packed_sequence(\n",
    "            packed_outputs, batch_first=True\n",
    "        )\n",
    "        predictions = self.dense(self.dropout(padded_outputs))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:13.490930Z",
     "iopub.status.busy": "2020-11-12T17:36:13.490630Z",
     "iopub.status.idle": "2020-11-12T17:36:13.570607Z",
     "shell.execute_reply": "2020-11-12T17:36:13.569550Z",
     "shell.execute_reply.started": "2020-11-12T17:36:13.490894Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dimension = 128\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.0\n",
    "\n",
    "baseline_model = POSTaggingModel(\n",
    "    len(word_to_index),\n",
    "    embedding_dimension,\n",
    "    hidden_dimension,\n",
    "    len(tag_to_index),\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    retrain_embeddings=True,\n",
    "    gru=False,\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:15.510277Z",
     "iopub.status.busy": "2020-11-12T17:36:15.509964Z",
     "iopub.status.idle": "2020-11-12T17:36:15.563948Z",
     "shell.execute_reply": "2020-11-12T17:36:15.562912Z",
     "shell.execute_reply.started": "2020-11-12T17:36:15.510240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 694,392 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(baseline_model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:16.429189Z",
     "iopub.status.busy": "2020-11-12T17:36:16.428760Z",
     "iopub.status.idle": "2020-11-12T17:36:16.548324Z",
     "shell.execute_reply": "2020-11-12T17:36:16.547122Z",
     "shell.execute_reply.started": "2020-11-12T17:36:16.429145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POSTaggingModel(\n",
       "  (embedding): Embedding(9965, 50, padding_idx=0)\n",
       "  (recurrent_module): LSTM(50, 128, batch_first=True, bidirectional=True)\n",
       "  (dense): Linear(in_features=256, out_features=46, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "\n",
    "baseline_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:17.523016Z",
     "iopub.status.busy": "2020-11-12T17:36:17.522713Z",
     "iopub.status.idle": "2020-11-12T17:36:17.575426Z",
     "shell.execute_reply": "2020-11-12T17:36:17.574249Z",
     "shell.execute_reply.started": "2020-11-12T17:36:17.522979Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(baseline_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:18.427615Z",
     "iopub.status.busy": "2020-11-12T17:36:18.427296Z",
     "iopub.status.idle": "2020-11-12T17:36:18.481810Z",
     "shell.execute_reply": "2020-11-12T17:36:18.480736Z",
     "shell.execute_reply.started": "2020-11-12T17:36:18.427560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model = baseline_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:46:03.428848Z",
     "iopub.status.busy": "2020-11-12T17:46:03.428379Z",
     "iopub.status.idle": "2020-11-12T17:46:03.630955Z",
     "shell.execute_reply": "2020-11-12T17:46:03.629453Z",
     "shell.execute_reply.started": "2020-11-12T17:46:03.428802Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    max_predictions = predictions.argmax(dim=1, keepdim=True)\n",
    "    non_pad_elements = torch.where(ground_truth != 0)[0]\n",
    "    correct = (\n",
    "        max_predictions[non_pad_elements].squeeze(1).eq(ground_truth[non_pad_elements])\n",
    "    )\n",
    "    return correct.sum() / torch.FloatTensor([ground_truth[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:29.563502Z",
     "iopub.status.busy": "2020-11-12T18:19:29.563062Z",
     "iopub.status.idle": "2020-11-12T18:19:29.623189Z",
     "shell.execute_reply": "2020-11-12T18:19:29.622060Z",
     "shell.execute_reply.started": "2020-11-12T18:19:29.563459Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score(predictions, ground_truth, labels):\n",
    "    \"\"\"\n",
    "    Returns F1-macro per batch\n",
    "    \"\"\"\n",
    "    max_predictions = predictions.argmax(dim=1)\n",
    "    non_pad_elements = torch.where(ground_truth != 0)[0]\n",
    "    return sklearn.metrics.f1_score(\n",
    "        ground_truth[non_pad_elements].cpu().detach().tolist(),\n",
    "        max_predictions[non_pad_elements].cpu().detach().tolist(),\n",
    "        labels=labels,\n",
    "        average=\"macro\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:50.983334Z",
     "iopub.status.busy": "2020-11-12T18:19:50.982911Z",
     "iopub.status.idle": "2020-11-12T18:19:51.046466Z",
     "shell.execute_reply": "2020-11-12T18:19:51.045303Z",
     "shell.execute_reply.started": "2020-11-12T18:19:50.983291Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train the given model with the given dataloader, optimizer and criterion\n",
    "    \"\"\"\n",
    "    epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0\n",
    "    model.train()\n",
    "    for tokens, tags, tokens_lenghts, tags_lenghts in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(tokens, tokens_lenghts)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = categorical_accuracy(predictions, tags)\n",
    "        f1 = f1_score(predictions, tags, no_punct_tags_indexes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    return (\n",
    "        epoch_loss / len(dataloader),\n",
    "        epoch_acc / len(dataloader),\n",
    "        epoch_f1 / len(dataloader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:51.518158Z",
     "iopub.status.busy": "2020-11-12T18:19:51.517857Z",
     "iopub.status.idle": "2020-11-12T18:19:51.574172Z",
     "shell.execute_reply": "2020-11-12T18:19:51.573109Z",
     "shell.execute_reply.started": "2020-11-12T18:19:51.518122Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the given model with the given dataloader, optimizer and criterion\n",
    "    \"\"\"\n",
    "    epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tokens, tags, tokens_lenghts, tags_lenghts in tqdm(dataloader):\n",
    "            predictions = model(tokens, tokens_lenghts)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags)\n",
    "            f1 = f1_score(predictions, tags, no_punct_tags_indexes)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1.item()\n",
    "\n",
    "    return (\n",
    "        epoch_loss / len(dataloader),\n",
    "        epoch_acc / len(dataloader),\n",
    "        epoch_f1 / len(dataloader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:55.128530Z",
     "iopub.status.busy": "2020-11-12T18:19:55.128226Z",
     "iopub.status.idle": "2020-11-12T18:19:55.186601Z",
     "shell.execute_reply": "2020-11-12T18:19:55.185471Z",
     "shell.execute_reply.started": "2020-11-12T18:19:55.128493Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_test(model, model_name, epochs=10):\n",
    "    \"\"\"\n",
    "    Perform training, validation and testing on the given model,\n",
    "    for the specified number of epochs\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        # Perform train and validation\n",
    "        print(f\"Epoch: {epoch + 1:02}\")\n",
    "        start_time = time.time()\n",
    "        print(f\"Training...\")\n",
    "        train_loss, train_acc, train_f1 = train(\n",
    "            model, train_dataloader, optimizer, criterion\n",
    "        )\n",
    "        print(f\"Evaluating...\")\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_dataloader, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Print epoch stats\n",
    "        print(f\"Epoch Time: {end_time - start_time}s\")\n",
    "        print(\n",
    "            f\"Train loss: {train_loss:.3f} | Train accuracy: {train_acc * 100:.2f}% | Train F1: {train_f1 * 100:.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Validation loss: {val_loss:.3f} | Validation accuracy: {val_acc * 100:.2f}% | Validation F1: {val_f1 * 100:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"Saving new checkpoint...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"models/{model_name}.pt\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Test the model\n",
    "    model.load_state_dict(torch.load(f\"models/{model_name}.pt\"))\n",
    "    print(\"Testing...\")\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "    print(\n",
    "        f\"Test loss: {test_loss:.3f} | Test accuracy: {test_acc * 100:.2f}% | Test F1: {test_f1 * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:57.430678Z",
     "iopub.status.busy": "2020-11-12T18:19:57.430246Z",
     "iopub.status.idle": "2020-11-12T18:35:28.423102Z",
     "shell.execute_reply": "2020-11-12T18:35:28.421763Z",
     "shell.execute_reply.started": "2020-11-12T18:19:57.430634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:26<00:00,  4.33s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 92.6566071510315s\n",
      "Train loss: 1.261 | Train accuracy: 68.97% | Train F1: 26.91%\n",
      "Validation Loss: 1.184 | Validation accuracy: 71.30% | Validation F1: 32.50%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 02\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:25<00:00,  4.28s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 91.85422563552856s\n",
      "Train loss: 0.949 | Train accuracy: 79.24% | Train F1: 39.56%\n",
      "Validation Loss: 0.959 | Validation accuracy: 78.06% | Validation F1: 41.83%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 03\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:29<00:00,  4.49s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.67it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 95.89124608039856s\n",
      "Train loss: 0.717 | Train accuracy: 85.20% | Train F1: 45.80%\n",
      "Validation Loss: 0.789 | Validation accuracy: 81.43% | Validation F1: 45.53%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 04\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:30<00:00,  4.53s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 96.48748970031738s\n",
      "Train loss: 0.549 | Train accuracy: 88.88% | Train F1: 49.24%\n",
      "Validation Loss: 0.671 | Validation accuracy: 83.50% | Validation F1: 47.76%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 05\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:27<00:00,  4.36s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.69it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 93.24930310249329s\n",
      "Train loss: 0.427 | Train accuracy: 91.57% | Train F1: 52.85%\n",
      "Validation Loss: 0.592 | Validation accuracy: 85.18% | Validation F1: 51.88%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 06\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:29<00:00,  4.46s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.61it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 95.58493089675903s\n",
      "Train loss: 0.336 | Train accuracy: 93.45% | Train F1: 58.30%\n",
      "Validation Loss: 0.536 | Validation accuracy: 86.19% | Validation F1: 55.01%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 07\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:24<00:00,  4.21s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.67it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 90.28789329528809s\n",
      "Train loss: 0.269 | Train accuracy: 94.92% | Train F1: 63.45%\n",
      "Validation Loss: 0.496 | Validation accuracy: 87.14% | Validation F1: 59.28%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 08\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:22<00:00,  4.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.64it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 88.86557984352112s\n",
      "Train loss: 0.220 | Train accuracy: 95.84% | Train F1: 65.78%\n",
      "Validation Loss: 0.466 | Validation accuracy: 87.67% | Validation F1: 61.24%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 09\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:22<00:00,  4.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.68it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 88.8346483707428s\n",
      "Train loss: 0.183 | Train accuracy: 96.62% | Train F1: 68.58%\n",
      "Validation Loss: 0.446 | Validation accuracy: 88.09% | Validation F1: 65.14%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 10\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:27<00:00,  4.39s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:05<00:00,  1.68it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 93.85730004310608s\n",
      "Train loss: 0.155 | Train accuracy: 97.15% | Train F1: 70.27%\n",
      "Validation Loss: 0.435 | Validation accuracy: 88.31% | Validation F1: 66.61%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.377 | Test accuracy: 89.51% | Test F1: 63.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_val_test(baseline_model, \"baseline_model\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

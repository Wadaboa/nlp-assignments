{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3 : Sequence labelling with RNNs\n",
    "In this assignement we will ask you to perform POS tagging.\n",
    "\n",
    "You are asked to follow these steps:\n",
    "\n",
    "- Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "- Embed the words using GloVe embeddings\n",
    "- Create a baseline model, using a simple neural architecture\n",
    "- Experiment doing small modifications to the model\n",
    "- Evaluate your best model\n",
    "- Analyze the errors of your model\n",
    "- Corpora: Ignore the numeric value in the third column, use only the words/symbols and its label. https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
    "\n",
    "Splits: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
    "\n",
    "Baseline: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
    "\n",
    "Modifications: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
    "\n",
    "Training and Experiments: all the experiments must involve only the training and validation sets.\n",
    "\n",
    "Evaluation: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
    "\n",
    "Error Analysis (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
    "\n",
    "Report: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:31:16.959742Z",
     "iopub.status.busy": "2020-11-12T17:31:16.959289Z",
     "iopub.status.idle": "2020-11-12T17:31:17.034536Z",
     "shell.execute_reply": "2020-11-12T17:31:17.032908Z",
     "shell.execute_reply.started": "2020-11-12T17:31:16.959688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "from functools import partial\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import dependency_treebank\n",
    "\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T16:11:38.427960Z",
     "iopub.status.busy": "2020-11-12T16:11:38.427561Z",
     "iopub.status.idle": "2020-11-12T16:11:38.716162Z",
     "shell.execute_reply": "2020-11-12T16:11:38.715085Z",
     "shell.execute_reply.started": "2020-11-12T16:11:38.427919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"dependency_treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:09.673246Z",
     "iopub.status.busy": "2020-11-12T17:34:09.672818Z",
     "iopub.status.idle": "2020-11-12T17:34:09.735332Z",
     "shell.execute_reply": "2020-11-12T17:34:09.734162Z",
     "shell.execute_reply.started": "2020-11-12T17:34:09.673204Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed):\n",
    "    \"\"\"\n",
    "    Fix all the possible sources of randomness\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "fix_random(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:10.179022Z",
     "iopub.status.busy": "2020-11-12T17:34:10.178726Z",
     "iopub.status.idle": "2020-11-12T17:34:10.232985Z",
     "shell.execute_reply": "2020-11-12T17:34:10.231821Z",
     "shell.execute_reply.started": "2020-11-12T17:34:10.178985Z"
    }
   },
   "outputs": [],
   "source": [
    "file_prefix = \"wsj_\"\n",
    "file_ext = \".dp\"\n",
    "train_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(1, 101)]\n",
    "val_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(101, 151)]\n",
    "test_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(151, 200)]\n",
    "splits = (\n",
    "    [\"train\"] * len(train_files) + [\"val\"] * len(val_files) + [\"test\"] * len(test_files)\n",
    ")\n",
    "whole_files = train_files + val_files + test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:10.722948Z",
     "iopub.status.busy": "2020-11-12T17:34:10.722648Z",
     "iopub.status.idle": "2020-11-12T17:34:10.776303Z",
     "shell.execute_reply": "2020-11-12T17:34:10.774925Z",
     "shell.execute_reply.started": "2020-11-12T17:34:10.722911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0001.dp',\n",
       " 'wsj_0021.dp',\n",
       " 'wsj_0041.dp',\n",
       " 'wsj_0061.dp',\n",
       " 'wsj_0081.dp',\n",
       " 'wsj_0101.dp',\n",
       " 'wsj_0121.dp',\n",
       " 'wsj_0141.dp',\n",
       " 'wsj_0161.dp',\n",
       " 'wsj_0181.dp']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_files[0:-1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:11.427088Z",
     "iopub.status.busy": "2020-11-12T17:34:11.426786Z",
     "iopub.status.idle": "2020-11-12T17:34:11.480857Z",
     "shell.execute_reply": "2020-11-12T17:34:11.479661Z",
     "shell.execute_reply.started": "2020-11-12T17:34:11.427051Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre\\tNNP\\t2\\nVinken\\tNNP\\t8\\n,\\t,\\t2\\n61\\tCD\\t5\\nyears\\tNNS\\t6\\nold\\tJJ\\t2\\n,\\t,\\t2\\nwill\\tMD\\t0\\njoin\\tVB\\t8\\nthe\\tDT\\t11\\nboard\\tNN\\t9\\nas\\tIN\\t9\\na\\tDT\\t15\\nnonexecutive\\tJJ\\t15\\ndirector\\tNN\\t12\\nNov.\\tNNP\\t9\\n29\\tCD\\t16\\n.\\t.\\t8\\n\\nMr.\\tNNP\\t2\\nVinken\\tNNP\\t3\\nis\\tVBZ\\t0\\nchairman\\tNN\\t3\\nof\\tIN\\t4\\nElsevier\\tNNP\\t7\\nN.V.\\tNNP\\t12\\n,\\t,\\t12\\nthe\\tDT\\t12\\nDutch\\tNNP\\t12\\npublishing\\tVBG\\t12\\ngroup\\tNN\\t5\\n.\\t.\\t3\\n'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_treebank.raw(fileids=whole_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:11.823053Z",
     "iopub.status.busy": "2020-11-12T17:34:11.822756Z",
     "iopub.status.idle": "2020-11-12T17:34:11.880736Z",
     "shell.execute_reply": "2020-11-12T17:34:11.879699Z",
     "shell.execute_reply.started": "2020-11-12T17:34:11.823017Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_file(fileid, preprocessor=None):\n",
    "    \"\"\"\n",
    "    Parse the given file identifier from the dependency treebank corpus\n",
    "    and return a tuple (`tokens`, `tags`), where `tokens` is the list\n",
    "    of tokens retrieved in the document and `tags` is the associated\n",
    "    list of tags (`tokens` and `tags` have the same lenght)\n",
    "    \n",
    "    If you wish to preprocess tokens, you can pass a function to the\n",
    "    `preprocessor` argument, which should take as input only the token\n",
    "    to transform\n",
    "    \"\"\"\n",
    "    file_str = dependency_treebank.raw(fileids=fileid)\n",
    "    splitted_file_str = [\n",
    "        x for x in re.split(\"\\t|\\n\", file_str.strip()) if x.strip() != \"\"\n",
    "    ]\n",
    "    if preprocessor is None:\n",
    "        preprocessor = lambda t: t\n",
    "    tokens, tags = [], []\n",
    "    for i in range(0, len(splitted_file_str), 3):\n",
    "        token = preprocessor(splitted_file_str[i])\n",
    "        tag = splitted_file_str[i + 1]\n",
    "        tokens.append(token)\n",
    "        tags.append(tag)\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def parse_files(fileids, preprocessor=None):\n",
    "    \"\"\"\n",
    "    Parse a set of file identifiers from the dependency treebank corpus\n",
    "    and return two lists, one which contains lists of tokens and one \n",
    "    containing lists of corresponding tags for each file identifier\n",
    "    \"\"\"\n",
    "    tokens_list, tags_list = [], []\n",
    "    for fileid in fileids:\n",
    "        tokens, tags = parse_file(fileid, preprocessor=preprocessor)\n",
    "        tokens_list.append(tokens)\n",
    "        tags_list.append(tags)\n",
    "    return tokens_list, tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:13.165872Z",
     "iopub.status.busy": "2020-11-12T17:34:13.165571Z",
     "iopub.status.idle": "2020-11-12T17:34:13.217359Z",
     "shell.execute_reply": "2020-11-12T17:34:13.215968Z",
     "shell.execute_reply.started": "2020-11-12T17:34:13.165835Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_token(token):\n",
    "    \"\"\"\n",
    "    Peform small modifications to the given token:\n",
    "        - Transform to lowercase\n",
    "        - Encode numbers as \"<num>\"\n",
    "    \"\"\"\n",
    "    token = token.lower()\n",
    "    token = \"<num>\" if re.match(utils.FLOAT_RE, token) else token\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:13.639359Z",
     "iopub.status.busy": "2020-11-12T17:34:13.639058Z",
     "iopub.status.idle": "2020-11-12T17:34:14.221134Z",
     "shell.execute_reply": "2020-11-12T17:34:14.219671Z",
     "shell.execute_reply.started": "2020-11-12T17:34:13.639323Z"
    }
   },
   "outputs": [],
   "source": [
    "whole_tokens, whole_tags = parse_files(whole_files, preprocessor=preprocess_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:14.291167Z",
     "iopub.status.busy": "2020-11-12T17:34:14.290875Z",
     "iopub.status.idle": "2020-11-12T17:34:14.368225Z",
     "shell.execute_reply": "2020-11-12T17:34:14.366821Z",
     "shell.execute_reply.started": "2020-11-12T17:34:14.291130Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(a):\n",
    "    \"\"\"\n",
    "    Given a 2D list, returns its flattened version\n",
    "    \"\"\"\n",
    "    return [i for s in a for i in s]\n",
    "\n",
    "\n",
    "flattened_tags = flatten(whole_tags)\n",
    "flattened_tokens = flatten(whole_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:15.178452Z",
     "iopub.status.busy": "2020-11-12T17:34:15.178153Z",
     "iopub.status.idle": "2020-11-12T17:34:15.265197Z",
     "shell.execute_reply": "2020-11-12T17:34:15.264153Z",
     "shell.execute_reply.started": "2020-11-12T17:34:15.178415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = np.unique(flattened_tags)\n",
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:15.897727Z",
     "iopub.status.busy": "2020-11-12T17:34:15.897427Z",
     "iopub.status.idle": "2020-11-12T17:34:16.077684Z",
     "shell.execute_reply": "2020-11-12T17:34:16.076675Z",
     "shell.execute_reply.started": "2020-11-12T17:34:15.897690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 13166),\n",
       " ('IN', 9857),\n",
       " ('NNP', 9410),\n",
       " ('DT', 8165),\n",
       " ('NNS', 6047),\n",
       " ('JJ', 5834),\n",
       " (',', 4886),\n",
       " ('.', 3874),\n",
       " ('CD', 3546),\n",
       " ('VBD', 3043)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_fd = nltk.probability.FreqDist(flattened_tags)\n",
    "tags_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:17.425939Z",
     "iopub.status.busy": "2020-11-12T17:34:17.425515Z",
     "iopub.status.idle": "2020-11-12T17:34:17.544989Z",
     "shell.execute_reply": "2020-11-12T17:34:17.543752Z",
     "shell.execute_reply.started": "2020-11-12T17:34:17.425897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9964"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = np.unique(flattened_tokens)\n",
    "len(unique_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:18.173252Z",
     "iopub.status.busy": "2020-11-12T17:34:18.172953Z",
     "iopub.status.idle": "2020-11-12T17:34:18.360529Z",
     "shell.execute_reply": "2020-11-12T17:34:18.359480Z",
     "shell.execute_reply.started": "2020-11-12T17:34:18.173215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4885),\n",
       " ('the', 4764),\n",
       " ('.', 3828),\n",
       " ('<num>', 2471),\n",
       " ('of', 2325),\n",
       " ('to', 2182),\n",
       " ('a', 1988),\n",
       " ('in', 1769),\n",
       " ('and', 1556),\n",
       " (\"'s\", 865)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_fd = nltk.probability.FreqDist(flattened_tokens)\n",
    "tokens_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:19.575524Z",
     "iopub.status.busy": "2020-11-12T17:34:19.575222Z",
     "iopub.status.idle": "2020-11-12T17:34:19.630167Z",
     "shell.execute_reply": "2020-11-12T17:34:19.628832Z",
     "shell.execute_reply.started": "2020-11-12T17:34:19.575487Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, padding_token=\"0\"):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, builds the corresponding word vocabulary\n",
    "    \"\"\"\n",
    "    words = sorted(set(tokens))\n",
    "    vocabulary, inverse_vocabulary = dict(), dict()\n",
    "    vocabulary[0] = str(padding_token)\n",
    "    inverse_vocabulary[str(padding_token)] = 0\n",
    "    for i, w in tqdm(enumerate(words)):\n",
    "        vocabulary[i + 1] = w\n",
    "        inverse_vocabulary[w] = i + 1\n",
    "    return vocabulary, inverse_vocabulary, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:20.144822Z",
     "iopub.status.busy": "2020-11-12T17:34:20.144522Z",
     "iopub.status.idle": "2020-11-12T17:34:20.197851Z",
     "shell.execute_reply": "2020-11-12T17:34:20.196514Z",
     "shell.execute_reply.started": "2020-11-12T17:34:20.144785Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PADDING_TOKEN = \"0\"\n",
    "PADDING_TOKEN in (flattened_tokens, flattened_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:21.484733Z",
     "iopub.status.busy": "2020-11-12T17:34:21.484433Z",
     "iopub.status.idle": "2020-11-12T17:34:21.568143Z",
     "shell.execute_reply": "2020-11-12T17:34:21.566983Z",
     "shell.execute_reply.started": "2020-11-12T17:34:21.484697Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9964it [00:00, 635079.55it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_word, word_to_index, word_listing = build_vocabulary(\n",
    "    flattened_tokens, padding_token=PADDING_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:22.027211Z",
     "iopub.status.busy": "2020-11-12T17:34:22.026907Z",
     "iopub.status.idle": "2020-11-12T17:34:22.096757Z",
     "shell.execute_reply": "2020-11-12T17:34:22.095408Z",
     "shell.execute_reply.started": "2020-11-12T17:34:22.027174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0'),\n",
       " (1, '!'),\n",
       " (2, '#'),\n",
       " (3, '$'),\n",
       " (4, '%'),\n",
       " (5, '&'),\n",
       " (6, \"'\"),\n",
       " (7, \"''\"),\n",
       " (8, \"'30s\"),\n",
       " (9, \"'40s\")]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:23.428466Z",
     "iopub.status.busy": "2020-11-12T17:34:23.428160Z",
     "iopub.status.idle": "2020-11-12T17:34:23.490462Z",
     "shell.execute_reply": "2020-11-12T17:34:23.489421Z",
     "shell.execute_reply.started": "2020-11-12T17:34:23.428429Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:00, 166001.48it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_tag, tag_to_index, tag_listing = build_vocabulary(\n",
    "    flattened_tags, padding_token=PADDING_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:23.889513Z",
     "iopub.status.busy": "2020-11-12T17:34:23.889212Z",
     "iopub.status.idle": "2020-11-12T17:34:23.944127Z",
     "shell.execute_reply": "2020-11-12T17:34:23.942967Z",
     "shell.execute_reply.started": "2020-11-12T17:34:23.889476Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0'),\n",
       " (1, '#'),\n",
       " (2, '$'),\n",
       " (3, \"''\"),\n",
       " (4, ','),\n",
       " (5, '-LRB-'),\n",
       " (6, '-RRB-'),\n",
       " (7, '.'),\n",
       " (8, ':'),\n",
       " (9, 'CC')]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_tag.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:20.428894Z",
     "iopub.status.busy": "2020-11-12T18:19:20.428457Z",
     "iopub.status.idle": "2020-11-12T18:19:20.547856Z",
     "shell.execute_reply": "2020-11-12T18:19:20.546542Z",
     "shell.execute_reply.started": "2020-11-12T18:19:20.428852Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DT', 'NNPS', 'WDT', 'IN', 'JJR', 'NN', 'PRP', 'VBD', 'WP$', 'LS', 'MD', 'NNS', 'VB', '``', 'RB', 'RP', 'RBS', 'CC', 'WP', 'VBG', 'CD', 'POS', 'FW', 'VBP', 'NNP', 'UH', 'VBZ', 'JJS', 'PRP$', '-LRB-', '-RRB-', 'JJ', 'SYM', 'EX', \"''\", 'RBR', 'WRB', 'PDT', 'VBN', 'TO']\n"
     ]
    }
   ],
   "source": [
    "no_punct_tags = list(set(tag_listing) - set(string.punctuation))\n",
    "no_punct_tags_indexes = [tag_to_index[t] for t in no_punct_tags]\n",
    "print(no_punct_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:25.429803Z",
     "iopub.status.busy": "2020-11-12T17:34:25.429500Z",
     "iopub.status.idle": "2020-11-12T17:34:25.483263Z",
     "shell.execute_reply": "2020-11-12T17:34:25.482141Z",
     "shell.execute_reply.started": "2020-11-12T17:34:25.429766Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_indexes(values, to_index):\n",
    "    \"\"\"\n",
    "    Given a list of keys and a dictionary indexed by those keys,\n",
    "    return the corresponding values in the dictionary\n",
    "    \"\"\"\n",
    "    return [to_index[v] for v in values]\n",
    "\n",
    "\n",
    "whole_indexed_tokens = map(\n",
    "    lambda tokens: to_indexes(tokens, word_to_index), whole_tokens\n",
    ")\n",
    "whole_indexed_tags = map(lambda tags: to_indexes(tags, tag_to_index), whole_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:25.855465Z",
     "iopub.status.busy": "2020-11-12T17:34:25.855166Z",
     "iopub.status.idle": "2020-11-12T17:34:26.008522Z",
     "shell.execute_reply": "2020-11-12T17:34:26.007416Z",
     "shell.execute_reply.started": "2020-11-12T17:34:25.855429Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>indexed_tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>indexed_tags</th>\n",
       "      <th>split</th>\n",
       "      <th>fileid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pierre, vinken, ,, &lt;num&gt;, years, old, ,, will...</td>\n",
       "      <td>[6562, 9557, 20, 40, 9919, 6130, 20, 9793, 474...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0001.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[rudolph, agnew, ,, &lt;num&gt;, years, old, and, fo...</td>\n",
       "      <td>[7695, 256, 20, 40, 9919, 6130, 394, 3607, 146...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[21, 21, 4, 10, 23, 15, 9, 15, 20, 14, 21, 21,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0002.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[a, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[45, 3601, 6095, 570, 6146, 9453, 9063, 5302, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[11, 20, 14, 20, 28, 38, 33, 35, 21, 20, 23, 4...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0003.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[yields, on, money-market, mutual, funds, cont...</td>\n",
       "      <td>[9931, 6144, 5688, 5792, 3725, 1968, 9063, 821...</td>\n",
       "      <td>[NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...</td>\n",
       "      <td>[23, 14, 15, 15, 23, 36, 33, 35, 4, 14, 23, 14...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0004.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[j.p., bolduc, ,, vice, chairman, of, w.r., gr...</td>\n",
       "      <td>[4701, 1024, 20, 9535, 1462, 6095, 9609, 3894,...</td>\n",
       "      <td>[NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...</td>\n",
       "      <td>[21, 21, 4, 20, 20, 14, 21, 21, 9, 21, 4, 41, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0005.dp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [pierre, vinken, ,, <num>, years, old, ,, will...   \n",
       "1  [rudolph, agnew, ,, <num>, years, old, and, fo...   \n",
       "2  [a, form, of, asbestos, once, used, to, make, ...   \n",
       "3  [yields, on, money-market, mutual, funds, cont...   \n",
       "4  [j.p., bolduc, ,, vice, chairman, of, w.r., gr...   \n",
       "\n",
       "                                      indexed_tokens  \\\n",
       "0  [6562, 9557, 20, 40, 9919, 6130, 20, 9793, 474...   \n",
       "1  [7695, 256, 20, 40, 9919, 6130, 394, 3607, 146...   \n",
       "2  [45, 3601, 6095, 570, 6146, 9453, 9063, 5302, ...   \n",
       "3  [9931, 6144, 5688, 5792, 3725, 1968, 9063, 821...   \n",
       "4  [4701, 1024, 20, 9535, 1462, 6095, 9609, 3894,...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "1  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "2  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "3  [NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...   \n",
       "4  [NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...   \n",
       "\n",
       "                                        indexed_tags  split       fileid  \n",
       "0  [21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14,...  train  wsj_0001.dp  \n",
       "1  [21, 21, 4, 10, 23, 15, 9, 15, 20, 14, 21, 21,...  train  wsj_0002.dp  \n",
       "2  [11, 20, 14, 20, 28, 38, 33, 35, 21, 20, 23, 4...  train  wsj_0003.dp  \n",
       "3  [23, 14, 15, 15, 23, 36, 33, 35, 4, 14, 23, 14...  train  wsj_0004.dp  \n",
       "4  [21, 21, 4, 20, 20, 14, 21, 21, 9, 21, 4, 41, ...  train  wsj_0005.dp  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"tokens\": whole_tokens,\n",
    "        \"indexed_tokens\": whole_indexed_tokens,\n",
    "        \"tags\": whole_tags,\n",
    "        \"indexed_tags\": whole_indexed_tags,\n",
    "        \"split\": splits,\n",
    "        \"fileid\": whole_files,\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:27.429873Z",
     "iopub.status.busy": "2020-11-12T17:34:27.429495Z",
     "iopub.status.idle": "2020-11-12T17:34:27.493906Z",
     "shell.execute_reply": "2020-11-12T17:34:27.492180Z",
     "shell.execute_reply.started": "2020-11-12T17:34:27.429832Z"
    }
   },
   "outputs": [],
   "source": [
    "class DependencyTreebankDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dependency treebank dataset for POS tagging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert isinstance(index, int)\n",
    "        tokens = self.df.loc[index, \"indexed_tokens\"]\n",
    "        tags = self.df.loc[index, \"indexed_tags\"]\n",
    "        return tokens, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:33.425466Z",
     "iopub.status.busy": "2020-11-12T17:34:33.425155Z",
     "iopub.status.idle": "2020-11-12T17:34:33.483153Z",
     "shell.execute_reply": "2020-11-12T17:34:33.481596Z",
     "shell.execute_reply.started": "2020-11-12T17:34:33.425428Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"train\"])\n",
    "val_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"val\"])\n",
    "test_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:34.430280Z",
     "iopub.status.busy": "2020-11-12T17:34:34.429971Z",
     "iopub.status.idle": "2020-11-12T17:34:34.483464Z",
     "shell.execute_reply": "2020-11-12T17:34:34.482194Z",
     "shell.execute_reply.started": "2020-11-12T17:34:34.430243Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([6562, 9557, 20, 40, 9919, 6130, 20, 9793, 4749, 8966, 1013, 568, 45, 5988, 2554, 6028, 40, 27, 5755, 9557, 4676, 1462, 6095, 2953, 5808, 20, 8966, 2826, 7018, 3943, 27], [21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14, 11, 15, 20, 21, 10, 7, 21, 21, 40, 20, 14, 21, 21, 4, 11, 21, 37, 20, 7])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:47.029594Z",
     "iopub.status.busy": "2020-11-12T17:34:47.029161Z",
     "iopub.status.idle": "2020-11-12T17:34:47.094014Z",
     "shell.execute_reply": "2020-11-12T17:34:47.092745Z",
     "shell.execute_reply.started": "2020-11-12T17:34:47.029551Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    \"\"\"\n",
    "    This function expects to receive a list of tuples (i.e. a batch),\n",
    "    s.t. each tuple contains tokens and tags for one sentence in the batch\n",
    "    and returns the same sequences padded with the padding token\n",
    "    \"\"\"\n",
    "    (tokens, tags) = zip(*batch)\n",
    "    tokens_lenghts = [len(x) for x in tokens]\n",
    "    tags_lenghts = [len(y) for y in tags]\n",
    "    padded_tokens = pad_sequence(\n",
    "        [torch.tensor(t) for t in tokens],\n",
    "        batch_first=True,\n",
    "        padding_value=int(PADDING_TOKEN),\n",
    "    )\n",
    "    padded_tags = pad_sequence(\n",
    "        [torch.tensor(t) for t in tags],\n",
    "        batch_first=True,\n",
    "        padding_value=int(PADDING_TOKEN),\n",
    "    )\n",
    "    return padded_tokens, padded_tags, tokens_lenghts, tags_lenghts\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "default_dataloader = partial(\n",
    "    DataLoader,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_dataloader = default_dataloader(train_dataset)\n",
    "val_dataloader = default_dataloader(val_dataset)\n",
    "test_dataloader = default_dataloader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:34:49.427321Z",
     "iopub.status.busy": "2020-11-12T17:34:49.427012Z",
     "iopub.status.idle": "2020-11-12T17:35:40.046475Z",
     "shell.execute_reply": "2020-11-12T17:35:40.045109Z",
     "shell.execute_reply.started": "2020-11-12T17:34:49.427283Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "embedding_model = utils.load_embedding_model(\"glove\", embedding_dimension=embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:35:40.117376Z",
     "iopub.status.busy": "2020-11-12T17:35:40.117075Z",
     "iopub.status.idle": "2020-11-12T17:35:40.169480Z",
     "shell.execute_reply": "2020-11-12T17:35:40.168358Z",
     "shell.execute_reply.started": "2020-11-12T17:35:40.117339Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_oov_terms(embedding_model, word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms\n",
    "    \"\"\"\n",
    "    oov_terms = []\n",
    "    for word in word_listing:\n",
    "        if word not in embedding_model.vocab:\n",
    "            oov_terms.append(word)\n",
    "    return oov_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:35:40.171689Z",
     "iopub.status.busy": "2020-11-12T17:35:40.171386Z",
     "iopub.status.idle": "2020-11-12T17:35:40.231694Z",
     "shell.execute_reply": "2020-11-12T17:35:40.230314Z",
     "shell.execute_reply.started": "2020-11-12T17:35:40.171652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 508 (0.05%)\n"
     ]
    }
   ],
   "source": [
    "oov_terms = check_oov_terms(embedding_model, word_listing)\n",
    "print(\n",
    "    f\"Total OOV terms: {len(oov_terms)} ({round(len(oov_terms) / len(word_listing), 2)}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:06.138240Z",
     "iopub.status.busy": "2020-11-12T17:36:06.137810Z",
     "iopub.status.idle": "2020-11-12T17:36:06.202516Z",
     "shell.execute_reply": "2020-11-12T17:36:06.201244Z",
     "shell.execute_reply.started": "2020-11-12T17:36:06.138196Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_index,\n",
    "    oov_terms,\n",
    "    method=\"normal\",\n",
    "    padding_token=\"0\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained Gensim word embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    def uniform_embedding(embedding_dimension, interval=(-1, 1)):\n",
    "        return interval[0] + np.random.sample(embedding_dimension) + interval[1]\n",
    "\n",
    "    def normal_embedding(embedding_dimension):\n",
    "        return np.random.normal(embedding_dimension)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_index), embedding_dimension))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word == padding_token:\n",
    "            word_vector = np.zeros((1, embedding_dimension))\n",
    "        # Words that are no OOV are taken from the Gensim model\n",
    "        elif word not in oov_terms:\n",
    "            word_vector = embedding_model[word]\n",
    "        # OOV words computed as random normal vectors\n",
    "        elif method == \"normal\":\n",
    "            word_vector = normal_embedding(embedding_dimension)\n",
    "        # OOV words computed as uniform vectors in range [-1, 1]\n",
    "        elif method == \"uniform\":\n",
    "            word_vector = uniform_embedding(embedding_dimension)\n",
    "        embedding_matrix[index, :] = word_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:08.534515Z",
     "iopub.status.busy": "2020-11-12T17:36:08.534082Z",
     "iopub.status.idle": "2020-11-12T17:36:08.895488Z",
     "shell.execute_reply": "2020-11-12T17:36:08.894382Z",
     "shell.execute_reply.started": "2020-11-12T17:36:08.534472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,\n",
       "        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,\n",
       "       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,\n",
       "       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,\n",
       "        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,\n",
       "       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,\n",
       "       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,\n",
       "       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,\n",
       "       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,\n",
       "        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,\n",
       "       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,\n",
       "       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,\n",
       "       -1.15139998e-01, -7.85809994e-01])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_index,\n",
    "    oov_terms,\n",
    "    method=\"normal\",\n",
    "    padding_token=PADDING_TOKEN,\n",
    ")\n",
    "embedding_matrix[word_to_index[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:10.516587Z",
     "iopub.status.busy": "2020-11-12T17:36:10.516284Z",
     "iopub.status.idle": "2020-11-12T17:36:10.571847Z",
     "shell.execute_reply": "2020-11-12T17:36:10.570577Z",
     "shell.execute_reply.started": "2020-11-12T17:36:10.516550Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:12.518713Z",
     "iopub.status.busy": "2020-11-12T17:36:12.518407Z",
     "iopub.status.idle": "2020-11-12T17:36:12.581916Z",
     "shell.execute_reply": "2020-11-12T17:36:12.580690Z",
     "shell.execute_reply.started": "2020-11-12T17:36:12.518675Z"
    }
   },
   "outputs": [],
   "source": [
    "class POSTaggingModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        embedding_dimension,\n",
    "        hidden_dimension,\n",
    "        output_dimension,\n",
    "        embedding_matrix=None,\n",
    "        retrain_embeddings=True,\n",
    "        gru=True,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        dropout_rate=0.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Build a generic POS tagging model, with recurrent modules\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding module\n",
    "        self.embedding = nn.Embedding(\n",
    "            input_dimension, embedding_dimension, padding_idx=0\n",
    "        )\n",
    "        if embedding_matrix is not None:\n",
    "            assert (\n",
    "                embedding_matrix.shape[0] == input_dimension\n",
    "                and embedding_matrix.shape[1] == embedding_dimension\n",
    "            )\n",
    "            self.embedding.weight = nn.Parameter(torch.FloatTensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = retrain_embeddings\n",
    "\n",
    "        # Recurrent module\n",
    "        recurrent_module = nn.GRU if gru else nn.LSTM\n",
    "        self.recurrent_module = recurrent_module(\n",
    "            embedding_dimension,\n",
    "            hidden_dimension,\n",
    "            batch_first=True,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        # Dense and dropout\n",
    "        self.dense = nn.Linear(\n",
    "            hidden_dimension * 2 if bidirectional else hidden_dimension,\n",
    "            output_dimension,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, tokens, tokens_lenghts):\n",
    "        embedded = self.dropout(self.embedding(tokens))\n",
    "        packed = pack_padded_sequence(\n",
    "            embedded, tokens_lenghts, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_outputs, (hidden, cell) = self.recurrent_module(packed)\n",
    "        padded_outputs, outputs_lengths = pad_packed_sequence(\n",
    "            packed_outputs, batch_first=True\n",
    "        )\n",
    "        predictions = self.dense(self.dropout(padded_outputs))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:13.490930Z",
     "iopub.status.busy": "2020-11-12T17:36:13.490630Z",
     "iopub.status.idle": "2020-11-12T17:36:13.570607Z",
     "shell.execute_reply": "2020-11-12T17:36:13.569550Z",
     "shell.execute_reply.started": "2020-11-12T17:36:13.490894Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dimension = 128\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.0\n",
    "\n",
    "baseline_model = POSTaggingModel(\n",
    "    len(word_to_index),\n",
    "    embedding_dimension,\n",
    "    hidden_dimension,\n",
    "    len(tag_to_index),\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    retrain_embeddings=True,\n",
    "    gru=False,\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:15.510277Z",
     "iopub.status.busy": "2020-11-12T17:36:15.509964Z",
     "iopub.status.idle": "2020-11-12T17:36:15.563948Z",
     "shell.execute_reply": "2020-11-12T17:36:15.562912Z",
     "shell.execute_reply.started": "2020-11-12T17:36:15.510240Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 694,392 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(baseline_model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:16.429189Z",
     "iopub.status.busy": "2020-11-12T17:36:16.428760Z",
     "iopub.status.idle": "2020-11-12T17:36:16.548324Z",
     "shell.execute_reply": "2020-11-12T17:36:16.547122Z",
     "shell.execute_reply.started": "2020-11-12T17:36:16.429145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POSTaggingModel(\n",
       "  (embedding): Embedding(9965, 50, padding_idx=0)\n",
       "  (recurrent_module): LSTM(50, 128, batch_first=True, bidirectional=True)\n",
       "  (dense): Linear(in_features=256, out_features=46, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "\n",
    "baseline_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:17.523016Z",
     "iopub.status.busy": "2020-11-12T17:36:17.522713Z",
     "iopub.status.idle": "2020-11-12T17:36:17.575426Z",
     "shell.execute_reply": "2020-11-12T17:36:17.574249Z",
     "shell.execute_reply.started": "2020-11-12T17:36:17.522979Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(baseline_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:36:18.427615Z",
     "iopub.status.busy": "2020-11-12T17:36:18.427296Z",
     "iopub.status.idle": "2020-11-12T17:36:18.481810Z",
     "shell.execute_reply": "2020-11-12T17:36:18.480736Z",
     "shell.execute_reply.started": "2020-11-12T17:36:18.427560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "baseline_model = baseline_model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T17:46:03.428848Z",
     "iopub.status.busy": "2020-11-12T17:46:03.428379Z",
     "iopub.status.idle": "2020-11-12T17:46:03.630955Z",
     "shell.execute_reply": "2020-11-12T17:46:03.629453Z",
     "shell.execute_reply.started": "2020-11-12T17:46:03.428802Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    max_predictions = predictions.argmax(dim=1, keepdim=True)\n",
    "    non_pad_elements = torch.where(ground_truth != 0)[0]\n",
    "    correct = (\n",
    "        max_predictions[non_pad_elements].squeeze(1).eq(ground_truth[non_pad_elements])\n",
    "    )\n",
    "    return correct.sum() / torch.FloatTensor([ground_truth[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:29.563502Z",
     "iopub.status.busy": "2020-11-12T18:19:29.563062Z",
     "iopub.status.idle": "2020-11-12T18:19:29.623189Z",
     "shell.execute_reply": "2020-11-12T18:19:29.622060Z",
     "shell.execute_reply.started": "2020-11-12T18:19:29.563459Z"
    }
   },
   "outputs": [],
   "source": [
    "def f1_score(predictions, ground_truth, labels):\n",
    "    \"\"\"\n",
    "    Returns F1-macro per batch\n",
    "    \"\"\"\n",
    "    max_predictions = predictions.argmax(dim=1)\n",
    "    non_pad_elements = torch.where(ground_truth != 0)[0]\n",
    "    return sklearn.metrics.f1_score(\n",
    "        ground_truth[non_pad_elements].cpu().detach().tolist(),\n",
    "        max_predictions[non_pad_elements].cpu().detach().tolist(),\n",
    "        labels=labels,\n",
    "        average=\"macro\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:50.983334Z",
     "iopub.status.busy": "2020-11-12T18:19:50.982911Z",
     "iopub.status.idle": "2020-11-12T18:19:51.046466Z",
     "shell.execute_reply": "2020-11-12T18:19:51.045303Z",
     "shell.execute_reply.started": "2020-11-12T18:19:50.983291Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    \"\"\"\n",
    "    Train the given model with the given dataloader, optimizer and criterion\n",
    "    \"\"\"\n",
    "    epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0\n",
    "    model.train()\n",
    "    for tokens, tags, tokens_lenghts, tags_lenghts in tqdm(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(tokens, tokens_lenghts)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = categorical_accuracy(predictions, tags)\n",
    "        f1 = f1_score(predictions, tags, no_punct_tags_indexes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()\n",
    "\n",
    "    return (\n",
    "        epoch_loss / len(dataloader),\n",
    "        epoch_acc / len(dataloader),\n",
    "        epoch_f1 / len(dataloader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:51.518158Z",
     "iopub.status.busy": "2020-11-12T18:19:51.517857Z",
     "iopub.status.idle": "2020-11-12T18:19:51.574172Z",
     "shell.execute_reply": "2020-11-12T18:19:51.573109Z",
     "shell.execute_reply.started": "2020-11-12T18:19:51.518122Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the given model with the given dataloader, optimizer and criterion\n",
    "    \"\"\"\n",
    "    epoch_loss, epoch_acc, epoch_f1 = 0, 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tokens, tags, tokens_lenghts, tags_lenghts in tqdm(dataloader):\n",
    "            predictions = model(tokens, tokens_lenghts)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags)\n",
    "            f1 = f1_score(predictions, tags, no_punct_tags_indexes)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1.item()\n",
    "\n",
    "    return (\n",
    "        epoch_loss / len(dataloader),\n",
    "        epoch_acc / len(dataloader),\n",
    "        epoch_f1 / len(dataloader),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:55.128530Z",
     "iopub.status.busy": "2020-11-12T18:19:55.128226Z",
     "iopub.status.idle": "2020-11-12T18:19:55.186601Z",
     "shell.execute_reply": "2020-11-12T18:19:55.185471Z",
     "shell.execute_reply.started": "2020-11-12T18:19:55.128493Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_val_test(model, model_name, epochs=10):\n",
    "    \"\"\"\n",
    "    Perform training, validation and testing on the given model,\n",
    "    for the specified number of epochs\n",
    "    \"\"\"\n",
    "    best_val_loss = float(\"inf\")\n",
    "    for epoch in range(epochs):\n",
    "        # Perform train and validation\n",
    "        print(f\"Epoch: {epoch + 1:02}\")\n",
    "        start_time = time.time()\n",
    "        print(f\"Training...\")\n",
    "        train_loss, train_acc, train_f1 = train(\n",
    "            model, train_dataloader, optimizer, criterion\n",
    "        )\n",
    "        print(f\"Evaluating...\")\n",
    "        val_loss, val_acc, val_f1 = evaluate(model, val_dataloader, criterion)\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Print epoch stats\n",
    "        print(f\"Epoch Time: {end_time - start_time}s\")\n",
    "        print(\n",
    "            f\"Train loss: {train_loss:.3f} | Train accuracy: {train_acc * 100:.2f}% | Train F1: {train_f1 * 100:.2f}%\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Validation loss: {val_loss:.3f} | Validation accuracy: {val_acc * 100:.2f}% | Validation F1: {val_f1 * 100:.2f}%\"\n",
    "        )\n",
    "\n",
    "        # Save the best model so far\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"Saving new checkpoint...\")\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"models/{model_name}.pt\")\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Test the model\n",
    "    model.load_state_dict(torch.load(f\"models/{model_name}.pt\"))\n",
    "    print(\"Testing...\")\n",
    "    test_loss, test_acc, test_f1 = evaluate(model, test_dataloader, criterion)\n",
    "    print(\n",
    "        f\"Test loss: {test_loss:.3f} | Test accuracy: {test_acc * 100:.2f}% | Test F1: {test_f1 * 100:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T18:19:57.430678Z",
     "iopub.status.busy": "2020-11-12T18:19:57.430246Z",
     "iopub.status.idle": "2020-11-12T18:35:28.423102Z",
     "shell.execute_reply": "2020-11-12T18:35:28.421763Z",
     "shell.execute_reply.started": "2020-11-12T18:19:57.430634Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:26<00:00,  4.33s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.69it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 92.6566071510315s\n",
      "Train loss: 1.261 | Train accuracy: 68.97% | Train F1: 26.91%\n",
      "Validation Loss: 1.184 | Validation accuracy: 71.30% | Validation F1: 32.50%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 02\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:25<00:00,  4.28s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:06<00:00,  1.64it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 91.85422563552856s\n",
      "Train loss: 0.949 | Train accuracy: 79.24% | Train F1: 39.56%\n",
      "Validation Loss: 0.959 | Validation accuracy: 78.06% | Validation F1: 41.83%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 03\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:29<00:00,  4.49s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.67it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 95.89124608039856s\n",
      "Train loss: 0.717 | Train accuracy: 85.20% | Train F1: 45.80%\n",
      "Validation Loss: 0.789 | Validation accuracy: 81.43% | Validation F1: 45.53%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 04\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:30<00:00,  4.53s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.69it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 96.48748970031738s\n",
      "Train loss: 0.549 | Train accuracy: 88.88% | Train F1: 49.24%\n",
      "Validation Loss: 0.671 | Validation accuracy: 83.50% | Validation F1: 47.76%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 05\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:27<00:00,  4.36s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.69it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 93.24930310249329s\n",
      "Train loss: 0.427 | Train accuracy: 91.57% | Train F1: 52.85%\n",
      "Validation Loss: 0.592 | Validation accuracy: 85.18% | Validation F1: 51.88%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 06\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:29<00:00,  4.46s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:06<00:00,  1.61it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 95.58493089675903s\n",
      "Train loss: 0.336 | Train accuracy: 93.45% | Train F1: 58.30%\n",
      "Validation Loss: 0.536 | Validation accuracy: 86.19% | Validation F1: 55.01%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 07\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:24<00:00,  4.21s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.67it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 90.28789329528809s\n",
      "Train loss: 0.269 | Train accuracy: 94.92% | Train F1: 63.45%\n",
      "Validation Loss: 0.496 | Validation accuracy: 87.14% | Validation F1: 59.28%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 08\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:22<00:00,  4.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:06<00:00,  1.64it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 88.86557984352112s\n",
      "Train loss: 0.220 | Train accuracy: 95.84% | Train F1: 65.78%\n",
      "Validation Loss: 0.466 | Validation accuracy: 87.67% | Validation F1: 61.24%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 09\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:22<00:00,  4.14s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.68it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 88.8346483707428s\n",
      "Train loss: 0.183 | Train accuracy: 96.62% | Train F1: 68.58%\n",
      "Validation Loss: 0.446 | Validation accuracy: 88.09% | Validation F1: 65.14%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Epoch: 10\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [01:27<00:00,  4.39s/it]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:05<00:00,  1.68it/s]\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Time: 93.85730004310608s\n",
      "Train loss: 0.155 | Train accuracy: 97.15% | Train F1: 70.27%\n",
      "Validation Loss: 0.435 | Validation accuracy: 88.31% | Validation F1: 66.61%\n",
      "Saving new checkpoint...\n",
      "\n",
      "Testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 10/10 [00:03<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.377 | Test accuracy: 89.51% | Test F1: 63.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_val_test(baseline_model, \"baseline_model\", epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

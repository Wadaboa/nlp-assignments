{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3 : Sequence labelling with RNNs\n",
    "In this assignement we will ask you to perform POS tagging.\n",
    "\n",
    "You are asked to follow these steps:\n",
    "\n",
    "- Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "- Embed the words using GloVe embeddings\n",
    "- Create a baseline model, using a simple neural architecture\n",
    "- Experiment doing small modifications to the model\n",
    "- Evaluate your best model\n",
    "- Analyze the errors of your model\n",
    "- Corpora: Ignore the numeric value in the third column, use only the words/symbols and its label. https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
    "\n",
    "Splits: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
    "\n",
    "Baseline: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
    "\n",
    "Modifications: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
    "\n",
    "Training and Experiments: all the experiments must involve only the training and validation sets.\n",
    "\n",
    "Evaluation: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
    "\n",
    "Error Analysis (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
    "\n",
    "Report: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:04:35.776626Z",
     "iopub.status.busy": "2020-11-11T21:04:35.776173Z",
     "iopub.status.idle": "2020-11-11T21:04:35.871550Z",
     "shell.execute_reply": "2020-11-11T21:04:35.870394Z",
     "shell.execute_reply.started": "2020-11-11T21:04:35.776581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import dependency_treebank\n",
    "nltk.download('dependency_treebank')\n",
    "\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T16:48:55.537174Z",
     "iopub.status.busy": "2020-11-10T16:48:55.536862Z",
     "iopub.status.idle": "2020-11-10T16:48:55.581955Z",
     "shell.execute_reply": "2020-11-10T16:48:55.580700Z",
     "shell.execute_reply.started": "2020-11-10T16:48:55.537137Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed):\n",
    "    \"\"\"\n",
    "    Fix all the possible sources of randomness\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "fix_random(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:31:08.428530Z",
     "iopub.status.busy": "2020-11-11T08:31:08.428214Z",
     "iopub.status.idle": "2020-11-11T08:31:08.476211Z",
     "shell.execute_reply": "2020-11-11T08:31:08.475051Z",
     "shell.execute_reply.started": "2020-11-11T08:31:08.428489Z"
    }
   },
   "outputs": [],
   "source": [
    "file_prefix = \"wsj_\"\n",
    "file_ext = \".dp\"\n",
    "train_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(1, 101)]\n",
    "val_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(101, 151)]\n",
    "test_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(151, 200)]\n",
    "splits = (\n",
    "    [\"train\"] * len(train_files) + [\"val\"] * len(val_files) + [\"test\"] * len(test_files)\n",
    ")\n",
    "whole_files = train_files + val_files + test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:31:09.419953Z",
     "iopub.status.busy": "2020-11-11T08:31:09.419636Z",
     "iopub.status.idle": "2020-11-11T08:31:09.471336Z",
     "shell.execute_reply": "2020-11-11T08:31:09.470269Z",
     "shell.execute_reply.started": "2020-11-11T08:31:09.419912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0001.dp',\n",
       " 'wsj_0021.dp',\n",
       " 'wsj_0041.dp',\n",
       " 'wsj_0061.dp',\n",
       " 'wsj_0081.dp',\n",
       " 'wsj_0101.dp',\n",
       " 'wsj_0121.dp',\n",
       " 'wsj_0141.dp',\n",
       " 'wsj_0161.dp',\n",
       " 'wsj_0181.dp']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_files[0:-1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:12:00.430512Z",
     "iopub.status.busy": "2020-11-11T20:12:00.430103Z",
     "iopub.status.idle": "2020-11-11T20:12:00.482817Z",
     "shell.execute_reply": "2020-11-11T20:12:00.481737Z",
     "shell.execute_reply.started": "2020-11-11T20:12:00.430468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre\\tNNP\\t2\\nVinken\\tNNP\\t8\\n,\\t,\\t2\\n61\\tCD\\t5\\nyears\\tNNS\\t6\\nold\\tJJ\\t2\\n,\\t,\\t2\\nwill\\tMD\\t0\\njoin\\tVB\\t8\\nthe\\tDT\\t11\\nboard\\tNN\\t9\\nas\\tIN\\t9\\na\\tDT\\t15\\nnonexecutive\\tJJ\\t15\\ndirector\\tNN\\t12\\nNov.\\tNNP\\t9\\n29\\tCD\\t16\\n.\\t.\\t8\\n\\nMr.\\tNNP\\t2\\nVinken\\tNNP\\t3\\nis\\tVBZ\\t0\\nchairman\\tNN\\t3\\nof\\tIN\\t4\\nElsevier\\tNNP\\t7\\nN.V.\\tNNP\\t12\\n,\\t,\\t12\\nthe\\tDT\\t12\\nDutch\\tNNP\\t12\\npublishing\\tVBG\\t12\\ngroup\\tNN\\t5\\n.\\t.\\t3\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_treebank.raw(fileids=whole_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:23:46.937149Z",
     "iopub.status.busy": "2020-11-11T20:23:46.936730Z",
     "iopub.status.idle": "2020-11-11T20:23:46.992262Z",
     "shell.execute_reply": "2020-11-11T20:23:46.990844Z",
     "shell.execute_reply.started": "2020-11-11T20:23:46.937105Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_file(fileid):\n",
    "    file_str = dependency_treebank.raw(fileids=fileid)\n",
    "    splitted_file_str = [\n",
    "        x for x in re.split(\"\\t|\\n\", file_str.strip()) if x.strip() != \"\"\n",
    "    ]\n",
    "    tokens, tags = [], []\n",
    "    for i in range(0, len(splitted_file_str), 3):\n",
    "        token = splitted_file_str[i]\n",
    "        tag = splitted_file_str[i + 1]\n",
    "        tokens.append(token)\n",
    "        tags.append(tag)\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def parse_files(fileids):\n",
    "    tokens_list, tags_list = [], []\n",
    "    for fileid in fileids:\n",
    "        tokens, tags = parse_file(fileid)\n",
    "        tokens_list.append(tokens)\n",
    "        tags_list.append(tags)\n",
    "    return tokens_list, tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:23:47.915927Z",
     "iopub.status.busy": "2020-11-11T20:23:47.915611Z",
     "iopub.status.idle": "2020-11-11T20:23:48.234995Z",
     "shell.execute_reply": "2020-11-11T20:23:48.233780Z",
     "shell.execute_reply.started": "2020-11-11T20:23:47.915886Z"
    }
   },
   "outputs": [],
   "source": [
    "whole_tokens, whole_tags = parse_files(whole_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:41:05.919258Z",
     "iopub.status.busy": "2020-11-11T20:41:05.918836Z",
     "iopub.status.idle": "2020-11-11T20:41:05.984272Z",
     "shell.execute_reply": "2020-11-11T20:41:05.982683Z",
     "shell.execute_reply.started": "2020-11-11T20:41:05.919214Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(a):\n",
    "    return [i for s in a for i in s]\n",
    "\n",
    "\n",
    "flattened_tags = flatten(whole_tags)\n",
    "flattened_tokens = flatten(whole_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:16:07.503530Z",
     "iopub.status.busy": "2020-11-11T21:16:07.503090Z",
     "iopub.status.idle": "2020-11-11T21:16:07.604381Z",
     "shell.execute_reply": "2020-11-11T21:16:07.603227Z",
     "shell.execute_reply.started": "2020-11-11T21:16:07.503484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = np.unique(flattened_tags)\n",
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:45:26.049099Z",
     "iopub.status.busy": "2020-11-11T20:45:26.048677Z",
     "iopub.status.idle": "2020-11-11T20:45:26.224585Z",
     "shell.execute_reply": "2020-11-11T20:45:26.223332Z",
     "shell.execute_reply.started": "2020-11-11T20:45:26.049055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 13166),\n",
       " ('IN', 9857),\n",
       " ('NNP', 9410),\n",
       " ('DT', 8165),\n",
       " ('NNS', 6047),\n",
       " ('JJ', 5834),\n",
       " (',', 4886),\n",
       " ('.', 3874),\n",
       " ('CD', 3546),\n",
       " ('VBD', 3043)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_fd = nltk.probability.FreqDist(flattened_tags)\n",
    "tags_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:41:46.834398Z",
     "iopub.status.busy": "2020-11-11T20:41:46.833962Z",
     "iopub.status.idle": "2020-11-11T20:41:47.025174Z",
     "shell.execute_reply": "2020-11-11T20:41:47.024028Z",
     "shell.execute_reply.started": "2020-11-11T20:41:46.834353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4885),\n",
       " ('the', 4045),\n",
       " ('.', 3828),\n",
       " ('of', 2319),\n",
       " ('to', 2164),\n",
       " ('a', 1878),\n",
       " ('in', 1572),\n",
       " ('and', 1511),\n",
       " (\"'s\", 864),\n",
       " ('for', 817)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_fd = nltk.probability.FreqDist(flattened_tokens)\n",
    "tokens_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:25:39.183756Z",
     "iopub.status.busy": "2020-11-11T20:25:39.183329Z",
     "iopub.status.idle": "2020-11-11T20:25:39.275955Z",
     "shell.execute_reply": "2020-11-11T20:25:39.274514Z",
     "shell.execute_reply.started": "2020-11-11T20:25:39.183713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>split</th>\n",
       "      <th>fileid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0001.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0002.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0003.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Yields, on, money-market, mutual, funds, cont...</td>\n",
       "      <td>[NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0004.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[J.P., Bolduc, ,, vice, chairman, of, W.R., Gr...</td>\n",
       "      <td>[NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0005.dp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "2  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "3  [Yields, on, money-market, mutual, funds, cont...   \n",
       "4  [J.P., Bolduc, ,, vice, chairman, of, W.R., Gr...   \n",
       "\n",
       "                                                tags  split       fileid  \n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  train  wsj_0001.dp  \n",
       "1  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  train  wsj_0002.dp  \n",
       "2  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  train  wsj_0003.dp  \n",
       "3  [NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...  train  wsj_0004.dp  \n",
       "4  [NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...  train  wsj_0005.dp  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"tokens\": whole_tokens,\n",
    "        \"tags\": whole_tags,\n",
    "        \"split\": splits,\n",
    "        \"fileid\": whole_files,\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:26:07.748946Z",
     "iopub.status.busy": "2020-11-11T20:26:07.748510Z",
     "iopub.status.idle": "2020-11-11T20:26:07.803045Z",
     "shell.execute_reply": "2020-11-11T20:26:07.801892Z",
     "shell.execute_reply.started": "2020-11-11T20:26:07.748903Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = df[df[\"split\"] == \"train\"]\n",
    "val_df = df[df[\"split\"] == \"val\"]\n",
    "test_df = df[df[\"split\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:00:47.601889Z",
     "iopub.status.busy": "2020-11-11T21:00:47.601467Z",
     "iopub.status.idle": "2020-11-11T21:01:36.722537Z",
     "shell.execute_reply": "2020-11-11T21:01:36.720656Z",
     "shell.execute_reply.started": "2020-11-11T21:00:47.601844Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "embedding_model = utils.load_embedding_model(\"glove\", embedding_dimension=embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:47:53.429014Z",
     "iopub.status.busy": "2020-11-11T20:47:53.428611Z",
     "iopub.status.idle": "2020-11-11T20:47:53.487621Z",
     "shell.execute_reply": "2020-11-11T20:47:53.486485Z",
     "shell.execute_reply.started": "2020-11-11T20:47:53.428970Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, builds the corresponding word vocabulary\n",
    "    \"\"\"\n",
    "    words = sorted(set(tokens))\n",
    "    vocabulary, inverse_vocabulary = dict(), dict()\n",
    "    for i, w in tqdm(enumerate(words)):\n",
    "        vocabulary[i] = w\n",
    "        inverse_vocabulary[w] = i\n",
    "    return vocabulary, inverse_vocabulary, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:51:36.430329Z",
     "iopub.status.busy": "2020-11-11T20:51:36.429913Z",
     "iopub.status.idle": "2020-11-11T20:51:36.522227Z",
     "shell.execute_reply": "2020-11-11T20:51:36.521061Z",
     "shell.execute_reply.started": "2020-11-11T20:51:36.430285Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11968it [00:00, 714310.13it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_word, word_to_index, word_listing = build_vocabulary(flattened_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:51:37.616049Z",
     "iopub.status.busy": "2020-11-11T20:51:37.615731Z",
     "iopub.status.idle": "2020-11-11T20:51:37.682711Z",
     "shell.execute_reply": "2020-11-11T20:51:37.681611Z",
     "shell.execute_reply.started": "2020-11-11T20:51:37.616008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '!'),\n",
       " (1, '#'),\n",
       " (2, '$'),\n",
       " (3, '%'),\n",
       " (4, '&'),\n",
       " (5, \"'\"),\n",
       " (6, \"''\"),\n",
       " (7, \"'30s\"),\n",
       " (8, \"'40s\"),\n",
       " (9, \"'50s\")]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:51:40.923533Z",
     "iopub.status.busy": "2020-11-11T20:51:40.923200Z",
     "iopub.status.idle": "2020-11-11T20:51:40.982965Z",
     "shell.execute_reply": "2020-11-11T20:51:40.981783Z",
     "shell.execute_reply.started": "2020-11-11T20:51:40.923492Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_oov_terms(embedding_model, word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms\n",
    "    \"\"\"\n",
    "    oov_terms = []\n",
    "    for word in word_listing:\n",
    "        if word not in embedding_model.vocab:\n",
    "            oov_terms.append(word)\n",
    "    return oov_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:52:23.428895Z",
     "iopub.status.busy": "2020-11-11T20:52:23.428538Z",
     "iopub.status.idle": "2020-11-11T20:52:23.500045Z",
     "shell.execute_reply": "2020-11-11T20:52:23.498822Z",
     "shell.execute_reply.started": "2020-11-11T20:52:23.428853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 3745 (0.31%)\n"
     ]
    }
   ],
   "source": [
    "oov_terms = check_oov_terms(embedding_model, word_listing)\n",
    "print(\n",
    "    f\"Total OOV terms: {len(oov_terms)} ({round(len(oov_terms) / len(word_listing), 2)}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:00:19.506264Z",
     "iopub.status.busy": "2020-11-11T21:00:19.505844Z",
     "iopub.status.idle": "2020-11-11T21:00:19.567318Z",
     "shell.execute_reply": "2020-11-11T21:00:19.566121Z",
     "shell.execute_reply.started": "2020-11-11T21:00:19.506220Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_index,\n",
    "    oov_terms,\n",
    "    method=\"normal\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained Gensim word embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    def uniform_embedding(embedding_dimension, interval=(-1, 1)):\n",
    "        return interval[0] + np.random.sample(embedding_dimension) + interval[1]\n",
    "\n",
    "    def normal_embedding(embedding_dimension):\n",
    "        return np.random.normal(embedding_dimension)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_index), embedding_dimension))\n",
    "    for word, index in word_to_index.items():\n",
    "        # Words that are no OOV are taken from the Gensim model\n",
    "        if word not in oov_terms:\n",
    "            word_vector = embedding_model[word]\n",
    "        # OOV words computed as random normal vectors\n",
    "        elif method == \"normal\":\n",
    "            word_vector = normal_embedding(embedding_dimension)\n",
    "        # OOV words computed as uniform vectors in range [-1, 1]\n",
    "        elif method == \"uniform\":\n",
    "            word_vector = uniform_embedding(embedding_dimension)\n",
    "        embedding_matrix[index, :] = word_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:01:54.915899Z",
     "iopub.status.busy": "2020-11-11T21:01:54.915448Z",
     "iopub.status.idle": "2020-11-11T21:01:56.796106Z",
     "shell.execute_reply": "2020-11-11T21:01:56.794908Z",
     "shell.execute_reply.started": "2020-11-11T21:01:54.915854Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,\n",
       "        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,\n",
       "       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,\n",
       "       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,\n",
       "        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,\n",
       "       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,\n",
       "       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,\n",
       "       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,\n",
       "       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,\n",
       "        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,\n",
       "       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,\n",
       "       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,\n",
       "       -1.15139998e-01, -7.85809994e-01])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(\n",
    "    embedding_model, embedding_dimension, word_to_index, oov_terms, method=\"normal\",\n",
    ")\n",
    "embedding_matrix[word_to_index[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:12:23.430604Z",
     "iopub.status.busy": "2020-11-11T21:12:23.430168Z",
     "iopub.status.idle": "2020-11-11T21:12:23.510092Z",
     "shell.execute_reply": "2020-11-11T21:12:23.508519Z",
     "shell.execute_reply.started": "2020-11-11T21:12:23.430559Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        embedding_dimension,\n",
    "        hidden_dimension,\n",
    "        output_dimension,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        dropout_rate=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dimension, embedding_dimension)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dimension,\n",
    "            hidden_dimension,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dimension * 2 if bidirectional else hidden_dimension,\n",
    "            output_dimension,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:17:59.431058Z",
     "iopub.status.busy": "2020-11-11T21:17:59.430621Z",
     "iopub.status.idle": "2020-11-11T21:17:59.534112Z",
     "shell.execute_reply": "2020-11-11T21:17:59.532929Z",
     "shell.execute_reply.started": "2020-11-11T21:17:59.431013Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dimension = 128\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = BiLSTMPOSTagger(\n",
    "    len(word_to_index),\n",
    "    embedding_dimension,\n",
    "    hidden_dimension,\n",
    "    len(unique_tags),\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:19:49.429175Z",
     "iopub.status.busy": "2020-11-11T21:19:49.428738Z",
     "iopub.status.idle": "2020-11-11T21:19:49.565430Z",
     "shell.execute_reply": "2020-11-11T21:19:49.564231Z",
     "shell.execute_reply.started": "2020-11-11T21:19:49.429130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMPOSTagger(\n",
       "  (embedding): Embedding(11968, 50)\n",
       "  (lstm): LSTM(50, 128, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=45, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:19:52.121345Z",
     "iopub.status.busy": "2020-11-11T21:19:52.120953Z",
     "iopub.status.idle": "2020-11-11T21:19:52.188706Z",
     "shell.execute_reply": "2020-11-11T21:19:52.187582Z",
     "shell.execute_reply.started": "2020-11-11T21:19:52.121302Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 794,285 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:19:53.435285Z",
     "iopub.status.busy": "2020-11-11T21:19:53.434965Z",
     "iopub.status.idle": "2020-11-11T21:19:53.500773Z",
     "shell.execute_reply": "2020-11-11T21:19:53.499670Z",
     "shell.execute_reply.started": "2020-11-11T21:19:53.435245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5840,  0.3903,  0.6528,  ..., -1.2338,  0.4672,  0.7886],\n",
       "        [-1.5925,  0.7557,  1.4947,  ..., -0.8408,  0.4949,  0.3405],\n",
       "        [ 0.4389,  0.9030,  1.4060,  ...,  0.0744, -0.6935,  0.8135],\n",
       "        ...,\n",
       "        [-0.7826, -0.3365,  1.2228,  ..., -0.3224,  0.2601, -0.0665],\n",
       "        [-0.3452, -0.0519,  0.0702,  ...,  0.2199,  0.1535, -0.1313],\n",
       "        [ 0.0380, -0.9954,  1.7751,  ..., -0.1395, -0.4050, -0.6642]])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:20:10.430382Z",
     "iopub.status.busy": "2020-11-11T21:20:10.429936Z",
     "iopub.status.idle": "2020-11-11T21:20:10.497003Z",
     "shell.execute_reply": "2020-11-11T21:20:10.495827Z",
     "shell.execute_reply.started": "2020-11-11T21:20:10.430337Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

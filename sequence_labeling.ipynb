{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 3 : Sequence labelling with RNNs\n",
    "In this assignement we will ask you to perform POS tagging.\n",
    "\n",
    "You are asked to follow these steps:\n",
    "\n",
    "- Download the corpora and split it in training and test sets, structuring a dataframe.\n",
    "- Embed the words using GloVe embeddings\n",
    "- Create a baseline model, using a simple neural architecture\n",
    "- Experiment doing small modifications to the model\n",
    "- Evaluate your best model\n",
    "- Analyze the errors of your model\n",
    "- Corpora: Ignore the numeric value in the third column, use only the words/symbols and its label. https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip\n",
    "\n",
    "Splits: documents 1-100 are the train set, 101-150 validation set, 151-199 test set.\n",
    "\n",
    "Baseline: two layers architecture: a Bidirectional LSTM and a Dense/Fully-Connected layer on top.\n",
    "\n",
    "Modifications: experiment using a GRU instead of the LSTM, adding an additional LSTM layer, and using a CRF in addition to the LSTM. Each of this change must be done by itself (don't mix these modifications).\n",
    "\n",
    "Training and Experiments: all the experiments must involve only the training and validation sets.\n",
    "\n",
    "Evaluation: in the end, only the best model of your choice must be evaluated on the test set. The main metric must be F1-Macro computed between the various part of speech (without considering punctuation classes).\n",
    "\n",
    "Error Analysis (optional) : analyze the errors done by your model, try to understand which may be the causes and think about how to improve it.\n",
    "\n",
    "Report: You are asked to deliver a small report of about 4-5 lines in the .txt file that sums up your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/dependency_treebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:24:20.745013Z",
     "iopub.status.busy": "2020-11-12T10:24:20.744540Z",
     "iopub.status.idle": "2020-11-12T10:24:20.823546Z",
     "shell.execute_reply": "2020-11-12T10:24:20.822350Z",
     "shell.execute_reply.started": "2020-11-12T10:24:20.744961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package dependency_treebank to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package dependency_treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_sequence\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import dependency_treebank\n",
    "nltk.download('dependency_treebank')\n",
    "\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T16:48:55.537174Z",
     "iopub.status.busy": "2020-11-10T16:48:55.536862Z",
     "iopub.status.idle": "2020-11-10T16:48:55.581955Z",
     "shell.execute_reply": "2020-11-10T16:48:55.580700Z",
     "shell.execute_reply.started": "2020-11-10T16:48:55.537137Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "\n",
    "def fix_random(seed):\n",
    "    \"\"\"\n",
    "    Fix all the possible sources of randomness\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "fix_random(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:31:08.428530Z",
     "iopub.status.busy": "2020-11-11T08:31:08.428214Z",
     "iopub.status.idle": "2020-11-11T08:31:08.476211Z",
     "shell.execute_reply": "2020-11-11T08:31:08.475051Z",
     "shell.execute_reply.started": "2020-11-11T08:31:08.428489Z"
    }
   },
   "outputs": [],
   "source": [
    "file_prefix = \"wsj_\"\n",
    "file_ext = \".dp\"\n",
    "train_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(1, 101)]\n",
    "val_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(101, 151)]\n",
    "test_files = [f\"{file_prefix}{i:04d}{file_ext}\" for i in range(151, 200)]\n",
    "splits = (\n",
    "    [\"train\"] * len(train_files) + [\"val\"] * len(val_files) + [\"test\"] * len(test_files)\n",
    ")\n",
    "whole_files = train_files + val_files + test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T08:31:09.419953Z",
     "iopub.status.busy": "2020-11-11T08:31:09.419636Z",
     "iopub.status.idle": "2020-11-11T08:31:09.471336Z",
     "shell.execute_reply": "2020-11-11T08:31:09.470269Z",
     "shell.execute_reply.started": "2020-11-11T08:31:09.419912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wsj_0001.dp',\n",
       " 'wsj_0021.dp',\n",
       " 'wsj_0041.dp',\n",
       " 'wsj_0061.dp',\n",
       " 'wsj_0081.dp',\n",
       " 'wsj_0101.dp',\n",
       " 'wsj_0121.dp',\n",
       " 'wsj_0141.dp',\n",
       " 'wsj_0161.dp',\n",
       " 'wsj_0181.dp']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_files[0:-1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:12:00.430512Z",
     "iopub.status.busy": "2020-11-11T20:12:00.430103Z",
     "iopub.status.idle": "2020-11-11T20:12:00.482817Z",
     "shell.execute_reply": "2020-11-11T20:12:00.481737Z",
     "shell.execute_reply.started": "2020-11-11T20:12:00.430468Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pierre\\tNNP\\t2\\nVinken\\tNNP\\t8\\n,\\t,\\t2\\n61\\tCD\\t5\\nyears\\tNNS\\t6\\nold\\tJJ\\t2\\n,\\t,\\t2\\nwill\\tMD\\t0\\njoin\\tVB\\t8\\nthe\\tDT\\t11\\nboard\\tNN\\t9\\nas\\tIN\\t9\\na\\tDT\\t15\\nnonexecutive\\tJJ\\t15\\ndirector\\tNN\\t12\\nNov.\\tNNP\\t9\\n29\\tCD\\t16\\n.\\t.\\t8\\n\\nMr.\\tNNP\\t2\\nVinken\\tNNP\\t3\\nis\\tVBZ\\t0\\nchairman\\tNN\\t3\\nof\\tIN\\t4\\nElsevier\\tNNP\\t7\\nN.V.\\tNNP\\t12\\n,\\t,\\t12\\nthe\\tDT\\t12\\nDutch\\tNNP\\t12\\npublishing\\tVBG\\t12\\ngroup\\tNN\\t5\\n.\\t.\\t3\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependency_treebank.raw(fileids=whole_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:23:46.937149Z",
     "iopub.status.busy": "2020-11-11T20:23:46.936730Z",
     "iopub.status.idle": "2020-11-11T20:23:46.992262Z",
     "shell.execute_reply": "2020-11-11T20:23:46.990844Z",
     "shell.execute_reply.started": "2020-11-11T20:23:46.937105Z"
    }
   },
   "outputs": [],
   "source": [
    "def parse_file(fileid):\n",
    "    file_str = dependency_treebank.raw(fileids=fileid)\n",
    "    splitted_file_str = [\n",
    "        x for x in re.split(\"\\t|\\n\", file_str.strip()) if x.strip() != \"\"\n",
    "    ]\n",
    "    tokens, tags = [], []\n",
    "    for i in range(0, len(splitted_file_str), 3):\n",
    "        token = splitted_file_str[i]\n",
    "        tag = splitted_file_str[i + 1]\n",
    "        tokens.append(token)\n",
    "        tags.append(tag)\n",
    "    return tokens, tags\n",
    "\n",
    "\n",
    "def parse_files(fileids):\n",
    "    tokens_list, tags_list = [], []\n",
    "    for fileid in fileids:\n",
    "        tokens, tags = parse_file(fileid)\n",
    "        tokens_list.append(tokens)\n",
    "        tags_list.append(tags)\n",
    "    return tokens_list, tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:23:47.915927Z",
     "iopub.status.busy": "2020-11-11T20:23:47.915611Z",
     "iopub.status.idle": "2020-11-11T20:23:48.234995Z",
     "shell.execute_reply": "2020-11-11T20:23:48.233780Z",
     "shell.execute_reply.started": "2020-11-11T20:23:47.915886Z"
    }
   },
   "outputs": [],
   "source": [
    "whole_tokens, whole_tags = parse_files(whole_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:41:05.919258Z",
     "iopub.status.busy": "2020-11-11T20:41:05.918836Z",
     "iopub.status.idle": "2020-11-11T20:41:05.984272Z",
     "shell.execute_reply": "2020-11-11T20:41:05.982683Z",
     "shell.execute_reply.started": "2020-11-11T20:41:05.919214Z"
    }
   },
   "outputs": [],
   "source": [
    "def flatten(a):\n",
    "    return [i for s in a for i in s]\n",
    "\n",
    "\n",
    "flattened_tags = flatten(whole_tags)\n",
    "flattened_tokens = flatten(whole_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:16:07.503530Z",
     "iopub.status.busy": "2020-11-11T21:16:07.503090Z",
     "iopub.status.idle": "2020-11-11T21:16:07.604381Z",
     "shell.execute_reply": "2020-11-11T21:16:07.603227Z",
     "shell.execute_reply.started": "2020-11-11T21:16:07.503484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags = np.unique(flattened_tags)\n",
    "len(unique_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:45:26.049099Z",
     "iopub.status.busy": "2020-11-11T20:45:26.048677Z",
     "iopub.status.idle": "2020-11-11T20:45:26.224585Z",
     "shell.execute_reply": "2020-11-11T20:45:26.223332Z",
     "shell.execute_reply.started": "2020-11-11T20:45:26.049055Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 13166),\n",
       " ('IN', 9857),\n",
       " ('NNP', 9410),\n",
       " ('DT', 8165),\n",
       " ('NNS', 6047),\n",
       " ('JJ', 5834),\n",
       " (',', 4886),\n",
       " ('.', 3874),\n",
       " ('CD', 3546),\n",
       " ('VBD', 3043)]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_fd = nltk.probability.FreqDist(flattened_tags)\n",
    "tags_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T20:41:46.834398Z",
     "iopub.status.busy": "2020-11-11T20:41:46.833962Z",
     "iopub.status.idle": "2020-11-11T20:41:47.025174Z",
     "shell.execute_reply": "2020-11-11T20:41:47.024028Z",
     "shell.execute_reply.started": "2020-11-11T20:41:46.834353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4885),\n",
       " ('the', 4045),\n",
       " ('.', 3828),\n",
       " ('of', 2319),\n",
       " ('to', 2164),\n",
       " ('a', 1878),\n",
       " ('in', 1572),\n",
       " ('and', 1511),\n",
       " (\"'s\", 864),\n",
       " ('for', 817)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_fd = nltk.probability.FreqDist(flattened_tokens)\n",
    "tokens_fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:05:41.024800Z",
     "iopub.status.busy": "2020-11-12T09:05:41.024199Z",
     "iopub.status.idle": "2020-11-12T09:05:41.093556Z",
     "shell.execute_reply": "2020-11-12T09:05:41.092323Z",
     "shell.execute_reply.started": "2020-11-12T09:05:41.024755Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(tokens, padding_token=\"0\"):\n",
    "    \"\"\"\n",
    "    Given a list of tokens, builds the corresponding word vocabulary\n",
    "    \"\"\"\n",
    "    words = sorted(set(tokens))\n",
    "    vocabulary, inverse_vocabulary = dict(), dict()\n",
    "    vocabulary[0] = str(padding_token)\n",
    "    inverse_vocabulary[str(padding_token)] = 0\n",
    "    for i, w in tqdm(enumerate(words)):\n",
    "        vocabulary[i + 1] = w\n",
    "        inverse_vocabulary[w] = i + 1\n",
    "    return vocabulary, inverse_vocabulary, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:25:41.113787Z",
     "iopub.status.busy": "2020-11-12T10:25:41.113372Z",
     "iopub.status.idle": "2020-11-12T10:25:41.171007Z",
     "shell.execute_reply": "2020-11-12T10:25:41.169878Z",
     "shell.execute_reply.started": "2020-11-12T10:25:41.113742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_token = \"0\"\n",
    "padding_token in (flattened_tokens, flattened_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:06:14.429670Z",
     "iopub.status.busy": "2020-11-12T09:06:14.429253Z",
     "iopub.status.idle": "2020-11-12T09:06:14.526215Z",
     "shell.execute_reply": "2020-11-12T09:06:14.525044Z",
     "shell.execute_reply.started": "2020-11-12T09:06:14.429626Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11968it [00:00, 621301.46it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_word, word_to_index, word_listing = build_vocabulary(flattened_tokens, padding_token=padding_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:06:15.629669Z",
     "iopub.status.busy": "2020-11-12T09:06:15.629337Z",
     "iopub.status.idle": "2020-11-12T09:06:15.707777Z",
     "shell.execute_reply": "2020-11-12T09:06:15.706541Z",
     "shell.execute_reply.started": "2020-11-12T09:06:15.629627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0'),\n",
       " (1, '!'),\n",
       " (2, '#'),\n",
       " (3, '$'),\n",
       " (4, '%'),\n",
       " (5, '&'),\n",
       " (6, \"'\"),\n",
       " (7, \"''\"),\n",
       " (8, \"'30s\"),\n",
       " (9, \"'40s\")]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_word.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:28:02.430227Z",
     "iopub.status.busy": "2020-11-12T09:28:02.429802Z",
     "iopub.status.idle": "2020-11-12T09:28:02.498424Z",
     "shell.execute_reply": "2020-11-12T09:28:02.497004Z",
     "shell.execute_reply.started": "2020-11-12T09:28:02.430183Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [00:00, 208326.36it/s]\n"
     ]
    }
   ],
   "source": [
    "index_to_tag, tag_to_index, tag_listing = build_vocabulary(flattened_tags, padding_token=padding_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:28:10.128153Z",
     "iopub.status.busy": "2020-11-12T09:28:10.127708Z",
     "iopub.status.idle": "2020-11-12T09:28:10.197268Z",
     "shell.execute_reply": "2020-11-12T09:28:10.195829Z",
     "shell.execute_reply.started": "2020-11-12T09:28:10.128108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0'),\n",
       " (1, '#'),\n",
       " (2, '$'),\n",
       " (3, \"''\"),\n",
       " (4, ','),\n",
       " (5, '-LRB-'),\n",
       " (6, '-RRB-'),\n",
       " (7, '.'),\n",
       " (8, ':'),\n",
       " (9, 'CC')]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(index_to_tag.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:41:58.483395Z",
     "iopub.status.busy": "2020-11-12T09:41:58.482979Z",
     "iopub.status.idle": "2020-11-12T09:41:58.540682Z",
     "shell.execute_reply": "2020-11-12T09:41:58.539563Z",
     "shell.execute_reply.started": "2020-11-12T09:41:58.483351Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_indexes(values, to_index):\n",
    "    return [to_index[v] for v in values]\n",
    "\n",
    "\n",
    "whole_indexed_tokens = map(\n",
    "    lambda tokens: to_indexes(tokens, word_to_index), whole_tokens\n",
    ")\n",
    "whole_indexed_tags = map(lambda tags: to_indexes(tags, tag_to_index), whole_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:42:01.982994Z",
     "iopub.status.busy": "2020-11-12T09:42:01.982560Z",
     "iopub.status.idle": "2020-11-12T09:42:02.152919Z",
     "shell.execute_reply": "2020-11-12T09:42:02.151747Z",
     "shell.execute_reply.started": "2020-11-12T09:42:01.982950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>indexed_tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>indexed_tags</th>\n",
       "      <th>split</th>\n",
       "      <th>fileid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[3328, 4106, 21, 824, 11941, 8847, 21, 11846, ...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "      <td>[21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0001.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[3564, 1090, 21, 773, 11941, 8847, 4553, 7110,...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "      <td>[21, 21, 4, 10, 23, 15, 9, 15, 20, 14, 21, 21,...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0002.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[1028, 7104, 8819, 4687, 8857, 11599, 11278, 8...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "      <td>[11, 20, 14, 20, 28, 38, 33, 35, 21, 20, 23, 4...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0003.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Yields, on, money-market, mutual, funds, cont...</td>\n",
       "      <td>[4242, 8855, 8546, 8624, 7194, 5749, 11278, 10...</td>\n",
       "      <td>[NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...</td>\n",
       "      <td>[23, 14, 15, 15, 23, 36, 33, 35, 4, 14, 23, 14...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0004.dp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[J.P., Bolduc, ,, vice, chairman, of, W.R., Gr...</td>\n",
       "      <td>[2528, 1352, 21, 11661, 5340, 8819, 4122, 2248...</td>\n",
       "      <td>[NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...</td>\n",
       "      <td>[21, 21, 4, 20, 20, 14, 21, 21, 9, 21, 4, 41, ...</td>\n",
       "      <td>train</td>\n",
       "      <td>wsj_0005.dp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens  \\\n",
       "0  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "2  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "3  [Yields, on, money-market, mutual, funds, cont...   \n",
       "4  [J.P., Bolduc, ,, vice, chairman, of, W.R., Gr...   \n",
       "\n",
       "                                      indexed_tokens  \\\n",
       "0  [3328, 4106, 21, 824, 11941, 8847, 21, 11846, ...   \n",
       "1  [3564, 1090, 21, 773, 11941, 8847, 4553, 7110,...   \n",
       "2  [1028, 7104, 8819, 4687, 8857, 11599, 11278, 8...   \n",
       "3  [4242, 8855, 8546, 8624, 7194, 5749, 11278, 10...   \n",
       "4  [2528, 1352, 21, 11661, 5340, 8819, 4122, 2248...   \n",
       "\n",
       "                                                tags  \\\n",
       "0  [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...   \n",
       "1  [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...   \n",
       "2  [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...   \n",
       "3  [NNS, IN, JJ, JJ, NNS, VBD, TO, VB, ,, IN, NNS...   \n",
       "4  [NNP, NNP, ,, NN, NN, IN, NNP, NNP, CC, NNP, ,...   \n",
       "\n",
       "                                        indexed_tags  split       fileid  \n",
       "0  [21, 21, 4, 10, 23, 15, 4, 19, 35, 11, 20, 14,...  train  wsj_0001.dp  \n",
       "1  [21, 21, 4, 10, 23, 15, 9, 15, 20, 14, 21, 21,...  train  wsj_0002.dp  \n",
       "2  [11, 20, 14, 20, 28, 38, 33, 35, 21, 20, 23, 4...  train  wsj_0003.dp  \n",
       "3  [23, 14, 15, 15, 23, 36, 33, 35, 4, 14, 23, 14...  train  wsj_0004.dp  \n",
       "4  [21, 21, 4, 20, 20, 14, 21, 21, 9, 21, 4, 41, ...  train  wsj_0005.dp  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"tokens\": whole_tokens,\n",
    "        \"indexed_tokens\": whole_indexed_tokens,\n",
    "        \"tags\": whole_tags,\n",
    "        \"indexed_tags\": whole_indexed_tags,\n",
    "        \"split\": splits,\n",
    "        \"fileid\": whole_files,\n",
    "    }\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:26:48.430868Z",
     "iopub.status.busy": "2020-11-12T10:26:48.430246Z",
     "iopub.status.idle": "2020-11-12T10:26:48.498363Z",
     "shell.execute_reply": "2020-11-12T10:26:48.497261Z",
     "shell.execute_reply.started": "2020-11-12T10:26:48.430800Z"
    }
   },
   "outputs": [],
   "source": [
    "class DependencyTreebankDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dependency treebank dataset for POS tagging\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        tokens = df.iloc[index][\"indexed_tokens\"]\n",
    "        if not isinstance(tokens, list):\n",
    "            tokens = tokens.tolist()\n",
    "        tags = df.iloc[index][\"indexed_tags\"]\n",
    "        if not isinstance(tags, list):\n",
    "            tags = tags.tolist()\n",
    "        sample = list(zip(tokens, tags))\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:26:49.624198Z",
     "iopub.status.busy": "2020-11-12T10:26:49.623706Z",
     "iopub.status.idle": "2020-11-12T10:26:49.694144Z",
     "shell.execute_reply": "2020-11-12T10:26:49.692979Z",
     "shell.execute_reply.started": "2020-11-12T10:26:49.624131Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"train\"])\n",
    "val_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"val\"])\n",
    "test_dataset = DependencyTreebankDataset(df[df[\"split\"] == \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:26:50.861106Z",
     "iopub.status.busy": "2020-11-12T10:26:50.860785Z",
     "iopub.status.idle": "2020-11-12T10:26:50.924526Z",
     "shell.execute_reply": "2020-11-12T10:26:50.923205Z",
     "shell.execute_reply.started": "2020-11-12T10:26:50.861066Z"
    }
   },
   "outputs": [],
   "source": [
    "def pad_batch(batch):\n",
    "    data = [item[0] for item in batch]\n",
    "    data = pack_sequence(data, enforce_sorted=False)\n",
    "    targets = [item[1] for item in batch]\n",
    "    return [data, targets]\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "default_dataloader = partial(\n",
    "    DataLoader,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_batch,\n",
    "    pin_memory=True,\n",
    ")\n",
    "train_dataloader = default_dataloader(train_dataset)\n",
    "val_dataloader = default_dataloader(val_dataset)\n",
    "test_dataloader = default_dataloader(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-11T21:00:47.601889Z",
     "iopub.status.busy": "2020-11-11T21:00:47.601467Z",
     "iopub.status.idle": "2020-11-11T21:01:36.722537Z",
     "shell.execute_reply": "2020-11-11T21:01:36.720656Z",
     "shell.execute_reply.started": "2020-11-11T21:00:47.601844Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dimension = 50\n",
    "embedding_model = utils.load_embedding_model(\"glove\", embedding_dimension=embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:06:19.747020Z",
     "iopub.status.busy": "2020-11-12T09:06:19.746574Z",
     "iopub.status.idle": "2020-11-12T09:06:19.815814Z",
     "shell.execute_reply": "2020-11-12T09:06:19.814634Z",
     "shell.execute_reply.started": "2020-11-12T09:06:19.746976Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_oov_terms(embedding_model, word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms\n",
    "    \"\"\"\n",
    "    oov_terms = []\n",
    "    for word in word_listing:\n",
    "        if word not in embedding_model.vocab:\n",
    "            oov_terms.append(word)\n",
    "    return oov_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:06:21.511027Z",
     "iopub.status.busy": "2020-11-12T09:06:21.510703Z",
     "iopub.status.idle": "2020-11-12T09:06:21.588619Z",
     "shell.execute_reply": "2020-11-12T09:06:21.587383Z",
     "shell.execute_reply.started": "2020-11-12T09:06:21.510986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 3745 (0.31%)\n"
     ]
    }
   ],
   "source": [
    "oov_terms = check_oov_terms(embedding_model, word_listing)\n",
    "print(\n",
    "    f\"Total OOV terms: {len(oov_terms)} ({round(len(oov_terms) / len(word_listing), 2)}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:08:24.119332Z",
     "iopub.status.busy": "2020-11-12T09:08:24.118910Z",
     "iopub.status.idle": "2020-11-12T09:08:24.182304Z",
     "shell.execute_reply": "2020-11-12T09:08:24.181065Z",
     "shell.execute_reply.started": "2020-11-12T09:08:24.119288Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_index,\n",
    "    oov_terms,\n",
    "    method=\"normal\",\n",
    "    padding_token=\"0\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained Gensim word embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    def uniform_embedding(embedding_dimension, interval=(-1, 1)):\n",
    "        return interval[0] + np.random.sample(embedding_dimension) + interval[1]\n",
    "\n",
    "    def normal_embedding(embedding_dimension):\n",
    "        return np.random.normal(embedding_dimension)\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_index), embedding_dimension))\n",
    "    for word, index in word_to_index.items():\n",
    "        if word == padding_token:\n",
    "            word_vector = np.zeros((1, embedding_dimension))\n",
    "        # Words that are no OOV are taken from the Gensim model\n",
    "        elif word not in oov_terms:\n",
    "            word_vector = embedding_model[word]\n",
    "        # OOV words computed as random normal vectors\n",
    "        elif method == \"normal\":\n",
    "            word_vector = normal_embedding(embedding_dimension)\n",
    "        # OOV words computed as uniform vectors in range [-1, 1]\n",
    "        elif method == \"uniform\":\n",
    "            word_vector = uniform_embedding(embedding_dimension)\n",
    "        embedding_matrix[index, :] = word_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:08:28.126330Z",
     "iopub.status.busy": "2020-11-12T09:08:28.125920Z",
     "iopub.status.idle": "2020-11-12T09:08:30.029531Z",
     "shell.execute_reply": "2020-11-12T09:08:30.028336Z",
     "shell.execute_reply.started": "2020-11-12T09:08:28.126286Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,\n",
       "        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,\n",
       "       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,\n",
       "       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,\n",
       "        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,\n",
       "       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,\n",
       "       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,\n",
       "       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,\n",
       "       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,\n",
       "        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,\n",
       "       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,\n",
       "       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,\n",
       "       -1.15139998e-01, -7.85809994e-01])"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = build_embedding_matrix(\n",
    "    embedding_model, embedding_dimension, word_to_index, oov_terms, method=\"normal\",\n",
    ")\n",
    "embedding_matrix[word_to_index[\"the\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:44:15.438724Z",
     "iopub.status.busy": "2020-11-12T09:44:15.438305Z",
     "iopub.status.idle": "2020-11-12T09:44:15.498688Z",
     "shell.execute_reply": "2020-11-12T09:44:15.497271Z",
     "shell.execute_reply.started": "2020-11-12T09:44:15.438680Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:10:46.996338Z",
     "iopub.status.busy": "2020-11-12T09:10:46.995898Z",
     "iopub.status.idle": "2020-11-12T09:10:47.072528Z",
     "shell.execute_reply": "2020-11-12T09:10:47.071217Z",
     "shell.execute_reply.started": "2020-11-12T09:10:46.996293Z"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dimension,\n",
    "        embedding_dimension,\n",
    "        hidden_dimension,\n",
    "        output_dimension,\n",
    "        num_layers=1,\n",
    "        bidirectional=True,\n",
    "        dropout_rate=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            input_dimension, embedding_dimension, padding_idx=0\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dimension,\n",
    "            hidden_dimension,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.fc = nn.Linear(\n",
    "            hidden_dimension * 2 if bidirectional else hidden_dimension,\n",
    "            output_dimension,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:34:37.469170Z",
     "iopub.status.busy": "2020-11-12T09:34:37.468748Z",
     "iopub.status.idle": "2020-11-12T09:34:37.556027Z",
     "shell.execute_reply": "2020-11-12T09:34:37.554760Z",
     "shell.execute_reply.started": "2020-11-12T09:34:37.469125Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_dimension = 128\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.0\n",
    "\n",
    "model = BiLSTMPOSTagger(\n",
    "    len(word_to_index),\n",
    "    embedding_dimension,\n",
    "    hidden_dimension,\n",
    "    len(tag_to_index),\n",
    "    num_layers=num_layers,\n",
    "    bidirectional=bidirectional,\n",
    "    dropout_rate=dropout_rate,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:34:39.649213Z",
     "iopub.status.busy": "2020-11-12T09:34:39.648874Z",
     "iopub.status.idle": "2020-11-12T09:34:39.716039Z",
     "shell.execute_reply": "2020-11-12T09:34:39.714798Z",
     "shell.execute_reply.started": "2020-11-12T09:34:39.649172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 794,592 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:34:40.783968Z",
     "iopub.status.busy": "2020-11-12T09:34:40.783643Z",
     "iopub.status.idle": "2020-11-12T09:34:40.908526Z",
     "shell.execute_reply": "2020-11-12T09:34:40.907453Z",
     "shell.execute_reply.started": "2020-11-12T09:34:40.783928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiLSTMPOSTagger(\n",
       "  (embedding): Embedding(11969, 50, padding_idx=0)\n",
       "  (lstm): LSTM(50, 128, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=46, bias=True)\n",
       "  (dropout): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean = 0, std = 0.1)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:44:51.429733Z",
     "iopub.status.busy": "2020-11-12T09:44:51.429319Z",
     "iopub.status.idle": "2020-11-12T09:44:51.496717Z",
     "shell.execute_reply": "2020-11-12T09:44:51.495437Z",
     "shell.execute_reply.started": "2020-11-12T09:44:51.429689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.5840,  0.3903,  0.6528,  ..., -1.2338,  0.4672,  0.7886],\n",
       "        [-1.5925,  0.7557,  1.4947,  ..., -0.8408,  0.4949,  0.3405],\n",
       "        ...,\n",
       "        [-0.7826, -0.3365,  1.2228,  ..., -0.3224,  0.2601, -0.0665],\n",
       "        [-0.3452, -0.0519,  0.0702,  ...,  0.2199,  0.1535, -0.1313],\n",
       "        [ 0.0380, -0.9954,  1.7751,  ..., -0.1395, -0.4050, -0.6642]])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:34:44.534123Z",
     "iopub.status.busy": "2020-11-12T09:34:44.533795Z",
     "iopub.status.idle": "2020-11-12T09:34:44.591270Z",
     "shell.execute_reply": "2020-11-12T09:34:44.589905Z",
     "shell.execute_reply.started": "2020-11-12T09:34:44.534082Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:34:46.651309Z",
     "iopub.status.busy": "2020-11-12T09:34:46.650986Z",
     "iopub.status.idle": "2020-11-12T09:34:46.710027Z",
     "shell.execute_reply": "2020-11-12T09:34:46.708843Z",
     "shell.execute_reply.started": "2020-11-12T09:34:46.651266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T09:15:38.427864Z",
     "iopub.status.busy": "2020-11-12T09:15:38.427203Z",
     "iopub.status.idle": "2020-11-12T09:15:38.494251Z",
     "shell.execute_reply": "2020-11-12T09:15:38.493063Z",
     "shell.execute_reply.started": "2020-11-12T09:15:38.427797Z"
    }
   },
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    max_preds = preds.argmax(dim=1, keepdim=True)\n",
    "    non_pad_elements = (y != 0).nonzero()\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:22:40.994562Z",
     "iopub.status.busy": "2020-11-12T10:22:40.994126Z",
     "iopub.status.idle": "2020-11-12T10:22:41.069489Z",
     "shell.execute_reply": "2020-11-12T10:22:41.068067Z",
     "shell.execute_reply.started": "2020-11-12T10:22:40.994518Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, criterion):\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    model.train()\n",
    "    for tokens, tags in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(tokens)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = categorical_accuracy(predictions, tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:23:04.612519Z",
     "iopub.status.busy": "2020-11-12T10:23:04.612089Z",
     "iopub.status.idle": "2020-11-12T10:23:04.675824Z",
     "shell.execute_reply": "2020-11-12T10:23:04.674425Z",
     "shell.execute_reply.started": "2020-11-12T10:23:04.612475Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    epoch_loss, epoch_acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for tokens, tags in dataloader:\n",
    "            predictions = model(tokens)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-12T10:30:27.055647Z",
     "iopub.status.busy": "2020-11-12T10:30:27.055210Z",
     "iopub.status.idle": "2020-11-12T10:30:27.216833Z",
     "shell.execute_reply": "2020-11-12T10:30:27.214729Z",
     "shell.execute_reply.started": "2020-11-12T10:30:27.055584Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-289-f0f2d4aad78e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-273-27289c39ba8f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-287-5030a2829c2d>\u001b[0m in \u001b[0;36mpad_batch\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpad_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_sequence\u001b[0;34m(sequences, enforce_sorted)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPackedSequence\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \"\"\"\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPackedSequence\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \"\"\"\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menforce_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_dataloader, criterion)\n",
    "    end_time = time.time()\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"models/model.pt\")\n",
    "\n",
    "    print(f\"Epoch: {epoch+1:02} | Epoch Time: {end_time - start_time}s\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%\")\n",
    "    print(f\"\\t Val. Loss: {val_loss:.3f} |  Val. Acc: {val_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

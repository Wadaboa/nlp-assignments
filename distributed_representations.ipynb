{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we leverage the IMDB movie review dataset (which was already analyzed for the purpose of scores classification) to test various textual embeddings, going from sparse to dense representations.\n",
    "\n",
    "In particular, we will try to see how different embeddings reflect in different semantic concepts and how much each embedding can be trusted for semantic purposes.\n",
    "\n",
    "In order to keep things efficient, we exploit the use of sparse matrices whenever we can (we will stick to the usage of `csr_matrix` by the `scipy` package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T08:31:17.652643Z",
     "iopub.status.busy": "2020-11-06T08:31:17.652209Z",
     "iopub.status.idle": "2020-11-06T08:31:17.717817Z",
     "shell.execute_reply": "2020-11-06T08:31:17.716750Z",
     "shell.execute_reply.started": "2020-11-06T08:31:17.652599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity as fast_cosine\n",
    "\n",
    "import utils\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:47:53.479177Z",
     "iopub.status.busy": "2020-11-05T20:47:53.478755Z",
     "iopub.status.idle": "2020-11-05T20:47:53.529541Z",
     "shell.execute_reply": "2020-11-05T20:47:53.528484Z",
     "shell.execute_reply.started": "2020-11-05T20:47:53.479133Z"
    }
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is dedicated to data loading and data preparation. An important step is that the entire dataset gets preprocessed right from the start, using the `preprocess_text` function from the `utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:48:28.911983Z",
     "iopub.status.busy": "2020-11-05T20:48:28.911463Z",
     "iopub.status.idle": "2020-11-05T20:48:55.773315Z",
     "shell.execute_reply": "2020-11-05T20:48:55.772015Z",
     "shell.execute_reply.started": "2020-11-05T20:48:28.911939Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor = partial(utils.preprocess_text, regexes=True, start_end_symbols=True)\n",
    "dataset = utils.IMDBDataset(preprocessor=preprocessor)\n",
    "df = dataset.dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:48:55.850713Z",
     "iopub.status.busy": "2020-11-05T20:48:55.850410Z",
     "iopub.status.idle": "2020-11-05T20:48:55.908159Z",
     "shell.execute_reply": "2020-11-05T20:48:55.906522Z",
     "shell.execute_reply.started": "2020-11-05T20:48:55.850676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2257</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; sarafina was a fun movie, and some of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4778</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; like his early masterpiece \"the elephant m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7284</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; when i was young i had seen very few movie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4845</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; hello playmates.i recently watched this fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6822</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; \"opening night\" released in tries to be an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  file_id score  sentiment  split  \\\n",
       "0    2257     7          1  train   \n",
       "1    4778     9          1  train   \n",
       "2    7284     8          1  train   \n",
       "3    4845     9          1  train   \n",
       "4    6822     7          1  train   \n",
       "\n",
       "                                                text  \n",
       "0  <s> sarafina was a fun movie, and some of the ...  \n",
       "1  <s> like his early masterpiece \"the elephant m...  \n",
       "2  <s> when i was young i had seen very few movie...  \n",
       "3  <s> hello playmates.i recently watched this fi...  \n",
       "4  <s> \"opening night\" released in tries to be an...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to keep things simple and fast, we will extract a random portion of the entire dataset and do experimentation with this limited subset of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:48:55.910907Z",
     "iopub.status.busy": "2020-11-05T20:48:55.910453Z",
     "iopub.status.idle": "2020-11-05T20:48:55.967856Z",
     "shell.execute_reply": "2020-11-05T20:48:55.966319Z",
     "shell.execute_reply.started": "2020-11-05T20:48:55.910853Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df = dataset.get_portion(amount=500, seed=RANDOM_SEED)\n",
    "small_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:48:55.970168Z",
     "iopub.status.busy": "2020-11-05T20:48:55.969711Z",
     "iopub.status.idle": "2020-11-05T20:48:56.031888Z",
     "shell.execute_reply": "2020-11-05T20:48:56.030610Z",
     "shell.execute_reply.started": "2020-11-05T20:48:55.970113Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33553</th>\n",
       "      <td>4989</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;s&gt; too much added with too much taken away fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9427</th>\n",
       "      <td>6186</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; released just before the production code c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>8806</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; i chose to see the this film on the day it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12447</th>\n",
       "      <td>9759</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>&lt;s&gt; i believe i received this film when i was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39489</th>\n",
       "      <td>445</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>test</td>\n",
       "      <td>&lt;s&gt; once upon a time there was a great america...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id score  sentiment  split  \\\n",
       "33553    4989     7          1   test   \n",
       "9427     6186    10          1  train   \n",
       "199      8806     9          1  train   \n",
       "12447    9759    10          1  train   \n",
       "39489     445     3          0   test   \n",
       "\n",
       "                                                    text  \n",
       "33553  <s> too much added with too much taken away fr...  \n",
       "9427   <s> released just before the production code c...  \n",
       "199    <s> i chose to see the this film on the day it...  \n",
       "12447  <s> i believe i received this film when i was ...  \n",
       "39489  <s> once upon a time there was a great america...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `build_vocabulary` function takes a dataframe containing text documents in each row and extracts the set of words that appear in those documents. \n",
    "\n",
    "For example, if we assume to have a toy dataframe with just the two following \"documents\" (already pre-processed)\n",
    "\n",
    "1. \"hi my name is alessio hi how are you\"\n",
    "2. \"hi my name is lorenzo who are you\"\n",
    "\n",
    "Then, our function will output the following vocabulary:\n",
    "\n",
    "[\"alessio\", \"are\", \"hi\", \"how\", \"is\", \"lorenzo\", \"my\", \"name\", \"who\", \"you\"]\n",
    "\n",
    "As you can see, the vocabulary is sorted in ascending order, so that with the same documents we always get the same output.\n",
    "\n",
    "Moreover, the function will also output two more data structures, which will be used throughout the notebook for convenience:\n",
    "\n",
    "- Index to word dictionary: {0: \"alessio\", 1: \"are\", 2: \"hi\", 3: \"how\", 4: \"is\", 5: \"lorenzo\", 6: \"my\", 7: \"name\", 8: \"who\", 9: \"you\"}\n",
    "- Word to index dictionary: {\"alessio\": 0, \"are\": 1, \"hi\": 2, \"how\": 3, \"is\": 4, \"lorenzo\": 5, \"my\": 6, \"name\": 7, \"who\": 8, \"you\": 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T09:44:00.823060Z",
     "iopub.status.busy": "2020-11-06T09:44:00.822731Z",
     "iopub.status.idle": "2020-11-06T09:44:00.889216Z",
     "shell.execute_reply": "2020-11-06T09:44:00.888032Z",
     "shell.execute_reply.started": "2020-11-06T09:44:00.823019Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vocabulary(df, col_name=\"text\"):\n",
    "    \"\"\"\n",
    "    Given a dataset, builds the corresponding word vocabulary\n",
    "    \"\"\"\n",
    "    doc_str = \" \".join(df[col_name].tolist()).replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    words = sorted(set(doc_str.split()))\n",
    "    vocabulary, inverse_vocabulary = dict(), dict()\n",
    "    for i, w in tqdm(enumerate(words)):\n",
    "        vocabulary[i] = w\n",
    "        inverse_vocabulary[w] = i\n",
    "    return vocabulary, inverse_vocabulary, words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test the function to see if it actually works as intended$\\dots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T09:45:15.089019Z",
     "iopub.status.busy": "2020-11-06T09:45:15.088688Z",
     "iopub.status.idle": "2020-11-06T09:45:15.160234Z",
     "shell.execute_reply": "2020-11-06T09:45:15.157435Z",
     "shell.execute_reply.started": "2020-11-06T09:45:15.088977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 67869.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word listing: ['alessio', 'are', 'hi', 'how', 'is', 'lorenzo', 'my', 'name', 'who', 'you']\n",
      "Index to word dictionary: {0: 'alessio', 1: 'are', 2: 'hi', 3: 'how', 4: 'is', 5: 'lorenzo', 6: 'my', 7: 'name', 8: 'who', 9: 'you'}\n",
      "Word to index dictionary: {'alessio': 0, 'are': 1, 'hi': 2, 'how': 3, 'is': 4, 'lorenzo': 5, 'my': 6, 'name': 7, 'who': 8, 'you': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "toy_df = pd.DataFrame(\n",
    "    {\"text\": [\"hi my name is alessio hi how are you\", \"hi my name is lorenzo who are you\"]}\n",
    ")\n",
    "toy_idx_to_word, toy_word_to_idx, toy_word_listing = build_vocabulary(toy_df)\n",
    "print(f\"Word listing: {toy_word_listing}\")\n",
    "print(f\"Index to word dictionary: {toy_idx_to_word}\")\n",
    "print(f\"Word to index dictionary: {toy_word_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the function seems to work properly, we can build the vocabulary for our specific dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:49:09.021451Z",
     "iopub.status.busy": "2020-11-05T20:49:09.021036Z",
     "iopub.status.idle": "2020-11-05T20:49:09.142860Z",
     "shell.execute_reply": "2020-11-05T20:49:09.141793Z",
     "shell.execute_reply.started": "2020-11-05T20:49:09.021407Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19383it [00:00, 732752.84it/s]\n"
     ]
    }
   ],
   "source": [
    "idx_to_word, word_to_idx, word_listing = build_vocabulary(small_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:49:10.479322Z",
     "iopub.status.busy": "2020-11-05T20:49:10.478971Z",
     "iopub.status.idle": "2020-11-05T20:49:10.530390Z",
     "shell.execute_reply": "2020-11-05T20:49:10.528996Z",
     "shell.execute_reply.started": "2020-11-05T20:49:10.479282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19383"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_listing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:35:21.681600Z",
     "iopub.status.busy": "2020-11-05T21:35:21.681170Z",
     "iopub.status.idle": "2020-11-05T21:35:21.758316Z",
     "shell.execute_reply": "2020-11-05T21:35:21.757010Z",
     "shell.execute_reply.started": "2020-11-05T21:35:21.681554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '!'), (1, '!!!'), (2, '!....'), (3, '!....being'), (4, '\"')]\n",
      "[('£', 19378), ('£for', 19379), ('£it', 19380), ('½', 19381), ('½/*****).', 19382)]\n"
     ]
    }
   ],
   "source": [
    "print(list(idx_to_word.items())[:5])\n",
    "print(list(word_to_idx.items())[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to define different functions to assess similarity of words. \n",
    "\n",
    "In particular, the most important one is about the computation of cosine similarity: about that, we give two different implementations, one that follows the definition of the measure (which is not so efficient) and one which exploits the highly-efficient implementation of the `sklearn` package (which is around $10x$ faster than my hand-made method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(p, q, transpose_p=False, transpose_q=False):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity of two d-dimensional matrices,\n",
    "    where their second dimension should match\n",
    "    \"\"\"\n",
    "    # If it is a vector, consider it as a single sample matrix\n",
    "    if len(p.shape) == 1:\n",
    "        p = p.reshape(1, -1)\n",
    "    if len(q.shape) == 1:\n",
    "        q = q.reshape(1, -1)\n",
    "\n",
    "    # Check if dimensions match\n",
    "    assert p.shape[1] == q.shape[1]\n",
    "\n",
    "    # Check for sparsity\n",
    "    if not hasattr(scipy.sparse, type(p).__name__):\n",
    "        p = scipy.sparse.csr_matrix(p)\n",
    "    if not hasattr(scipy.sparse, type(q).__name__):\n",
    "        q = scipy.sparse.csr_matrix(q)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    p_norm = np.sqrt(p.dot(p.T).diagonal())\n",
    "    q_norm = np.sqrt(q.dot(q.T).diagonal())\n",
    "    norms_prod = np.outer(p_norm, q_norm)\n",
    "    if transpose_p:\n",
    "        res = q.dot(p.T) / norms_prod\n",
    "    else:\n",
    "        res = p.dot(q.T) / norms_prod\n",
    "        \n",
    "    return scipy.sparse.csr_matrix(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_cosine_similarity(p, q, transpose_p=False, transpose_q=False, to_dense=False):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity of two d-dimensional matrices,\n",
    "    using sklearn implementation\n",
    "    \"\"\"\n",
    "    # If it is a vector, consider it as a single sample matrix\n",
    "    if len(p.shape) == 1:\n",
    "        p = p.reshape(1, -1)\n",
    "    if len(q.shape) == 1:\n",
    "        q = q.reshape(1, -1)\n",
    "\n",
    "    # Check for sparsity\n",
    "    if not hasattr(scipy.sparse, type(p).__name__):\n",
    "        p = scipy.sparse.csr_matrix(p)\n",
    "    if not hasattr(scipy.sparse, type(q).__name__):\n",
    "        q = scipy.sparse.csr_matrix(q)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    return (\n",
    "        fast_cosine(p, q, dense_output=to_dense)\n",
    "        if transpose_q\n",
    "        else fast_cosine(q, p, dense_output=to_dense)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions are dedicated to the computation of semantic concepts, like synonyms (ideally words with high similarity) and antonyms (ideally words with low similarity).\n",
    "\n",
    "Moreover, the `get_analogy` method tries to solve the analogy problem: for four words in the analogical relationship $a : b = c : x$ , given the first three words, $a$, $b$ and $c$, we want to find $x$. Assume the word vector for the word $w$ is $v(w)$. To solve the analogy problem, we need to find the word vector that is most similar to the result vector of $v(c)+v(b)-v(a)$. From a geometric point of view, it boils down to finding the closest point to the vertex $v(x)$ of a parallelogram, where the other vertices are given by the vector representations of $a$, $b$ and $c$.\n",
    "\n",
    "Simple examples of analogies are the following:\n",
    "- Male - female: $man : woman = son : x$ ($x$ should be $daughter$)\n",
    "- Capital - country: $beijing : china = tokyo : x$ ($x$ should be $japan$)\n",
    "- Adjective - superlative adjective: $bad : worst = big : x$ ($x$ should be $biggest$)\n",
    "- Present tense verb - past tense verb: $do : did = go : x$ ($x$ should be $went$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_knn(word, similarity_matrix, word_to_idx, k=1, farthest=False):\n",
    "    '''\n",
    "    Find the k-nearest neighbors to the given word\n",
    "    '''\n",
    "    index = word_to_idx[word]\n",
    "    similarities = []\n",
    "    for w, i in word_to_idx.items():\n",
    "        similarities.append((w, similarity_matrix[index, i]))\n",
    "    return sorted(similarities, key=lambda t: t[1], reverse=(not farthest))[1 : k + 1]\n",
    "\n",
    "def vec_knn(vec, similarity_matrix, word_to_idx, k=1, farthest=False):\n",
    "    '''\n",
    "    Find the k-nearest neighbors to the given vector\n",
    "    '''\n",
    "    similarities = []\n",
    "    for w, i in word_to_idx.items():\n",
    "        similarities.append((w, similarity_matrix[i, 0]))\n",
    "    return sorted(similarities, key=lambda t: t[1], reverse=(not farthest))[1 : k + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogy(\n",
    "    embedding_matrix,\n",
    "    word_to_idx,\n",
    "    token_a,\n",
    "    token_b,\n",
    "    token_c,\n",
    "    k=1,\n",
    "    farthest=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the analogy a : b = c : x, find the word x which completes it,\n",
    "    s.t. x is the most similar word to c + b - a\n",
    "    \"\"\"\n",
    "    # Compute the x vector\n",
    "    token_a_idx, token_b_idx, token_c_idx = (\n",
    "        word_to_idx[token_a],\n",
    "        word_to_idx[token_b],\n",
    "        word_to_idx[token_c],\n",
    "    )\n",
    "    vecs = embedding_matrix[[token_a_idx, token_b_idx, token_c_idx]]\n",
    "    x = vecs[1] - vecs[0] + vecs[2]\n",
    "    if hasattr(scipy.sparse, type(x).__name__):\n",
    "        x = x.toarray()\n",
    "    \n",
    "    # Find the analogies\n",
    "    similarity_matrix = fast_cosine_similarity(x, embedding_matrix, transpose_q=True)\n",
    "    analogies = vec_knn(x, similarity_matrix.transpose(), word_to_idx, k=k, farthest=farthest)\n",
    "    return analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first type of embeddings that we may want to try is in the sparse realm and can be viewed as a direct evolution of `BoW` methods, going from document by term matrices to word-word ones. \n",
    "\n",
    "In this notebook we are going to explore two popular embeddings:\n",
    "1. The raw co-occurrence count matrix (which can be directly related to the straight document by term matrix of language models)\n",
    "2. The PPMI matrix (which can be related to the reweighting scheme of TF-IDF in language models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-occurrence count matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The co-occurrence count matrix represents words by the context they appear in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:49:14.570985Z",
     "iopub.status.busy": "2020-11-05T20:49:14.570564Z",
     "iopub.status.idle": "2020-11-05T20:49:14.623175Z",
     "shell.execute_reply": "2020-11-05T20:49:14.622072Z",
     "shell.execute_reply.started": "2020-11-05T20:49:14.570942Z"
    }
   },
   "outputs": [],
   "source": [
    "def dict_to_csr(term_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary like {(i, j): v}, returns a sparse matrix m\n",
    "    s.t. m[i, j] = v\n",
    "    \"\"\"\n",
    "    keys = list(term_dict.keys())\n",
    "    values = list(term_dict.values())\n",
    "    shape = list(np.repeat(np.asarray(keys).max() + 1, 2))\n",
    "    csr = scipy.sparse.csr_matrix((values, zip(*keys)), shape=shape)\n",
    "    return csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:49:14.990666Z",
     "iopub.status.busy": "2020-11-05T20:49:14.990356Z",
     "iopub.status.idle": "2020-11-05T20:49:15.040653Z",
     "shell.execute_reply": "2020-11-05T20:49:15.039512Z",
     "shell.execute_reply.started": "2020-11-05T20:49:14.990627Z"
    }
   },
   "outputs": [],
   "source": [
    "def co_occurrence_count(df, idx_to_word, word_to_idx, window_size=4):\n",
    "    \"\"\"\n",
    "    Builds word-word co-occurrence matrix based on word counts\n",
    "    \"\"\"\n",
    "    counts = dict()\n",
    "    for doc in tqdm(df[\"text\"]):\n",
    "        doc_words = doc.split()\n",
    "        for doc_word_index, central_word in enumerate(doc_words):\n",
    "            central_word_index = word_to_idx[central_word]\n",
    "            context = (\n",
    "                doc_words[max(0, doc_word_index - window_size) : doc_word_index] + \n",
    "                doc_words[doc_word_index + 1 : min(doc_word_index + window_size + 1, len(doc_words))]\n",
    "            )\n",
    "            for context_word in context:   \n",
    "                context_word_index = word_to_idx[context_word]\n",
    "                key = (central_word_index, context_word_index)\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "    sparse_matrix = dict_to_csr(counts)\n",
    "    del counts\n",
    "    return sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our vocabulary building method and, more importantly, if the co-occurrence count matrix is correct. To do so, we will use the previously defined toy dataframe containing just two \"documents\".\n",
    "\n",
    "Given a window size of $1$ and the sorted vocabulary, we would like to get the following (densified) output:\n",
    "\n",
    "|         | alessio | are | hi | how | is | lorenzo | my | name | who | you |\n",
    "|:-------:|:-------:|:---:|:--:|:---:|:--:|:-------:|:--:|:----:|:---:|:---:|\n",
    "| alessio |    0    |  0  |  1 |  0  |  1 |    0    |  0 |   0  |  0  |  0  |\n",
    "|   are   |    0    |  0  |  0 |  1  |  0 |    0    |  0 |   0  |  1  |  2  |\n",
    "|    hi   |    1    |  0  |  0 |  1  |  0 |    0    |  2 |   0  |  0  |  0  |\n",
    "|   how   |    0    |  1  |  1 |  0  |  0 |    0    |  0 |   0  |  0  |  0  |\n",
    "|    is   |    1    |  0  |  0 |  0  |  0 |    1    |  0 |   2  |  0  |  0  |\n",
    "| lorenzo |    0    |  0  |  0 |  0  |  1 |    0    |  0 |   0  |  1  |  0  |\n",
    "|    my   |    0    |  0  |  2 |  0  |  0 |    0    |  0 |   2  |  0  |  0  |\n",
    "|   name  |    0    |  0  |  0 |  0  |  2 |    0    |  2 |   0  |  0  |  0  |\n",
    "|   who   |    0    |  1  |  0 |  0  |  0 |    1    |  0 |   0  |  0  |  0  |\n",
    "|   you   |    0    |  2  |  0 |  0  |  0 |    0    |  0 |   0  |  0  |  0  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T09:13:09.580247Z",
     "iopub.status.busy": "2020-11-06T09:13:09.579796Z",
     "iopub.status.idle": "2020-11-06T09:13:09.662283Z",
     "shell.execute_reply": "2020-11-06T09:13:09.661157Z",
     "shell.execute_reply.started": "2020-11-06T09:13:09.580200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:00, 68759.08it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 8962.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index to word dictionary: {0: 'alessio', 1: 'are', 2: 'hi', 3: 'how', 4: 'is', 5: 'lorenzo', 6: 'my', 7: 'name', 8: 'who', 9: 'you'}\n",
      "Word to index dictionary: {'alessio': 0, 'are': 1, 'hi': 2, 'how': 3, 'is': 4, 'lorenzo': 5, 'my': 6, 'name': 7, 'who': 8, 'you': 9}\n",
      "Word listing: ['alessio', 'are', 'hi', 'how', 'is', 'lorenzo', 'my', 'name', 'who', 'you']\n",
      "Co-occurrence count matrix:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 1, 2],\n",
       "       [1, 0, 0, 1, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 2, 0, 0, 0, 0, 2, 0, 0],\n",
       "       [0, 0, 0, 0, 2, 0, 2, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 2, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy_co_occurrence_matrix = co_occurrence_count(\n",
    "    toy_df, toy_idx_to_word, toy_word_to_idx, window_size=1\n",
    ")\n",
    "print(\"Co-occurrence count matrix:\")\n",
    "toy_co_occurrence_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:49:21.480123Z",
     "iopub.status.busy": "2020-11-05T20:49:21.479710Z",
     "iopub.status.idle": "2020-11-05T20:49:25.472678Z",
     "shell.execute_reply": "2020-11-05T20:49:25.471331Z",
     "shell.execute_reply.started": "2020-11-05T20:49:21.480078Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 357.54it/s]\n"
     ]
    }
   ],
   "source": [
    "if \"co_occurrence_matrix\" in globals():\n",
    "    del co_occurrence_matrix\n",
    "    gc.collect()\n",
    "    time.sleep(10.0)\n",
    "\n",
    "window_size = 4\n",
    "co_occurrence_matrix = co_occurrence_count(\n",
    "    small_df, idx_to_word, word_to_idx, window_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:49:25.612856Z",
     "iopub.status.busy": "2020-11-05T20:49:25.612540Z",
     "iopub.status.idle": "2020-11-05T20:49:25.664205Z",
     "shell.execute_reply": "2020-11-05T20:49:25.662958Z",
     "shell.execute_reply.started": "2020-11-05T20:49:25.612816Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19383x19383 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 506894 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:53:29.984887Z",
     "iopub.status.busy": "2020-11-05T20:53:29.984475Z",
     "iopub.status.idle": "2020-11-05T20:53:30.411488Z",
     "shell.execute_reply": "2020-11-05T20:53:30.409996Z",
     "shell.execute_reply.started": "2020-11-05T20:53:29.984843Z"
    }
   },
   "outputs": [],
   "source": [
    "coo_svd = utils.reduce_svd(co_occurrence_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:53:37.938818Z",
     "iopub.status.busy": "2020-11-05T20:53:37.938391Z",
     "iopub.status.idle": "2020-11-05T20:53:38.070232Z",
     "shell.execute_reply": "2020-11-05T20:53:38.069166Z",
     "shell.execute_reply.started": "2020-11-05T20:53:37.938773Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a8a46ef2134091a1eb51a6496f2fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.visualize_embeddings(coo_svd, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:54:35.480979Z",
     "iopub.status.busy": "2020-11-05T20:54:35.480363Z",
     "iopub.status.idle": "2020-11-05T20:57:09.342217Z",
     "shell.execute_reply": "2020-11-05T20:57:09.340206Z",
     "shell.execute_reply.started": "2020-11-05T20:54:35.480909Z"
    }
   },
   "outputs": [],
   "source": [
    "coo_tsne = utils.reduce_tsne(co_occurrence_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T20:57:09.515475Z",
     "iopub.status.busy": "2020-11-05T20:57:09.515151Z",
     "iopub.status.idle": "2020-11-05T20:57:09.657166Z",
     "shell.execute_reply": "2020-11-05T20:57:09.655881Z",
     "shell.execute_reply.started": "2020-11-05T20:57:09.515434Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31260e67bdab49a2b7d912a2ad472e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.visualize_embeddings(coo_tsne, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:17:11.172081Z",
     "iopub.status.busy": "2020-11-05T21:17:11.171657Z",
     "iopub.status.idle": "2020-11-05T21:17:19.684005Z",
     "shell.execute_reply": "2020-11-05T21:17:19.682668Z",
     "shell.execute_reply.started": "2020-11-05T21:17:11.172037Z"
    }
   },
   "outputs": [],
   "source": [
    "coo_similarity_matrix = fast_cosine_similarity(\n",
    "    co_occurrence_matrix, co_occurrence_matrix, transpose_q=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:17:56.951525Z",
     "iopub.status.busy": "2020-11-05T21:17:56.951188Z",
     "iopub.status.idle": "2020-11-05T21:17:57.017465Z",
     "shell.execute_reply": "2020-11-05T21:17:57.016272Z",
     "shell.execute_reply.started": "2020-11-05T21:17:56.951484Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19383x19383 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 227364251 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T08:18:12.722014Z",
     "iopub.status.busy": "2020-11-06T08:18:12.721678Z",
     "iopub.status.idle": "2020-11-06T08:18:20.826205Z",
     "shell.execute_reply": "2020-11-06T08:18:20.824916Z",
     "shell.execute_reply.started": "2020-11-06T08:18:12.721973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('movie', 0.9700280746239791), ('film,', 0.9463668360456202), ('film.', 0.9461475062242477), ('in', 0.9428380248016573), ('is', 0.9400285409566773)]\n",
      "[('\"euro\".', 0.0), ('\"good\"haha.', 0.0), ('\"india', 0.0), ('\"left-wing', 0.0), ('\"nearly', 0.0)]\n",
      "[('very', 0.9252070826651666), ('little', 0.9155640236149524), ('great', 0.9123573420111016), ('nice', 0.8878501392324601), ('as', 0.8862166197402453)]\n",
      "[('(golden', 0.0), ('(little', 0.0), ('(spark,', 0.0), ('-),', 0.0), ('-episodes', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(word_knn(\"film\", coo_similarity_matrix, word_to_idx, k=5))\n",
    "print(word_knn(\"amazing\", coo_similarity_matrix, word_to_idx, k=5, farthest=True))\n",
    "print(word_knn(\"good\", coo_similarity_matrix, word_to_idx, k=5))\n",
    "print(word_knn(\"good\", coo_similarity_matrix, word_to_idx, k=5, farthest=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T08:43:55.525133Z",
     "iopub.status.busy": "2020-11-06T08:43:55.524796Z",
     "iopub.status.idle": "2020-11-06T08:43:57.670210Z",
     "shell.execute_reply": "2020-11-06T08:43:57.668922Z",
     "shell.execute_reply.started": "2020-11-06T08:43:55.525091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 13846)\t-0.025279622321219046\n",
      "  (0, 7495)\t-0.0357507847383376\n",
      "  (0, 7494)\t-0.01598823698477698\n",
      "  (0, 2620)\t-0.025279622321219046\n",
      "  (0, 15966)\t0.06331018843045884\n",
      "  (0, 15087)\t0.035391471249706666\n",
      "  (0, 12074)\t0.03165509421522942\n",
      "  (0, 10633)\t0.035391471249706666\n",
      "  (0, 9057)\t0.035391471249706666\n",
      "  (0, 6764)\t0.035391471249706666\n",
      "  (0, 932)\t0.04044739571395047\n",
      "  (0, 924)\t0.035391471249706666\n",
      "  (0, 264)\t0.035391471249706666\n",
      "  (0, 12888)\t-0.005055924464243809\n",
      "  (0, 10416)\t-0.005055924464243809\n",
      "  (0, 9685)\t-0.005055924464243809\n",
      "  (0, 5856)\t-0.005055924464243809\n",
      "  (0, 5082)\t-0.005055924464243809\n",
      "  (0, 1689)\t-0.005055924464243809\n",
      "  (0, 6601)\t0.030335546785462856\n",
      "  (0, 13182)\t0.01011184892848762\n",
      "  (0, 11022)\t0.05055924464243809\n",
      "  (0, 6047)\t0.01516777339273143\n",
      "  (0, 18973)\t-0.020223697856975236\n",
      "  (0, 2570)\t-0.020223697856975236\n",
      "  :\t:\n",
      "  (0, 1517)\t0.10487445099896678\n",
      "  (0, 378)\t0.015167773392731426\n",
      "  (0, 17519)\t0.2206927018350943\n",
      "  (0, 17195)\t0.2054770883063076\n",
      "  (0, 15217)\t0.19166728850361367\n",
      "  (0, 15049)\t0.07078294249941332\n",
      "  (0, 12705)\t-0.12134218714185142\n",
      "  (0, 12098)\t0.1233331019853878\n",
      "  (0, 11676)\t0.10853175159507228\n",
      "  (0, 10983)\t-0.0178753923691688\n",
      "  (0, 9493)\t0.08760505894486968\n",
      "  (0, 9483)\t-0.02154373376349465\n",
      "  (0, 9323)\t0.15456220306650786\n",
      "  (0, 9048)\t0.2682487559867141\n",
      "  (0, 7828)\t-0.04657006086929763\n",
      "  (0, 7654)\t0.12170495878844234\n",
      "  (0, 7200)\t0.19425254598159966\n",
      "  (0, 6214)\t0.032682810201150644\n",
      "  (0, 6022)\t0.003966193461912381\n",
      "  (0, 3269)\t0.013770435310540417\n",
      "  (0, 3170)\t0.13791446619616143\n",
      "  (0, 2398)\t0.09057178588510263\n",
      "  (0, 2257)\t-0.1377313701155717\n",
      "  (0, 1862)\t0.22528668398187698\n",
      "  (0, 1655)\t0.09598459715740455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('absolute', 0.48020216924246883),\n",
       " ('absurdity', 0.47325181474939865),\n",
       " ('hoods', 0.46245227781997383),\n",
       " ('\"rape', 0.4586303581685691),\n",
       " ('made!', 0.44769347532967324),\n",
       " ('illiterate,', 0.44767063557375547),\n",
       " ('poetry.', 0.44767063557375547),\n",
       " ('suits,', 0.44767063557375547),\n",
       " ('cellar', 0.44716459921435486),\n",
       " ('couch', 0.44716459921435486)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo_analogies = get_analogy(\n",
    "    co_occurrence_matrix, word_to_idx, \"bad\", \"worst\", \"big\", k=10\n",
    ")\n",
    "coo_analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:22:31.785050Z",
     "iopub.status.busy": "2020-11-05T21:22:31.784640Z",
     "iopub.status.idle": "2020-11-05T21:22:31.844145Z",
     "shell.execute_reply": "2020-11-05T21:22:31.842982Z",
     "shell.execute_reply.started": "2020-11-05T21:22:31.785006Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_ppmi(co_occurrence_matrix, to_dense=False):\n",
    "    \"\"\"\n",
    "    Converts a count-based co-occurrence matrix to a PPMI matrix\n",
    "    \"\"\"\n",
    "    # Compute sums\n",
    "    total_sum = float(co_occurrence_matrix.sum())\n",
    "    row_col_sums = np.array(\n",
    "        co_occurrence_matrix.sum(axis=1), dtype=np.float64\n",
    "    ).flatten()\n",
    "\n",
    "    # Get CSR matrix elements\n",
    "    if not hasattr(scipy.sparse, type(co_occurrence_matrix).__name__):\n",
    "        co_occurrence_matrix = scipy.sparse.csr_matrix(co_occurrence_matrix)\n",
    "    data, indices, indptr = (\n",
    "        list(enumerate(co_occurrence_matrix.data)),\n",
    "        co_occurrence_matrix.indices,\n",
    "        co_occurrence_matrix.indptr,\n",
    "    )\n",
    "\n",
    "    # Compute PPMI matrix\n",
    "    ppmi_data, ppmi_indices, ppmi_indptr = [], [], [0]\n",
    "    for row in tqdm(range(len(indptr) - 1)):\n",
    "        for col, elem in data[indptr[row] : indptr[row + 1]]:\n",
    "            pmi = np.log2(\n",
    "                (elem * total_sum) / (row_col_sums[row] * row_col_sums[indices[col]])\n",
    "            )\n",
    "            if pmi > 0:\n",
    "                ppmi_data.append(pmi)\n",
    "                ppmi_indices.append(indices[col])\n",
    "        if ppmi_indptr[-1] != len(ppmi_data):\n",
    "            ppmi_indptr.append(len(ppmi_data))\n",
    "\n",
    "    # Re-format as sparse matrix\n",
    "    res = scipy.sparse.csr_matrix(\n",
    "        (ppmi_data, ppmi_indices, ppmi_indptr), dtype=np.float64\n",
    "    )\n",
    "    res.eliminate_zeros()\n",
    "    return res if not to_dense else res.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:23:09.085796Z",
     "iopub.status.busy": "2020-11-05T21:23:09.085464Z",
     "iopub.status.idle": "2020-11-05T21:23:17.172845Z",
     "shell.execute_reply": "2020-11-05T21:23:17.171566Z",
     "shell.execute_reply.started": "2020-11-05T21:23:09.085755Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19383/19383 [00:07<00:00, 2641.40it/s]\n"
     ]
    }
   ],
   "source": [
    "ppmi_occurrence_matrix = convert_ppmi(co_occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:23:17.175311Z",
     "iopub.status.busy": "2020-11-05T21:23:17.174999Z",
     "iopub.status.idle": "2020-11-05T21:23:17.227987Z",
     "shell.execute_reply": "2020-11-05T21:23:17.226583Z",
     "shell.execute_reply.started": "2020-11-05T21:23:17.175271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<19383x19383 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 478480 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:23:17.229908Z",
     "iopub.status.busy": "2020-11-05T21:23:17.229498Z",
     "iopub.status.idle": "2020-11-05T21:23:17.620467Z",
     "shell.execute_reply": "2020-11-05T21:23:17.619139Z",
     "shell.execute_reply.started": "2020-11-05T21:23:17.229868Z"
    }
   },
   "outputs": [],
   "source": [
    "ppmi_svd = utils.reduce_svd(ppmi_occurrence_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T21:25:49.003414Z",
     "iopub.status.busy": "2020-11-05T21:25:49.003076Z",
     "iopub.status.idle": "2020-11-05T21:25:49.146234Z",
     "shell.execute_reply": "2020-11-05T21:25:49.145156Z",
     "shell.execute_reply.started": "2020-11-05T21:25:49.003370Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "445181d43c7b48e9bcb669f82aa1eb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.visualize_embeddings(ppmi_svd, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppmi_tsne = utils.reduce_tsne(ppmi_occurrence_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_embeddings(ppmi_tsne, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T08:47:04.483261Z",
     "iopub.status.busy": "2020-11-06T08:47:04.482931Z",
     "iopub.status.idle": "2020-11-06T08:47:06.640572Z",
     "shell.execute_reply": "2020-11-06T08:47:06.639444Z",
     "shell.execute_reply.started": "2020-11-06T08:47:04.483219Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.4622897628849123),\n",
       " ('move\"', 0.12746215326877222),\n",
       " (\"keeble's\", 0.1096449105794998),\n",
       " ('\"other\"', 0.1018528506263987),\n",
       " ('grungy', 0.10019862548009348),\n",
       " ('\"max', 0.09692032256119065),\n",
       " ('run,\"', 0.09569934649763695),\n",
       " ('strapping', 0.09500922973422851),\n",
       " ('phallus.still', 0.091658556809609),\n",
       " ('\"see', 0.09154154301772106)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_analogies = get_analogy(\n",
    "    ppmi_occurrence_matrix, word_to_idx, \"bad\", \"worst\", \"big\", k=10\n",
    ")\n",
    "ppmi_analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T18:04:06.614013Z",
     "iopub.status.busy": "2020-11-05T18:04:06.613604Z",
     "iopub.status.idle": "2020-11-05T18:04:06.661774Z",
     "shell.execute_reply": "2020-11-05T18:04:06.660500Z",
     "shell.execute_reply.started": "2020-11-05T18:04:06.613969Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_oov_terms(embedding_model, word_listing):\n",
    "    \"\"\"\n",
    "    Checks differences between pre-trained embedding model vocabulary\n",
    "    and dataset specific vocabulary in order to highlight out-of-vocabulary terms\n",
    "    \"\"\"\n",
    "    oov_terms = []\n",
    "    for word in word_listing:\n",
    "        if word not in embedding_model.vocab:\n",
    "            oov_terms.append(word)\n",
    "    return oov_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T17:55:35.744854Z",
     "iopub.status.busy": "2020-11-05T17:55:35.744438Z",
     "iopub.status.idle": "2020-11-05T17:55:35.799878Z",
     "shell.execute_reply": "2020-11-05T17:55:35.798523Z",
     "shell.execute_reply.started": "2020-11-05T17:55:35.744810Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_embedding_matrix(\n",
    "    embedding_model,\n",
    "    embedding_dimension,\n",
    "    word_to_idx,\n",
    "    idx_to_word,\n",
    "    oov_terms,\n",
    "    coo_matrix,\n",
    "    method=\"mean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Builds the embedding matrix of a specific dataset given a pre-trained Gensim word embedding model\n",
    "    \"\"\"\n",
    "\n",
    "    def random_embedding(embedding_dimension, interval=(-1, 1)):\n",
    "        return interval[0] + np.random.sample(embedding_dimension) + interval[1]\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_to_idx), embedding_dimension))\n",
    "    for word, index in word_to_idx.items():\n",
    "        # Words that are no OOV are taken from the Gensim model\n",
    "        if word not in oov_terms:\n",
    "            word_vector = embedding_model[word]\n",
    "        # OOV words computed as the mean of not OOV neighboring words in the dataset\n",
    "        elif method == \"mean\":\n",
    "            neighboring_word_indices = coo_matrix.indices[\n",
    "                coo_matrix.indptr[index]:coo_matrix.indptr[index + 1]\n",
    "            ]\n",
    "            neighboring_word_vectors = np.array(\n",
    "                [\n",
    "                    embedding_model[idx_to_word[k]]\n",
    "                    for k in neighboring_word_indices\n",
    "                    if idx_to_word[k] in embedding_model\n",
    "                ]\n",
    "            )\n",
    "            # Check if at least one neighboring word is in the Gensim model vocabulary\n",
    "            if len(neighboring_word_vectors) > 0:\n",
    "                word_vector = np.mean(neighboring_word_vectors, axis=0)\n",
    "            # If not, resort to random vectors\n",
    "            else:\n",
    "                word_vector = random_embedding(embedding_dimension)\n",
    "        # OOV words computed as random vectors in range [-1, 1]\n",
    "        elif method == \"random\":\n",
    "            word_vector = random_embedding(embedding_dimension)\n",
    "        embedding_matrix[index, :] = word_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T09:20:07.596376Z",
     "iopub.status.busy": "2020-11-06T09:20:07.596031Z",
     "iopub.status.idle": "2020-11-06T09:29:19.320480Z",
     "shell.execute_reply": "2020-11-06T09:29:19.318960Z",
     "shell.execute_reply.started": "2020-11-06T09:20:07.596334Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "w2v_dimension = 300\n",
    "w2v_model = utils.load_embedding_model(\"word2vec\", w2v_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-06T09:34:15.373028Z",
     "iopub.status.busy": "2020-11-06T09:34:15.372717Z",
     "iopub.status.idle": "2020-11-06T09:34:15.435529Z",
     "shell.execute_reply": "2020-11-06T09:34:15.433631Z",
     "shell.execute_reply.started": "2020-11-06T09:34:15.372988Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'check_oov_terms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-4ba5825a4515>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_oov_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_oov_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_listing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m print(\n\u001b[1;32m      3\u001b[0m     \u001b[0;34mf\"Total OOV terms: {len(w2v_oov_terms)} ({round(len(w2v_oov_terms) / len(word_listing), 2)}%)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'check_oov_terms' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_oov_terms = check_oov_terms(w2v_model, word_listing)\n",
    "print(\n",
    "    f\"Total OOV terms: {len(w2v_oov_terms)} ({round(len(w2v_oov_terms) / len(word_listing), 2)}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_matrix = build_embedding_matrix(\n",
    "    w2v_model,\n",
    "    w2v_dimension,\n",
    "    word_to_idx,\n",
    "    idx_to_word,\n",
    "    w2v_oov_terms,\n",
    "    co_occurrence_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_svd = utils.reduce_svd(w2v_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_embeddings(w2v_svd, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_tsne = utils.reduce_tsne(w2v_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_embeddings(w2v_tsne, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_analogies = get_analogy(\n",
    "    w2v_matrix, word_to_idx, \"bad\", \"worst\", \"big\", k=10\n",
    ")\n",
    "w2v_analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T18:03:04.346566Z",
     "iopub.status.busy": "2020-11-05T18:03:04.346013Z",
     "iopub.status.idle": "2020-11-05T18:03:51.692890Z",
     "shell.execute_reply": "2020-11-05T18:03:51.690971Z",
     "shell.execute_reply.started": "2020-11-05T18:03:04.346508Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_dimension = 50\n",
    "glove_model = utils.load_embedding_model(\"glove\", glove_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T18:04:07.240126Z",
     "iopub.status.busy": "2020-11-05T18:04:07.239807Z",
     "iopub.status.idle": "2020-11-05T18:04:07.305656Z",
     "shell.execute_reply": "2020-11-05T18:04:07.304429Z",
     "shell.execute_reply.started": "2020-11-05T18:04:07.240089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total OOV terms: 9390 (0.48%)\n"
     ]
    }
   ],
   "source": [
    "glove_oov_terms = check_oov_terms(glove_model, word_listing)\n",
    "print(\n",
    "    f\"Total OOV terms: {len(glove_oov_terms)} ({round(len(glove_oov_terms) / len(word_listing), 2)}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T17:56:50.295189Z",
     "iopub.status.busy": "2020-11-05T17:56:50.294779Z",
     "iopub.status.idle": "2020-11-05T17:56:58.704335Z",
     "shell.execute_reply": "2020-11-05T17:56:58.702570Z",
     "shell.execute_reply.started": "2020-11-05T17:56:50.295145Z"
    }
   },
   "outputs": [],
   "source": [
    "glove_matrix = build_embedding_matrix(\n",
    "    glove_model,\n",
    "    glove_dimension,\n",
    "    word_to_idx,\n",
    "    idx_to_word,\n",
    "    glove_oov_terms,\n",
    "    co_occurrence_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-05T17:56:58.772360Z",
     "iopub.status.busy": "2020-11-05T17:56:58.772056Z",
     "iopub.status.idle": "2020-11-05T17:56:58.822327Z",
     "shell.execute_reply": "2020-11-05T17:56:58.821084Z",
     "shell.execute_reply.started": "2020-11-05T17:56:58.772322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19383, 50)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_svd = utils.reduce_svd(glove_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_embeddings(glove_matrix, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_tsne = utils.reduce_tsne(glove_matrix, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.visualize_embeddings(glove_tsne, ['good', 'love', 'beautiful'], word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_analogies = get_analogy(\n",
    "    glove_matrix, word_to_idx, \"bad\", \"worst\", \"big\", k=10\n",
    ")\n",
    "glove_analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some code blocks, especially those regarding dataset loading/initialization and embeddings visualization, were taken from a notebook as part of an assignment for the NLP course of the Artificial Intelligence master's degree, at University of Bologna. The cited notebook is maintained by Andrea Galassi, Federico Ruggeri and Paolo Torroni."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
